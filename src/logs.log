2025-05-03 09:52:11,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:52:11,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:52:11,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:52:11,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:54:52,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:54:52,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:54:52,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:54:52,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 09:54:55,888:INFO:PyCaret ClassificationExperiment
2025-05-03 09:54:55,888:INFO:Logging name: clf-default-name
2025-05-03 09:54:55,888:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 09:54:55,888:INFO:version 3.3.2
2025-05-03 09:54:55,888:INFO:Initializing setup()
2025-05-03 09:54:55,888:INFO:self.USI: b9ba
2025-05-03 09:54:55,888:INFO:self._variable_keys: {'pipeline', 'data', 'exp_name_log', 'gpu_param', 'y_test', 'X_train', 'X_test', 'y_train', 'memory', 'y', 'exp_id', '_ml_usecase', 'fold_generator', 'X', 'n_jobs_param', 'logging_param', 'fix_imbalance', 'html_param', 'fold_shuffle_param', 'is_multiclass', '_available_plots', 'fold_groups_param', 'gpu_n_jobs_param', 'log_plots_param', 'idx', 'seed', 'target_param', 'USI'}
2025-05-03 09:54:55,888:INFO:Checking environment
2025-05-03 09:54:55,888:INFO:python_version: 3.11.11
2025-05-03 09:54:55,888:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 09:54:55,890:INFO:machine: AMD64
2025-05-03 09:54:55,890:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 09:54:55,894:INFO:Memory: svmem(total=16965230592, available=2711134208, percent=84.0, used=14254096384, free=2711134208)
2025-05-03 09:54:55,894:INFO:Physical Core: 4
2025-05-03 09:54:55,894:INFO:Logical Core: 8
2025-05-03 09:54:55,894:INFO:Checking libraries
2025-05-03 09:54:55,894:INFO:System:
2025-05-03 09:54:55,894:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 09:54:55,894:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 09:54:55,894:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 09:54:55,894:INFO:PyCaret required dependencies:
2025-05-03 09:54:55,894:INFO:                 pip: 25.0
2025-05-03 09:54:55,894:INFO:          setuptools: 75.8.0
2025-05-03 09:54:55,894:INFO:             pycaret: 3.3.2
2025-05-03 09:54:55,894:INFO:             IPython: 8.32.0
2025-05-03 09:54:55,894:INFO:          ipywidgets: 8.1.6
2025-05-03 09:54:55,894:INFO:                tqdm: 4.67.1
2025-05-03 09:54:55,894:INFO:               numpy: 1.26.4
2025-05-03 09:54:55,894:INFO:              pandas: 2.1.4
2025-05-03 09:54:55,894:INFO:              jinja2: 3.1.6
2025-05-03 09:54:55,894:INFO:               scipy: 1.11.4
2025-05-03 09:54:55,894:INFO:              joblib: 1.3.2
2025-05-03 09:54:55,894:INFO:             sklearn: 1.4.2
2025-05-03 09:54:55,894:INFO:                pyod: 2.0.5
2025-05-03 09:54:55,894:INFO:            imblearn: 0.13.0
2025-05-03 09:54:55,894:INFO:   category_encoders: 2.7.0
2025-05-03 09:54:55,894:INFO:            lightgbm: 4.6.0
2025-05-03 09:54:55,894:INFO:               numba: 0.61.2
2025-05-03 09:54:55,894:INFO:            requests: 2.32.3
2025-05-03 09:54:55,894:INFO:          matplotlib: 3.7.5
2025-05-03 09:54:55,894:INFO:          scikitplot: 0.3.7
2025-05-03 09:54:55,894:INFO:         yellowbrick: 1.5
2025-05-03 09:54:55,894:INFO:              plotly: 5.24.1
2025-05-03 09:54:55,894:INFO:    plotly-resampler: Not installed
2025-05-03 09:54:55,894:INFO:             kaleido: 0.2.1
2025-05-03 09:54:55,899:INFO:           schemdraw: 0.15
2025-05-03 09:54:55,899:INFO:         statsmodels: 0.14.4
2025-05-03 09:54:55,899:INFO:              sktime: 0.26.0
2025-05-03 09:54:55,899:INFO:               tbats: 1.1.3
2025-05-03 09:54:55,899:INFO:            pmdarima: 2.0.4
2025-05-03 09:54:55,899:INFO:              psutil: 6.1.1
2025-05-03 09:54:55,899:INFO:          markupsafe: 3.0.2
2025-05-03 09:54:55,899:INFO:             pickle5: Not installed
2025-05-03 09:54:55,899:INFO:         cloudpickle: 3.1.1
2025-05-03 09:54:55,899:INFO:         deprecation: 2.1.0
2025-05-03 09:54:55,899:INFO:              xxhash: 3.5.0
2025-05-03 09:54:55,899:INFO:           wurlitzer: Not installed
2025-05-03 09:54:55,899:INFO:PyCaret optional dependencies:
2025-05-03 09:54:57,558:INFO:                shap: 0.47.2
2025-05-03 09:54:57,558:INFO:           interpret: Not installed
2025-05-03 09:54:57,558:INFO:                umap: Not installed
2025-05-03 09:54:57,558:INFO:     ydata_profiling: Not installed
2025-05-03 09:54:57,558:INFO:  explainerdashboard: Not installed
2025-05-03 09:54:57,558:INFO:             autoviz: Not installed
2025-05-03 09:54:57,558:INFO:           fairlearn: Not installed
2025-05-03 09:54:57,558:INFO:          deepchecks: Not installed
2025-05-03 09:54:57,558:INFO:             xgboost: 3.0.0
2025-05-03 09:54:57,558:INFO:            catboost: Not installed
2025-05-03 09:54:57,558:INFO:              kmodes: Not installed
2025-05-03 09:54:57,558:INFO:             mlxtend: Not installed
2025-05-03 09:54:57,558:INFO:       statsforecast: Not installed
2025-05-03 09:54:57,558:INFO:        tune_sklearn: Not installed
2025-05-03 09:54:57,558:INFO:                 ray: Not installed
2025-05-03 09:54:57,558:INFO:            hyperopt: 0.2.7
2025-05-03 09:54:57,558:INFO:              optuna: Not installed
2025-05-03 09:54:57,558:INFO:               skopt: 0.10.2
2025-05-03 09:54:57,558:INFO:              mlflow: 2.22.0
2025-05-03 09:54:57,560:INFO:              gradio: Not installed
2025-05-03 09:54:57,560:INFO:             fastapi: 0.115.12
2025-05-03 09:54:57,560:INFO:             uvicorn: 0.34.2
2025-05-03 09:54:57,560:INFO:              m2cgen: Not installed
2025-05-03 09:54:57,560:INFO:           evidently: Not installed
2025-05-03 09:54:57,560:INFO:               fugue: Not installed
2025-05-03 09:54:57,560:INFO:           streamlit: Not installed
2025-05-03 09:54:57,560:INFO:             prophet: Not installed
2025-05-03 09:54:57,560:INFO:None
2025-05-03 09:54:57,560:INFO:Set up data.
2025-05-03 09:54:57,566:INFO:Set up folding strategy.
2025-05-03 09:54:57,566:INFO:Set up train/test split.
2025-05-03 09:54:57,592:INFO:Set up index.
2025-05-03 09:54:57,592:INFO:Assigning column types.
2025-05-03 09:54:57,603:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 09:54:57,641:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,641:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,669:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,671:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,703:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,703:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,724:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,730:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,730:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 09:54:57,766:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,783:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,821:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 09:54:57,850:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,850:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,850:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 09:54:57,916:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,919:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,975:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:57,975:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:57,983:INFO:Finished creating preprocessing pipeline.
2025-05-03 09:54:57,983:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 09:54:57,983:INFO:Creating final display dataframe.
2025-05-03 09:54:58,058:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 09:54:58,116:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:58,116:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:58,183:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 09:54:58,183:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 09:54:58,183:INFO:setup() successfully completed in 2.3s...............
2025-05-03 09:54:58,194:INFO:Initializing compare_models()
2025-05-03 09:54:58,194:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 09:54:58,194:INFO:Checking exceptions
2025-05-03 09:54:58,208:INFO:Preparing display monitor
2025-05-03 09:54:58,337:INFO:Initializing Logistic Regression
2025-05-03 09:54:58,337:INFO:Total runtime is 0.0 minutes
2025-05-03 09:54:58,341:INFO:SubProcess create_model() called ==================================
2025-05-03 09:54:58,342:INFO:Initializing create_model()
2025-05-03 09:54:58,342:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:54:58,342:INFO:Checking exceptions
2025-05-03 09:54:58,342:INFO:Importing libraries
2025-05-03 09:54:58,343:INFO:Copying training dataset
2025-05-03 09:54:58,365:INFO:Defining folds
2025-05-03 09:54:58,365:INFO:Declaring metric variables
2025-05-03 09:54:58,370:INFO:Importing untrained model
2025-05-03 09:54:58,375:INFO:Logistic Regression Imported successfully
2025-05-03 09:54:58,383:INFO:Starting cross validation
2025-05-03 09:54:58,384:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:01,950:INFO:Calculating mean and std
2025-05-03 09:55:01,952:INFO:Creating metrics dataframe
2025-05-03 09:55:01,956:INFO:Uploading results into container
2025-05-03 09:55:01,956:INFO:Uploading model into container now
2025-05-03 09:55:01,956:INFO:_master_model_container: 1
2025-05-03 09:55:01,956:INFO:_display_container: 2
2025-05-03 09:55:01,959:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 09:55:01,959:INFO:create_model() successfully completed......................................
2025-05-03 09:55:02,084:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:02,084:INFO:Creating metrics dataframe
2025-05-03 09:55:02,088:INFO:Initializing K Neighbors Classifier
2025-05-03 09:55:02,088:INFO:Total runtime is 0.0625140349070231 minutes
2025-05-03 09:55:02,094:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:02,094:INFO:Initializing create_model()
2025-05-03 09:55:02,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=knn, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:02,094:INFO:Checking exceptions
2025-05-03 09:55:02,094:INFO:Importing libraries
2025-05-03 09:55:02,094:INFO:Copying training dataset
2025-05-03 09:55:02,114:INFO:Defining folds
2025-05-03 09:55:02,114:INFO:Declaring metric variables
2025-05-03 09:55:02,117:INFO:Importing untrained model
2025-05-03 09:55:02,117:INFO:K Neighbors Classifier Imported successfully
2025-05-03 09:55:02,133:INFO:Starting cross validation
2025-05-03 09:55:02,134:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:06,646:INFO:Calculating mean and std
2025-05-03 09:55:06,648:INFO:Creating metrics dataframe
2025-05-03 09:55:06,650:INFO:Uploading results into container
2025-05-03 09:55:06,651:INFO:Uploading model into container now
2025-05-03 09:55:06,651:INFO:_master_model_container: 2
2025-05-03 09:55:06,651:INFO:_display_container: 2
2025-05-03 09:55:06,651:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-03 09:55:06,653:INFO:create_model() successfully completed......................................
2025-05-03 09:55:06,747:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:06,747:INFO:Creating metrics dataframe
2025-05-03 09:55:06,758:INFO:Initializing Naive Bayes
2025-05-03 09:55:06,758:INFO:Total runtime is 0.14034296274185182 minutes
2025-05-03 09:55:06,761:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:06,761:INFO:Initializing create_model()
2025-05-03 09:55:06,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=nb, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:06,761:INFO:Checking exceptions
2025-05-03 09:55:06,761:INFO:Importing libraries
2025-05-03 09:55:06,761:INFO:Copying training dataset
2025-05-03 09:55:06,772:INFO:Defining folds
2025-05-03 09:55:06,772:INFO:Declaring metric variables
2025-05-03 09:55:06,772:INFO:Importing untrained model
2025-05-03 09:55:06,772:INFO:Naive Bayes Imported successfully
2025-05-03 09:55:06,789:INFO:Starting cross validation
2025-05-03 09:55:06,789:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:09,397:INFO:Calculating mean and std
2025-05-03 09:55:09,397:INFO:Creating metrics dataframe
2025-05-03 09:55:09,399:INFO:Uploading results into container
2025-05-03 09:55:09,399:INFO:Uploading model into container now
2025-05-03 09:55:09,399:INFO:_master_model_container: 3
2025-05-03 09:55:09,401:INFO:_display_container: 2
2025-05-03 09:55:09,401:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-03 09:55:09,401:INFO:create_model() successfully completed......................................
2025-05-03 09:55:09,499:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:09,499:INFO:Creating metrics dataframe
2025-05-03 09:55:09,499:INFO:Initializing Decision Tree Classifier
2025-05-03 09:55:09,499:INFO:Total runtime is 0.18602941433588666 minutes
2025-05-03 09:55:09,509:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:09,509:INFO:Initializing create_model()
2025-05-03 09:55:09,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:09,509:INFO:Checking exceptions
2025-05-03 09:55:09,509:INFO:Importing libraries
2025-05-03 09:55:09,509:INFO:Copying training dataset
2025-05-03 09:55:09,528:INFO:Defining folds
2025-05-03 09:55:09,528:INFO:Declaring metric variables
2025-05-03 09:55:09,534:INFO:Importing untrained model
2025-05-03 09:55:09,541:INFO:Decision Tree Classifier Imported successfully
2025-05-03 09:55:09,548:INFO:Starting cross validation
2025-05-03 09:55:09,551:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:09,714:INFO:Calculating mean and std
2025-05-03 09:55:09,714:INFO:Creating metrics dataframe
2025-05-03 09:55:09,714:INFO:Uploading results into container
2025-05-03 09:55:09,714:INFO:Uploading model into container now
2025-05-03 09:55:09,717:INFO:_master_model_container: 4
2025-05-03 09:55:09,717:INFO:_display_container: 2
2025-05-03 09:55:09,717:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-05-03 09:55:09,717:INFO:create_model() successfully completed......................................
2025-05-03 09:55:09,816:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:09,818:INFO:Creating metrics dataframe
2025-05-03 09:55:09,823:INFO:Initializing SVM - Linear Kernel
2025-05-03 09:55:09,823:INFO:Total runtime is 0.1914294799168905 minutes
2025-05-03 09:55:09,823:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:09,823:INFO:Initializing create_model()
2025-05-03 09:55:09,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=svm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:09,823:INFO:Checking exceptions
2025-05-03 09:55:09,823:INFO:Importing libraries
2025-05-03 09:55:09,823:INFO:Copying training dataset
2025-05-03 09:55:09,845:INFO:Defining folds
2025-05-03 09:55:09,845:INFO:Declaring metric variables
2025-05-03 09:55:09,849:INFO:Importing untrained model
2025-05-03 09:55:09,854:INFO:SVM - Linear Kernel Imported successfully
2025-05-03 09:55:09,859:INFO:Starting cross validation
2025-05-03 09:55:09,859:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:09,991:INFO:Calculating mean and std
2025-05-03 09:55:09,991:INFO:Creating metrics dataframe
2025-05-03 09:55:09,991:INFO:Uploading results into container
2025-05-03 09:55:09,991:INFO:Uploading model into container now
2025-05-03 09:55:09,991:INFO:_master_model_container: 5
2025-05-03 09:55:09,991:INFO:_display_container: 2
2025-05-03 09:55:09,991:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-03 09:55:09,991:INFO:create_model() successfully completed......................................
2025-05-03 09:55:10,096:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:10,096:INFO:Creating metrics dataframe
2025-05-03 09:55:10,104:INFO:Initializing Ridge Classifier
2025-05-03 09:55:10,104:INFO:Total runtime is 0.19611298640569055 minutes
2025-05-03 09:55:10,108:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:10,108:INFO:Initializing create_model()
2025-05-03 09:55:10,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:10,108:INFO:Checking exceptions
2025-05-03 09:55:10,109:INFO:Importing libraries
2025-05-03 09:55:10,109:INFO:Copying training dataset
2025-05-03 09:55:10,123:INFO:Defining folds
2025-05-03 09:55:10,123:INFO:Declaring metric variables
2025-05-03 09:55:10,123:INFO:Importing untrained model
2025-05-03 09:55:10,129:INFO:Ridge Classifier Imported successfully
2025-05-03 09:55:10,129:INFO:Starting cross validation
2025-05-03 09:55:10,129:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:10,206:INFO:Calculating mean and std
2025-05-03 09:55:10,206:INFO:Creating metrics dataframe
2025-05-03 09:55:10,206:INFO:Uploading results into container
2025-05-03 09:55:10,208:INFO:Uploading model into container now
2025-05-03 09:55:10,208:INFO:_master_model_container: 6
2025-05-03 09:55:10,208:INFO:_display_container: 2
2025-05-03 09:55:10,208:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 09:55:10,208:INFO:create_model() successfully completed......................................
2025-05-03 09:55:10,306:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:10,306:INFO:Creating metrics dataframe
2025-05-03 09:55:10,306:INFO:Initializing Random Forest Classifier
2025-05-03 09:55:10,306:INFO:Total runtime is 0.19947114388147993 minutes
2025-05-03 09:55:10,321:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:10,321:INFO:Initializing create_model()
2025-05-03 09:55:10,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:10,321:INFO:Checking exceptions
2025-05-03 09:55:10,321:INFO:Importing libraries
2025-05-03 09:55:10,321:INFO:Copying training dataset
2025-05-03 09:55:10,344:INFO:Defining folds
2025-05-03 09:55:10,344:INFO:Declaring metric variables
2025-05-03 09:55:10,348:INFO:Importing untrained model
2025-05-03 09:55:10,352:INFO:Random Forest Classifier Imported successfully
2025-05-03 09:55:10,359:INFO:Starting cross validation
2025-05-03 09:55:10,360:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:11,640:INFO:Calculating mean and std
2025-05-03 09:55:11,641:INFO:Creating metrics dataframe
2025-05-03 09:55:11,644:INFO:Uploading results into container
2025-05-03 09:55:11,644:INFO:Uploading model into container now
2025-05-03 09:55:11,645:INFO:_master_model_container: 7
2025-05-03 09:55:11,645:INFO:_display_container: 2
2025-05-03 09:55:11,645:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 09:55:11,646:INFO:create_model() successfully completed......................................
2025-05-03 09:55:11,808:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:11,808:INFO:Creating metrics dataframe
2025-05-03 09:55:11,818:INFO:Initializing Quadratic Discriminant Analysis
2025-05-03 09:55:11,818:INFO:Total runtime is 0.22468181848526003 minutes
2025-05-03 09:55:11,829:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:11,829:INFO:Initializing create_model()
2025-05-03 09:55:11,829:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=qda, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:11,829:INFO:Checking exceptions
2025-05-03 09:55:11,829:INFO:Importing libraries
2025-05-03 09:55:11,829:INFO:Copying training dataset
2025-05-03 09:55:11,870:INFO:Defining folds
2025-05-03 09:55:11,870:INFO:Declaring metric variables
2025-05-03 09:55:11,878:INFO:Importing untrained model
2025-05-03 09:55:11,887:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-03 09:55:11,901:INFO:Starting cross validation
2025-05-03 09:55:11,903:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:12,044:INFO:Calculating mean and std
2025-05-03 09:55:12,044:INFO:Creating metrics dataframe
2025-05-03 09:55:12,048:INFO:Uploading results into container
2025-05-03 09:55:12,048:INFO:Uploading model into container now
2025-05-03 09:55:12,052:INFO:_master_model_container: 8
2025-05-03 09:55:12,052:INFO:_display_container: 2
2025-05-03 09:55:12,053:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-03 09:55:12,053:INFO:create_model() successfully completed......................................
2025-05-03 09:55:12,213:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:12,214:INFO:Creating metrics dataframe
2025-05-03 09:55:12,233:INFO:Initializing Ada Boost Classifier
2025-05-03 09:55:12,233:INFO:Total runtime is 0.23159303665161135 minutes
2025-05-03 09:55:12,237:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:12,237:INFO:Initializing create_model()
2025-05-03 09:55:12,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=ada, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:12,237:INFO:Checking exceptions
2025-05-03 09:55:12,237:INFO:Importing libraries
2025-05-03 09:55:12,237:INFO:Copying training dataset
2025-05-03 09:55:12,282:INFO:Defining folds
2025-05-03 09:55:12,282:INFO:Declaring metric variables
2025-05-03 09:55:12,283:INFO:Importing untrained model
2025-05-03 09:55:12,290:INFO:Ada Boost Classifier Imported successfully
2025-05-03 09:55:12,301:INFO:Starting cross validation
2025-05-03 09:55:12,301:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:12,330:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-03 09:55:12,343:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-03 09:55:12,352:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-03 09:55:13,202:INFO:Calculating mean and std
2025-05-03 09:55:13,205:INFO:Creating metrics dataframe
2025-05-03 09:55:13,207:INFO:Uploading results into container
2025-05-03 09:55:13,209:INFO:Uploading model into container now
2025-05-03 09:55:13,209:INFO:_master_model_container: 9
2025-05-03 09:55:13,209:INFO:_display_container: 2
2025-05-03 09:55:13,209:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-05-03 09:55:13,211:INFO:create_model() successfully completed......................................
2025-05-03 09:55:13,393:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:13,393:INFO:Creating metrics dataframe
2025-05-03 09:55:13,417:INFO:Initializing Gradient Boosting Classifier
2025-05-03 09:55:13,417:INFO:Total runtime is 0.2513250549634298 minutes
2025-05-03 09:55:13,420:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:13,425:INFO:Initializing create_model()
2025-05-03 09:55:13,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=gbc, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:13,425:INFO:Checking exceptions
2025-05-03 09:55:13,425:INFO:Importing libraries
2025-05-03 09:55:13,425:INFO:Copying training dataset
2025-05-03 09:55:13,469:INFO:Defining folds
2025-05-03 09:55:13,469:INFO:Declaring metric variables
2025-05-03 09:55:13,469:INFO:Importing untrained model
2025-05-03 09:55:13,481:INFO:Gradient Boosting Classifier Imported successfully
2025-05-03 09:55:13,498:INFO:Starting cross validation
2025-05-03 09:55:13,501:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:16,135:INFO:Calculating mean and std
2025-05-03 09:55:16,137:INFO:Creating metrics dataframe
2025-05-03 09:55:16,139:INFO:Uploading results into container
2025-05-03 09:55:16,140:INFO:Uploading model into container now
2025-05-03 09:55:16,141:INFO:_master_model_container: 10
2025-05-03 09:55:16,141:INFO:_display_container: 2
2025-05-03 09:55:16,142:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-05-03 09:55:16,142:INFO:create_model() successfully completed......................................
2025-05-03 09:55:16,310:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:16,310:INFO:Creating metrics dataframe
2025-05-03 09:55:16,333:INFO:Initializing Linear Discriminant Analysis
2025-05-03 09:55:16,334:INFO:Total runtime is 0.2999405940373739 minutes
2025-05-03 09:55:16,343:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:16,343:INFO:Initializing create_model()
2025-05-03 09:55:16,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=lda, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:16,344:INFO:Checking exceptions
2025-05-03 09:55:16,344:INFO:Importing libraries
2025-05-03 09:55:16,344:INFO:Copying training dataset
2025-05-03 09:55:16,383:INFO:Defining folds
2025-05-03 09:55:16,383:INFO:Declaring metric variables
2025-05-03 09:55:16,391:INFO:Importing untrained model
2025-05-03 09:55:16,399:INFO:Linear Discriminant Analysis Imported successfully
2025-05-03 09:55:16,410:INFO:Starting cross validation
2025-05-03 09:55:16,411:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:16,555:INFO:Calculating mean and std
2025-05-03 09:55:16,556:INFO:Creating metrics dataframe
2025-05-03 09:55:16,559:INFO:Uploading results into container
2025-05-03 09:55:16,559:INFO:Uploading model into container now
2025-05-03 09:55:16,560:INFO:_master_model_container: 11
2025-05-03 09:55:16,560:INFO:_display_container: 2
2025-05-03 09:55:16,561:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-05-03 09:55:16,561:INFO:create_model() successfully completed......................................
2025-05-03 09:55:16,732:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:16,732:INFO:Creating metrics dataframe
2025-05-03 09:55:16,760:INFO:Initializing Extra Trees Classifier
2025-05-03 09:55:16,762:INFO:Total runtime is 0.30706848303476975 minutes
2025-05-03 09:55:16,769:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:16,769:INFO:Initializing create_model()
2025-05-03 09:55:16,769:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:16,770:INFO:Checking exceptions
2025-05-03 09:55:16,770:INFO:Importing libraries
2025-05-03 09:55:16,770:INFO:Copying training dataset
2025-05-03 09:55:16,813:INFO:Defining folds
2025-05-03 09:55:16,813:INFO:Declaring metric variables
2025-05-03 09:55:16,820:INFO:Importing untrained model
2025-05-03 09:55:16,824:INFO:Extra Trees Classifier Imported successfully
2025-05-03 09:55:16,836:INFO:Starting cross validation
2025-05-03 09:55:16,837:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:18,234:INFO:Calculating mean and std
2025-05-03 09:55:18,236:INFO:Creating metrics dataframe
2025-05-03 09:55:18,238:INFO:Uploading results into container
2025-05-03 09:55:18,238:INFO:Uploading model into container now
2025-05-03 09:55:18,239:INFO:_master_model_container: 12
2025-05-03 09:55:18,239:INFO:_display_container: 2
2025-05-03 09:55:18,240:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 09:55:18,240:INFO:create_model() successfully completed......................................
2025-05-03 09:55:18,371:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:18,371:INFO:Creating metrics dataframe
2025-05-03 09:55:18,381:INFO:Initializing Extreme Gradient Boosting
2025-05-03 09:55:18,381:INFO:Total runtime is 0.33405207395553593 minutes
2025-05-03 09:55:18,385:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:18,387:INFO:Initializing create_model()
2025-05-03 09:55:18,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:18,387:INFO:Checking exceptions
2025-05-03 09:55:18,387:INFO:Importing libraries
2025-05-03 09:55:18,387:INFO:Copying training dataset
2025-05-03 09:55:18,416:INFO:Defining folds
2025-05-03 09:55:18,416:INFO:Declaring metric variables
2025-05-03 09:55:18,420:INFO:Importing untrained model
2025-05-03 09:55:18,427:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 09:55:18,440:INFO:Starting cross validation
2025-05-03 09:55:18,441:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:18,940:INFO:Calculating mean and std
2025-05-03 09:55:18,942:INFO:Creating metrics dataframe
2025-05-03 09:55:18,945:INFO:Uploading results into container
2025-05-03 09:55:18,946:INFO:Uploading model into container now
2025-05-03 09:55:18,947:INFO:_master_model_container: 13
2025-05-03 09:55:18,947:INFO:_display_container: 2
2025-05-03 09:55:18,950:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 09:55:18,950:INFO:create_model() successfully completed......................................
2025-05-03 09:55:19,154:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:19,154:INFO:Creating metrics dataframe
2025-05-03 09:55:19,194:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 09:55:19,194:INFO:Total runtime is 0.347603181997935 minutes
2025-05-03 09:55:19,202:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:19,202:INFO:Initializing create_model()
2025-05-03 09:55:19,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:19,203:INFO:Checking exceptions
2025-05-03 09:55:19,203:INFO:Importing libraries
2025-05-03 09:55:19,203:INFO:Copying training dataset
2025-05-03 09:55:19,242:INFO:Defining folds
2025-05-03 09:55:19,243:INFO:Declaring metric variables
2025-05-03 09:55:19,247:INFO:Importing untrained model
2025-05-03 09:55:19,251:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 09:55:19,257:INFO:Starting cross validation
2025-05-03 09:55:19,259:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:19,810:INFO:Calculating mean and std
2025-05-03 09:55:19,812:INFO:Creating metrics dataframe
2025-05-03 09:55:19,814:INFO:Uploading results into container
2025-05-03 09:55:19,815:INFO:Uploading model into container now
2025-05-03 09:55:19,815:INFO:_master_model_container: 14
2025-05-03 09:55:19,815:INFO:_display_container: 2
2025-05-03 09:55:19,816:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 09:55:19,817:INFO:create_model() successfully completed......................................
2025-05-03 09:55:19,968:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:19,969:INFO:Creating metrics dataframe
2025-05-03 09:55:19,989:INFO:Initializing Dummy Classifier
2025-05-03 09:55:19,989:INFO:Total runtime is 0.36086517572402954 minutes
2025-05-03 09:55:19,996:INFO:SubProcess create_model() called ==================================
2025-05-03 09:55:19,997:INFO:Initializing create_model()
2025-05-03 09:55:19,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=dummy, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B9FF7CDB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:19,997:INFO:Checking exceptions
2025-05-03 09:55:19,997:INFO:Importing libraries
2025-05-03 09:55:19,997:INFO:Copying training dataset
2025-05-03 09:55:20,030:INFO:Defining folds
2025-05-03 09:55:20,030:INFO:Declaring metric variables
2025-05-03 09:55:20,036:INFO:Importing untrained model
2025-05-03 09:55:20,043:INFO:Dummy Classifier Imported successfully
2025-05-03 09:55:20,052:INFO:Starting cross validation
2025-05-03 09:55:20,054:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 09:55:20,106:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-03 09:55:20,119:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-03 09:55:20,121:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-03 09:55:20,160:INFO:Calculating mean and std
2025-05-03 09:55:20,162:INFO:Creating metrics dataframe
2025-05-03 09:55:20,173:INFO:Uploading results into container
2025-05-03 09:55:20,178:INFO:Uploading model into container now
2025-05-03 09:55:20,181:INFO:_master_model_container: 15
2025-05-03 09:55:20,181:INFO:_display_container: 2
2025-05-03 09:55:20,181:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-05-03 09:55:20,181:INFO:create_model() successfully completed......................................
2025-05-03 09:55:20,369:INFO:SubProcess create_model() end ==================================
2025-05-03 09:55:20,369:INFO:Creating metrics dataframe
2025-05-03 09:55:20,391:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 09:55:20,412:INFO:Initializing create_model()
2025-05-03 09:55:20,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B9FED570D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 09:55:20,412:INFO:Checking exceptions
2025-05-03 09:55:20,417:INFO:Importing libraries
2025-05-03 09:55:20,417:INFO:Copying training dataset
2025-05-03 09:55:20,453:INFO:Defining folds
2025-05-03 09:55:20,454:INFO:Declaring metric variables
2025-05-03 09:55:20,454:INFO:Importing untrained model
2025-05-03 09:55:20,454:INFO:Declaring custom model
2025-05-03 09:55:20,457:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 09:55:20,459:INFO:Cross validation set to False
2025-05-03 09:55:20,460:INFO:Fitting Model
2025-05-03 09:55:20,499:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 09:55:20,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001989 seconds.
2025-05-03 09:55:20,505:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 09:55:20,505:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 09:55:20,505:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 09:55:20,506:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 09:55:20,506:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 09:55:20,507:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 09:55:20,698:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 09:55:20,698:INFO:create_model() successfully completed......................................
2025-05-03 09:55:20,873:INFO:_master_model_container: 15
2025-05-03 09:55:20,875:INFO:_display_container: 2
2025-05-03 09:55:20,877:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 09:55:20,877:INFO:compare_models() successfully completed......................................
2025-05-03 10:02:41,534:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:02:41,534:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:02:41,534:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:02:41,534:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:05:10,412:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:05:10,412:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:05:10,412:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:05:10,412:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:05:13,082:INFO:PyCaret ClassificationExperiment
2025-05-03 10:05:13,082:INFO:Logging name: clf-default-name
2025-05-03 10:05:13,082:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 10:05:13,082:INFO:version 3.3.2
2025-05-03 10:05:13,082:INFO:Initializing setup()
2025-05-03 10:05:13,082:INFO:self.USI: cdfa
2025-05-03 10:05:13,082:INFO:self._variable_keys: {'fold_groups_param', '_available_plots', 'fold_generator', 'idx', 'data', 'X', 'html_param', 'memory', 'fold_shuffle_param', 'fix_imbalance', 'X_train', 'USI', 'X_test', 'gpu_param', '_ml_usecase', 'n_jobs_param', 'target_param', 'pipeline', 'gpu_n_jobs_param', 'y_train', 'is_multiclass', 'log_plots_param', 'y', 'seed', 'exp_id', 'logging_param', 'exp_name_log', 'y_test'}
2025-05-03 10:05:13,082:INFO:Checking environment
2025-05-03 10:05:13,082:INFO:python_version: 3.11.11
2025-05-03 10:05:13,083:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 10:05:13,083:INFO:machine: AMD64
2025-05-03 10:05:13,083:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 10:05:13,085:INFO:Memory: svmem(total=16965230592, available=3546955776, percent=79.1, used=13418274816, free=3546955776)
2025-05-03 10:05:13,085:INFO:Physical Core: 4
2025-05-03 10:05:13,085:INFO:Logical Core: 8
2025-05-03 10:05:13,085:INFO:Checking libraries
2025-05-03 10:05:13,085:INFO:System:
2025-05-03 10:05:13,085:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 10:05:13,085:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 10:05:13,085:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 10:05:13,085:INFO:PyCaret required dependencies:
2025-05-03 10:05:13,085:INFO:                 pip: 25.0
2025-05-03 10:05:13,085:INFO:          setuptools: 75.8.0
2025-05-03 10:05:13,085:INFO:             pycaret: 3.3.2
2025-05-03 10:05:13,085:INFO:             IPython: 8.32.0
2025-05-03 10:05:13,085:INFO:          ipywidgets: 8.1.6
2025-05-03 10:05:13,085:INFO:                tqdm: 4.67.1
2025-05-03 10:05:13,085:INFO:               numpy: 1.26.4
2025-05-03 10:05:13,085:INFO:              pandas: 2.1.4
2025-05-03 10:05:13,085:INFO:              jinja2: 3.1.6
2025-05-03 10:05:13,085:INFO:               scipy: 1.11.4
2025-05-03 10:05:13,085:INFO:              joblib: 1.3.2
2025-05-03 10:05:13,085:INFO:             sklearn: 1.4.2
2025-05-03 10:05:13,085:INFO:                pyod: 2.0.5
2025-05-03 10:05:13,085:INFO:            imblearn: 0.13.0
2025-05-03 10:05:13,085:INFO:   category_encoders: 2.7.0
2025-05-03 10:05:13,085:INFO:            lightgbm: 4.6.0
2025-05-03 10:05:13,085:INFO:               numba: 0.61.2
2025-05-03 10:05:13,085:INFO:            requests: 2.32.3
2025-05-03 10:05:13,085:INFO:          matplotlib: 3.7.5
2025-05-03 10:05:13,085:INFO:          scikitplot: 0.3.7
2025-05-03 10:05:13,085:INFO:         yellowbrick: 1.5
2025-05-03 10:05:13,085:INFO:              plotly: 5.24.1
2025-05-03 10:05:13,085:INFO:    plotly-resampler: Not installed
2025-05-03 10:05:13,085:INFO:             kaleido: 0.2.1
2025-05-03 10:05:13,085:INFO:           schemdraw: 0.15
2025-05-03 10:05:13,085:INFO:         statsmodels: 0.14.4
2025-05-03 10:05:13,085:INFO:              sktime: 0.26.0
2025-05-03 10:05:13,085:INFO:               tbats: 1.1.3
2025-05-03 10:05:13,085:INFO:            pmdarima: 2.0.4
2025-05-03 10:05:13,085:INFO:              psutil: 6.1.1
2025-05-03 10:05:13,085:INFO:          markupsafe: 3.0.2
2025-05-03 10:05:13,085:INFO:             pickle5: Not installed
2025-05-03 10:05:13,085:INFO:         cloudpickle: 3.1.1
2025-05-03 10:05:13,085:INFO:         deprecation: 2.1.0
2025-05-03 10:05:13,085:INFO:              xxhash: 3.5.0
2025-05-03 10:05:13,085:INFO:           wurlitzer: Not installed
2025-05-03 10:05:13,085:INFO:PyCaret optional dependencies:
2025-05-03 10:05:13,329:INFO:                shap: 0.47.2
2025-05-03 10:05:13,329:INFO:           interpret: Not installed
2025-05-03 10:05:13,329:INFO:                umap: Not installed
2025-05-03 10:05:13,329:INFO:     ydata_profiling: Not installed
2025-05-03 10:05:13,329:INFO:  explainerdashboard: Not installed
2025-05-03 10:05:13,329:INFO:             autoviz: Not installed
2025-05-03 10:05:13,329:INFO:           fairlearn: Not installed
2025-05-03 10:05:13,329:INFO:          deepchecks: Not installed
2025-05-03 10:05:13,329:INFO:             xgboost: 3.0.0
2025-05-03 10:05:13,329:INFO:            catboost: Not installed
2025-05-03 10:05:13,329:INFO:              kmodes: Not installed
2025-05-03 10:05:13,329:INFO:             mlxtend: Not installed
2025-05-03 10:05:13,329:INFO:       statsforecast: Not installed
2025-05-03 10:05:13,329:INFO:        tune_sklearn: Not installed
2025-05-03 10:05:13,329:INFO:                 ray: Not installed
2025-05-03 10:05:13,329:INFO:            hyperopt: 0.2.7
2025-05-03 10:05:13,329:INFO:              optuna: Not installed
2025-05-03 10:05:13,329:INFO:               skopt: 0.10.2
2025-05-03 10:05:13,329:INFO:              mlflow: 2.22.0
2025-05-03 10:05:13,329:INFO:              gradio: Not installed
2025-05-03 10:05:13,329:INFO:             fastapi: 0.115.12
2025-05-03 10:05:13,329:INFO:             uvicorn: 0.34.2
2025-05-03 10:05:13,329:INFO:              m2cgen: Not installed
2025-05-03 10:05:13,329:INFO:           evidently: Not installed
2025-05-03 10:05:13,329:INFO:               fugue: Not installed
2025-05-03 10:05:13,329:INFO:           streamlit: Not installed
2025-05-03 10:05:13,329:INFO:             prophet: Not installed
2025-05-03 10:05:13,329:INFO:None
2025-05-03 10:05:13,329:INFO:Set up data.
2025-05-03 10:05:13,343:INFO:Set up folding strategy.
2025-05-03 10:05:13,343:INFO:Set up train/test split.
2025-05-03 10:05:13,359:INFO:Set up index.
2025-05-03 10:05:13,359:INFO:Assigning column types.
2025-05-03 10:05:13,376:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 10:05:13,410:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,413:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,428:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,428:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,459:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,459:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,492:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,492:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,492:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 10:05:13,526:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,545:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,545:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,576:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:05:13,611:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,613:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,613:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 10:05:13,659:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,659:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,725:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,741:INFO:Finished creating preprocessing pipeline.
2025-05-03 10:05:13,743:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 10:05:13,743:INFO:Creating final display dataframe.
2025-05-03 10:05:13,809:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 10:05:13,859:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,874:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,926:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:05:13,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:05:13,926:INFO:setup() successfully completed in 0.85s...............
2025-05-03 10:05:13,942:INFO:Initializing compare_models()
2025-05-03 10:05:13,942:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'catboost', 'et', 'mlp', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'catboost', 'et', 'mlp', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:05:13,942:INFO:Checking exceptions
2025-05-03 10:05:41,355:INFO:Initializing compare_models()
2025-05-03 10:05:41,355:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'mlp', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'mlp', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:05:41,355:INFO:Checking exceptions
2025-05-03 10:05:41,373:INFO:Preparing display monitor
2025-05-03 10:05:41,400:INFO:Initializing Logistic Regression
2025-05-03 10:05:41,401:INFO:Total runtime is 2.195437749226888e-05 minutes
2025-05-03 10:05:41,405:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:41,405:INFO:Initializing create_model()
2025-05-03 10:05:41,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:41,406:INFO:Checking exceptions
2025-05-03 10:05:41,406:INFO:Importing libraries
2025-05-03 10:05:41,406:INFO:Copying training dataset
2025-05-03 10:05:41,423:INFO:Defining folds
2025-05-03 10:05:41,423:INFO:Declaring metric variables
2025-05-03 10:05:41,426:INFO:Importing untrained model
2025-05-03 10:05:41,430:INFO:Logistic Regression Imported successfully
2025-05-03 10:05:41,436:INFO:Starting cross validation
2025-05-03 10:05:41,436:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:05:44,526:INFO:Calculating mean and std
2025-05-03 10:05:44,527:INFO:Creating metrics dataframe
2025-05-03 10:05:44,530:INFO:Uploading results into container
2025-05-03 10:05:44,531:INFO:Uploading model into container now
2025-05-03 10:05:44,531:INFO:_master_model_container: 1
2025-05-03 10:05:44,531:INFO:_display_container: 2
2025-05-03 10:05:44,532:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 10:05:44,532:INFO:create_model() successfully completed......................................
2025-05-03 10:05:44,668:INFO:SubProcess create_model() end ==================================
2025-05-03 10:05:44,668:INFO:Creating metrics dataframe
2025-05-03 10:05:44,668:INFO:Initializing Random Forest Classifier
2025-05-03 10:05:44,668:INFO:Total runtime is 0.05447080930074056 minutes
2025-05-03 10:05:44,668:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:44,668:INFO:Initializing create_model()
2025-05-03 10:05:44,668:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:44,668:INFO:Checking exceptions
2025-05-03 10:05:44,668:INFO:Importing libraries
2025-05-03 10:05:44,668:INFO:Copying training dataset
2025-05-03 10:05:44,685:INFO:Defining folds
2025-05-03 10:05:44,685:INFO:Declaring metric variables
2025-05-03 10:05:44,685:INFO:Importing untrained model
2025-05-03 10:05:44,701:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:05:44,705:INFO:Starting cross validation
2025-05-03 10:05:44,705:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:05:48,283:INFO:Calculating mean and std
2025-05-03 10:05:48,285:INFO:Creating metrics dataframe
2025-05-03 10:05:48,287:INFO:Uploading results into container
2025-05-03 10:05:48,289:INFO:Uploading model into container now
2025-05-03 10:05:48,289:INFO:_master_model_container: 2
2025-05-03 10:05:48,289:INFO:_display_container: 2
2025-05-03 10:05:48,289:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:05:48,291:INFO:create_model() successfully completed......................................
2025-05-03 10:05:48,414:INFO:SubProcess create_model() end ==================================
2025-05-03 10:05:48,414:INFO:Creating metrics dataframe
2025-05-03 10:05:48,429:INFO:Initializing Extreme Gradient Boosting
2025-05-03 10:05:48,430:INFO:Total runtime is 0.11716742912928263 minutes
2025-05-03 10:05:48,432:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:48,432:INFO:Initializing create_model()
2025-05-03 10:05:48,432:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:48,432:INFO:Checking exceptions
2025-05-03 10:05:48,432:INFO:Importing libraries
2025-05-03 10:05:48,432:INFO:Copying training dataset
2025-05-03 10:05:48,450:INFO:Defining folds
2025-05-03 10:05:48,452:INFO:Declaring metric variables
2025-05-03 10:05:48,454:INFO:Importing untrained model
2025-05-03 10:05:48,457:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:05:48,465:INFO:Starting cross validation
2025-05-03 10:05:48,465:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:05:51,025:INFO:Calculating mean and std
2025-05-03 10:05:51,025:INFO:Creating metrics dataframe
2025-05-03 10:05:51,029:INFO:Uploading results into container
2025-05-03 10:05:51,029:INFO:Uploading model into container now
2025-05-03 10:05:51,029:INFO:_master_model_container: 3
2025-05-03 10:05:51,029:INFO:_display_container: 2
2025-05-03 10:05:51,029:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:05:51,031:INFO:create_model() successfully completed......................................
2025-05-03 10:05:51,145:INFO:SubProcess create_model() end ==================================
2025-05-03 10:05:51,145:INFO:Creating metrics dataframe
2025-05-03 10:05:51,145:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 10:05:51,145:INFO:Total runtime is 0.1624249259630839 minutes
2025-05-03 10:05:51,145:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:51,145:INFO:Initializing create_model()
2025-05-03 10:05:51,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:51,145:INFO:Checking exceptions
2025-05-03 10:05:51,145:INFO:Importing libraries
2025-05-03 10:05:51,145:INFO:Copying training dataset
2025-05-03 10:05:51,165:INFO:Defining folds
2025-05-03 10:05:51,165:INFO:Declaring metric variables
2025-05-03 10:05:51,180:INFO:Importing untrained model
2025-05-03 10:05:51,183:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:05:51,183:INFO:Starting cross validation
2025-05-03 10:05:51,183:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:05:51,642:INFO:Calculating mean and std
2025-05-03 10:05:51,644:INFO:Creating metrics dataframe
2025-05-03 10:05:51,647:INFO:Uploading results into container
2025-05-03 10:05:51,648:INFO:Uploading model into container now
2025-05-03 10:05:51,648:INFO:_master_model_container: 4
2025-05-03 10:05:51,648:INFO:_display_container: 2
2025-05-03 10:05:51,648:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:05:51,648:INFO:create_model() successfully completed......................................
2025-05-03 10:05:51,779:INFO:SubProcess create_model() end ==================================
2025-05-03 10:05:51,779:INFO:Creating metrics dataframe
2025-05-03 10:05:51,795:INFO:Initializing Extra Trees Classifier
2025-05-03 10:05:51,795:INFO:Total runtime is 0.17325569788614908 minutes
2025-05-03 10:05:51,798:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:51,798:INFO:Initializing create_model()
2025-05-03 10:05:51,798:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:51,798:INFO:Checking exceptions
2025-05-03 10:05:51,798:INFO:Importing libraries
2025-05-03 10:05:51,798:INFO:Copying training dataset
2025-05-03 10:05:51,813:INFO:Defining folds
2025-05-03 10:05:51,813:INFO:Declaring metric variables
2025-05-03 10:05:51,817:INFO:Importing untrained model
2025-05-03 10:05:51,817:INFO:Extra Trees Classifier Imported successfully
2025-05-03 10:05:51,827:INFO:Starting cross validation
2025-05-03 10:05:51,828:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:05:52,866:INFO:Calculating mean and std
2025-05-03 10:05:52,866:INFO:Creating metrics dataframe
2025-05-03 10:05:52,866:INFO:Uploading results into container
2025-05-03 10:05:52,866:INFO:Uploading model into container now
2025-05-03 10:05:52,870:INFO:_master_model_container: 5
2025-05-03 10:05:52,870:INFO:_display_container: 2
2025-05-03 10:05:52,870:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 10:05:52,870:INFO:create_model() successfully completed......................................
2025-05-03 10:05:52,993:INFO:SubProcess create_model() end ==================================
2025-05-03 10:05:52,994:INFO:Creating metrics dataframe
2025-05-03 10:05:53,001:INFO:Initializing MLP Classifier
2025-05-03 10:05:53,001:INFO:Total runtime is 0.19334584871927896 minutes
2025-05-03 10:05:53,004:INFO:SubProcess create_model() called ==================================
2025-05-03 10:05:53,004:INFO:Initializing create_model()
2025-05-03 10:05:53,004:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=mlp, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:05:53,005:INFO:Checking exceptions
2025-05-03 10:05:53,005:INFO:Importing libraries
2025-05-03 10:05:53,005:INFO:Copying training dataset
2025-05-03 10:05:53,021:INFO:Defining folds
2025-05-03 10:05:53,021:INFO:Declaring metric variables
2025-05-03 10:05:53,025:INFO:Importing untrained model
2025-05-03 10:05:53,029:INFO:MLP Classifier Imported successfully
2025-05-03 10:05:53,033:INFO:Starting cross validation
2025-05-03 10:05:53,033:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:06:01,142:INFO:Calculating mean and std
2025-05-03 10:06:01,142:INFO:Creating metrics dataframe
2025-05-03 10:06:01,142:INFO:Uploading results into container
2025-05-03 10:06:01,142:INFO:Uploading model into container now
2025-05-03 10:06:01,142:INFO:_master_model_container: 6
2025-05-03 10:06:01,142:INFO:_display_container: 2
2025-05-03 10:06:01,142:INFO:MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_fun=15000, max_iter=500,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=42, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)
2025-05-03 10:06:01,142:INFO:create_model() successfully completed......................................
2025-05-03 10:06:01,254:INFO:SubProcess create_model() end ==================================
2025-05-03 10:06:01,254:INFO:Creating metrics dataframe
2025-05-03 10:06:01,254:INFO:Initializing Ridge Classifier
2025-05-03 10:06:01,254:INFO:Total runtime is 0.33089743852615355 minutes
2025-05-03 10:06:01,266:INFO:SubProcess create_model() called ==================================
2025-05-03 10:06:01,266:INFO:Initializing create_model()
2025-05-03 10:06:01,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C3AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:06:01,266:INFO:Checking exceptions
2025-05-03 10:06:01,269:INFO:Importing libraries
2025-05-03 10:06:01,269:INFO:Copying training dataset
2025-05-03 10:06:01,287:INFO:Defining folds
2025-05-03 10:06:01,287:INFO:Declaring metric variables
2025-05-03 10:06:01,291:INFO:Importing untrained model
2025-05-03 10:06:01,295:INFO:Ridge Classifier Imported successfully
2025-05-03 10:06:01,295:INFO:Starting cross validation
2025-05-03 10:06:01,295:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:06:01,370:INFO:Calculating mean and std
2025-05-03 10:06:01,370:INFO:Creating metrics dataframe
2025-05-03 10:06:01,370:INFO:Uploading results into container
2025-05-03 10:06:01,370:INFO:Uploading model into container now
2025-05-03 10:06:01,370:INFO:_master_model_container: 7
2025-05-03 10:06:01,370:INFO:_display_container: 2
2025-05-03 10:06:01,370:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 10:06:01,370:INFO:create_model() successfully completed......................................
2025-05-03 10:06:01,486:INFO:SubProcess create_model() end ==================================
2025-05-03 10:06:01,486:INFO:Creating metrics dataframe
2025-05-03 10:06:01,493:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 10:06:01,501:INFO:Initializing create_model()
2025-05-03 10:06:01,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:06:01,502:INFO:Checking exceptions
2025-05-03 10:06:01,504:INFO:Importing libraries
2025-05-03 10:06:01,504:INFO:Copying training dataset
2025-05-03 10:06:01,504:INFO:Defining folds
2025-05-03 10:06:01,504:INFO:Declaring metric variables
2025-05-03 10:06:01,504:INFO:Importing untrained model
2025-05-03 10:06:01,504:INFO:Declaring custom model
2025-05-03 10:06:01,504:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:06:01,504:INFO:Cross validation set to False
2025-05-03 10:06:01,504:INFO:Fitting Model
2025-05-03 10:06:01,535:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:06:01,538:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.
2025-05-03 10:06:01,538:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:06:01,538:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:06:01,538:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:06:01,538:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:06:01,539:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:06:01,539:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:06:01,635:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:06:01,635:INFO:create_model() successfully completed......................................
2025-05-03 10:06:01,788:INFO:_master_model_container: 7
2025-05-03 10:06:01,788:INFO:_display_container: 2
2025-05-03 10:06:01,788:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:06:01,788:INFO:compare_models() successfully completed......................................
2025-05-03 10:09:20,850:INFO:Initializing compare_models()
2025-05-03 10:09:20,850:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:09:20,850:INFO:Checking exceptions
2025-05-03 10:09:20,866:INFO:Preparing display monitor
2025-05-03 10:09:20,903:INFO:Initializing Logistic Regression
2025-05-03 10:09:20,903:INFO:Total runtime is 0.0 minutes
2025-05-03 10:09:20,903:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:20,903:INFO:Initializing create_model()
2025-05-03 10:09:20,903:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:20,903:INFO:Checking exceptions
2025-05-03 10:09:20,903:INFO:Importing libraries
2025-05-03 10:09:20,903:INFO:Copying training dataset
2025-05-03 10:09:20,946:INFO:Defining folds
2025-05-03 10:09:20,946:INFO:Declaring metric variables
2025-05-03 10:09:20,946:INFO:Importing untrained model
2025-05-03 10:09:20,964:INFO:Logistic Regression Imported successfully
2025-05-03 10:09:20,979:INFO:Starting cross validation
2025-05-03 10:09:20,979:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:21,262:INFO:Calculating mean and std
2025-05-03 10:09:21,263:INFO:Creating metrics dataframe
2025-05-03 10:09:21,267:INFO:Uploading results into container
2025-05-03 10:09:21,268:INFO:Uploading model into container now
2025-05-03 10:09:21,268:INFO:_master_model_container: 8
2025-05-03 10:09:21,269:INFO:_display_container: 3
2025-05-03 10:09:21,269:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 10:09:21,269:INFO:create_model() successfully completed......................................
2025-05-03 10:09:21,512:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:21,514:INFO:Creating metrics dataframe
2025-05-03 10:09:21,524:INFO:Initializing Random Forest Classifier
2025-05-03 10:09:21,524:INFO:Total runtime is 0.010347278912862141 minutes
2025-05-03 10:09:21,528:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:21,528:INFO:Initializing create_model()
2025-05-03 10:09:21,528:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:21,528:INFO:Checking exceptions
2025-05-03 10:09:21,528:INFO:Importing libraries
2025-05-03 10:09:21,528:INFO:Copying training dataset
2025-05-03 10:09:21,560:INFO:Defining folds
2025-05-03 10:09:21,560:INFO:Declaring metric variables
2025-05-03 10:09:21,560:INFO:Importing untrained model
2025-05-03 10:09:21,576:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:09:21,576:INFO:Starting cross validation
2025-05-03 10:09:21,576:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:22,821:INFO:Calculating mean and std
2025-05-03 10:09:22,821:INFO:Creating metrics dataframe
2025-05-03 10:09:22,821:INFO:Uploading results into container
2025-05-03 10:09:22,821:INFO:Uploading model into container now
2025-05-03 10:09:22,821:INFO:_master_model_container: 9
2025-05-03 10:09:22,821:INFO:_display_container: 3
2025-05-03 10:09:22,821:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:09:22,821:INFO:create_model() successfully completed......................................
2025-05-03 10:09:22,998:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:23,015:INFO:Creating metrics dataframe
2025-05-03 10:09:23,030:INFO:Initializing Extreme Gradient Boosting
2025-05-03 10:09:23,030:INFO:Total runtime is 0.03545024394989013 minutes
2025-05-03 10:09:23,037:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:23,037:INFO:Initializing create_model()
2025-05-03 10:09:23,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:23,037:INFO:Checking exceptions
2025-05-03 10:09:23,037:INFO:Importing libraries
2025-05-03 10:09:23,037:INFO:Copying training dataset
2025-05-03 10:09:23,078:INFO:Defining folds
2025-05-03 10:09:23,078:INFO:Declaring metric variables
2025-05-03 10:09:23,078:INFO:Importing untrained model
2025-05-03 10:09:23,086:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:09:23,102:INFO:Starting cross validation
2025-05-03 10:09:23,102:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:23,424:INFO:Calculating mean and std
2025-05-03 10:09:23,425:INFO:Creating metrics dataframe
2025-05-03 10:09:23,428:INFO:Uploading results into container
2025-05-03 10:09:23,428:INFO:Uploading model into container now
2025-05-03 10:09:23,429:INFO:_master_model_container: 10
2025-05-03 10:09:23,429:INFO:_display_container: 3
2025-05-03 10:09:23,430:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:09:23,431:INFO:create_model() successfully completed......................................
2025-05-03 10:09:23,537:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:23,537:INFO:Creating metrics dataframe
2025-05-03 10:09:23,554:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 10:09:23,554:INFO:Total runtime is 0.04417498111724853 minutes
2025-05-03 10:09:23,554:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:23,554:INFO:Initializing create_model()
2025-05-03 10:09:23,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:23,554:INFO:Checking exceptions
2025-05-03 10:09:23,554:INFO:Importing libraries
2025-05-03 10:09:23,554:INFO:Copying training dataset
2025-05-03 10:09:23,577:INFO:Defining folds
2025-05-03 10:09:23,577:INFO:Declaring metric variables
2025-05-03 10:09:23,580:INFO:Importing untrained model
2025-05-03 10:09:23,586:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:09:23,591:INFO:Starting cross validation
2025-05-03 10:09:23,591:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:24,074:INFO:Calculating mean and std
2025-05-03 10:09:24,075:INFO:Creating metrics dataframe
2025-05-03 10:09:24,078:INFO:Uploading results into container
2025-05-03 10:09:24,078:INFO:Uploading model into container now
2025-05-03 10:09:24,078:INFO:_master_model_container: 11
2025-05-03 10:09:24,078:INFO:_display_container: 3
2025-05-03 10:09:24,080:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:09:24,080:INFO:create_model() successfully completed......................................
2025-05-03 10:09:24,204:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:24,204:INFO:Creating metrics dataframe
2025-05-03 10:09:24,204:INFO:Initializing Extra Trees Classifier
2025-05-03 10:09:24,204:INFO:Total runtime is 0.05501683155695597 minutes
2025-05-03 10:09:24,204:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:24,204:INFO:Initializing create_model()
2025-05-03 10:09:24,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:24,204:INFO:Checking exceptions
2025-05-03 10:09:24,204:INFO:Importing libraries
2025-05-03 10:09:24,204:INFO:Copying training dataset
2025-05-03 10:09:24,220:INFO:Defining folds
2025-05-03 10:09:24,220:INFO:Declaring metric variables
2025-05-03 10:09:24,237:INFO:Importing untrained model
2025-05-03 10:09:24,240:INFO:Extra Trees Classifier Imported successfully
2025-05-03 10:09:24,240:INFO:Starting cross validation
2025-05-03 10:09:24,240:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:25,206:INFO:Calculating mean and std
2025-05-03 10:09:25,206:INFO:Creating metrics dataframe
2025-05-03 10:09:25,206:INFO:Uploading results into container
2025-05-03 10:09:25,210:INFO:Uploading model into container now
2025-05-03 10:09:25,210:INFO:_master_model_container: 12
2025-05-03 10:09:25,210:INFO:_display_container: 3
2025-05-03 10:09:25,211:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 10:09:25,211:INFO:create_model() successfully completed......................................
2025-05-03 10:09:25,326:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:25,326:INFO:Creating metrics dataframe
2025-05-03 10:09:25,355:INFO:Initializing Ridge Classifier
2025-05-03 10:09:25,355:INFO:Total runtime is 0.07419914404551188 minutes
2025-05-03 10:09:25,359:INFO:SubProcess create_model() called ==================================
2025-05-03 10:09:25,359:INFO:Initializing create_model()
2025-05-03 10:09:25,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369DA25390>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:25,359:INFO:Checking exceptions
2025-05-03 10:09:25,359:INFO:Importing libraries
2025-05-03 10:09:25,359:INFO:Copying training dataset
2025-05-03 10:09:25,393:INFO:Defining folds
2025-05-03 10:09:25,393:INFO:Declaring metric variables
2025-05-03 10:09:25,395:INFO:Importing untrained model
2025-05-03 10:09:25,395:INFO:Ridge Classifier Imported successfully
2025-05-03 10:09:25,406:INFO:Starting cross validation
2025-05-03 10:09:25,407:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:09:25,463:INFO:Calculating mean and std
2025-05-03 10:09:25,463:INFO:Creating metrics dataframe
2025-05-03 10:09:25,463:INFO:Uploading results into container
2025-05-03 10:09:25,463:INFO:Uploading model into container now
2025-05-03 10:09:25,463:INFO:_master_model_container: 13
2025-05-03 10:09:25,463:INFO:_display_container: 3
2025-05-03 10:09:25,463:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 10:09:25,463:INFO:create_model() successfully completed......................................
2025-05-03 10:09:25,569:INFO:SubProcess create_model() end ==================================
2025-05-03 10:09:25,569:INFO:Creating metrics dataframe
2025-05-03 10:09:25,585:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 10:09:25,593:INFO:Initializing create_model()
2025-05-03 10:09:25,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:09:25,593:INFO:Checking exceptions
2025-05-03 10:09:25,593:INFO:Importing libraries
2025-05-03 10:09:25,593:INFO:Copying training dataset
2025-05-03 10:09:25,609:INFO:Defining folds
2025-05-03 10:09:25,609:INFO:Declaring metric variables
2025-05-03 10:09:25,609:INFO:Importing untrained model
2025-05-03 10:09:25,609:INFO:Declaring custom model
2025-05-03 10:09:25,609:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:09:25,609:INFO:Cross validation set to False
2025-05-03 10:09:25,609:INFO:Fitting Model
2025-05-03 10:09:25,629:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:09:25,631:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000812 seconds.
2025-05-03 10:09:25,631:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:09:25,631:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:09:25,631:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:09:25,631:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:09:25,633:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:09:25,633:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:09:25,756:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:09:25,756:INFO:create_model() successfully completed......................................
2025-05-03 10:09:25,915:INFO:_master_model_container: 13
2025-05-03 10:09:25,915:INFO:_display_container: 3
2025-05-03 10:09:25,915:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:09:25,918:INFO:compare_models() successfully completed......................................
2025-05-03 10:10:00,548:INFO:Initializing compare_models()
2025-05-03 10:10:00,548:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, include=['log_reg', 'rand_for', 'xgboost', 'lightgbm', 'extr_trees', 'ridge_clf'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, 'include': ['log_reg', 'rand_for', 'xgboost', 'lightgbm', 'extr_trees', 'ridge_clf'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:10:00,548:INFO:Checking exceptions
2025-05-03 10:10:23,811:INFO:Initializing compare_models()
2025-05-03 10:10:23,811:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:10:23,813:INFO:Checking exceptions
2025-05-03 10:10:23,822:INFO:Preparing display monitor
2025-05-03 10:10:23,843:INFO:Initializing Logistic Regression
2025-05-03 10:10:23,843:INFO:Total runtime is 0.0 minutes
2025-05-03 10:10:23,847:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:23,848:INFO:Initializing create_model()
2025-05-03 10:10:23,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:23,848:INFO:Checking exceptions
2025-05-03 10:10:23,848:INFO:Importing libraries
2025-05-03 10:10:23,848:INFO:Copying training dataset
2025-05-03 10:10:23,876:INFO:Defining folds
2025-05-03 10:10:23,876:INFO:Declaring metric variables
2025-05-03 10:10:23,895:INFO:Importing untrained model
2025-05-03 10:10:23,900:INFO:Logistic Regression Imported successfully
2025-05-03 10:10:23,909:INFO:Starting cross validation
2025-05-03 10:10:23,910:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:24,055:INFO:Calculating mean and std
2025-05-03 10:10:24,055:INFO:Creating metrics dataframe
2025-05-03 10:10:24,057:INFO:Uploading results into container
2025-05-03 10:10:24,057:INFO:Uploading model into container now
2025-05-03 10:10:24,057:INFO:_master_model_container: 14
2025-05-03 10:10:24,057:INFO:_display_container: 4
2025-05-03 10:10:24,057:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 10:10:24,057:INFO:create_model() successfully completed......................................
2025-05-03 10:10:24,156:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:24,156:INFO:Creating metrics dataframe
2025-05-03 10:10:24,173:INFO:Initializing Random Forest Classifier
2025-05-03 10:10:24,174:INFO:Total runtime is 0.005511681238810222 minutes
2025-05-03 10:10:24,176:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:24,177:INFO:Initializing create_model()
2025-05-03 10:10:24,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:24,177:INFO:Checking exceptions
2025-05-03 10:10:24,177:INFO:Importing libraries
2025-05-03 10:10:24,177:INFO:Copying training dataset
2025-05-03 10:10:24,187:INFO:Defining folds
2025-05-03 10:10:24,187:INFO:Declaring metric variables
2025-05-03 10:10:24,187:INFO:Importing untrained model
2025-05-03 10:10:24,187:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:10:24,201:INFO:Starting cross validation
2025-05-03 10:10:24,201:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:25,288:INFO:Calculating mean and std
2025-05-03 10:10:25,288:INFO:Creating metrics dataframe
2025-05-03 10:10:25,292:INFO:Uploading results into container
2025-05-03 10:10:25,292:INFO:Uploading model into container now
2025-05-03 10:10:25,293:INFO:_master_model_container: 15
2025-05-03 10:10:25,293:INFO:_display_container: 4
2025-05-03 10:10:25,293:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:10:25,293:INFO:create_model() successfully completed......................................
2025-05-03 10:10:25,409:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:25,410:INFO:Creating metrics dataframe
2025-05-03 10:10:25,415:INFO:Initializing Extreme Gradient Boosting
2025-05-03 10:10:25,415:INFO:Total runtime is 0.026188071568806967 minutes
2025-05-03 10:10:25,417:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:25,418:INFO:Initializing create_model()
2025-05-03 10:10:25,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:25,418:INFO:Checking exceptions
2025-05-03 10:10:25,418:INFO:Importing libraries
2025-05-03 10:10:25,418:INFO:Copying training dataset
2025-05-03 10:10:25,420:INFO:Defining folds
2025-05-03 10:10:25,420:INFO:Declaring metric variables
2025-05-03 10:10:25,437:INFO:Importing untrained model
2025-05-03 10:10:25,437:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:10:25,437:INFO:Starting cross validation
2025-05-03 10:10:25,437:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:25,733:INFO:Calculating mean and std
2025-05-03 10:10:25,733:INFO:Creating metrics dataframe
2025-05-03 10:10:25,735:INFO:Uploading results into container
2025-05-03 10:10:25,736:INFO:Uploading model into container now
2025-05-03 10:10:25,736:INFO:_master_model_container: 16
2025-05-03 10:10:25,736:INFO:_display_container: 4
2025-05-03 10:10:25,736:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:10:25,736:INFO:create_model() successfully completed......................................
2025-05-03 10:10:25,838:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:25,838:INFO:Creating metrics dataframe
2025-05-03 10:10:25,852:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 10:10:25,852:INFO:Total runtime is 0.033482460180918376 minutes
2025-05-03 10:10:25,852:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:25,852:INFO:Initializing create_model()
2025-05-03 10:10:25,852:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:25,852:INFO:Checking exceptions
2025-05-03 10:10:25,852:INFO:Importing libraries
2025-05-03 10:10:25,852:INFO:Copying training dataset
2025-05-03 10:10:25,872:INFO:Defining folds
2025-05-03 10:10:25,873:INFO:Declaring metric variables
2025-05-03 10:10:25,873:INFO:Importing untrained model
2025-05-03 10:10:25,873:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:10:25,885:INFO:Starting cross validation
2025-05-03 10:10:25,886:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:26,326:INFO:Calculating mean and std
2025-05-03 10:10:26,326:INFO:Creating metrics dataframe
2025-05-03 10:10:26,330:INFO:Uploading results into container
2025-05-03 10:10:26,331:INFO:Uploading model into container now
2025-05-03 10:10:26,332:INFO:_master_model_container: 17
2025-05-03 10:10:26,332:INFO:_display_container: 4
2025-05-03 10:10:26,332:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:10:26,332:INFO:create_model() successfully completed......................................
2025-05-03 10:10:26,472:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:26,472:INFO:Creating metrics dataframe
2025-05-03 10:10:26,477:INFO:Initializing Extra Trees Classifier
2025-05-03 10:10:26,478:INFO:Total runtime is 0.04391466776529948 minutes
2025-05-03 10:10:26,480:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:26,481:INFO:Initializing create_model()
2025-05-03 10:10:26,481:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:26,481:INFO:Checking exceptions
2025-05-03 10:10:26,481:INFO:Importing libraries
2025-05-03 10:10:26,481:INFO:Copying training dataset
2025-05-03 10:10:26,490:INFO:Defining folds
2025-05-03 10:10:26,490:INFO:Declaring metric variables
2025-05-03 10:10:26,501:INFO:Importing untrained model
2025-05-03 10:10:26,504:INFO:Extra Trees Classifier Imported successfully
2025-05-03 10:10:26,507:INFO:Starting cross validation
2025-05-03 10:10:26,507:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:27,558:INFO:Calculating mean and std
2025-05-03 10:10:27,562:INFO:Creating metrics dataframe
2025-05-03 10:10:27,564:INFO:Uploading results into container
2025-05-03 10:10:27,564:INFO:Uploading model into container now
2025-05-03 10:10:27,564:INFO:_master_model_container: 18
2025-05-03 10:10:27,564:INFO:_display_container: 4
2025-05-03 10:10:27,566:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 10:10:27,567:INFO:create_model() successfully completed......................................
2025-05-03 10:10:27,697:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:27,697:INFO:Creating metrics dataframe
2025-05-03 10:10:27,704:INFO:Initializing Ridge Classifier
2025-05-03 10:10:27,704:INFO:Total runtime is 0.06434017419815063 minutes
2025-05-03 10:10:27,704:INFO:SubProcess create_model() called ==================================
2025-05-03 10:10:27,704:INFO:Initializing create_model()
2025-05-03 10:10:27,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369B276150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:27,704:INFO:Checking exceptions
2025-05-03 10:10:27,704:INFO:Importing libraries
2025-05-03 10:10:27,704:INFO:Copying training dataset
2025-05-03 10:10:27,723:INFO:Defining folds
2025-05-03 10:10:27,723:INFO:Declaring metric variables
2025-05-03 10:10:27,723:INFO:Importing untrained model
2025-05-03 10:10:27,723:INFO:Ridge Classifier Imported successfully
2025-05-03 10:10:27,738:INFO:Starting cross validation
2025-05-03 10:10:27,739:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:10:27,809:INFO:Calculating mean and std
2025-05-03 10:10:27,809:INFO:Creating metrics dataframe
2025-05-03 10:10:27,809:INFO:Uploading results into container
2025-05-03 10:10:27,809:INFO:Uploading model into container now
2025-05-03 10:10:27,809:INFO:_master_model_container: 19
2025-05-03 10:10:27,809:INFO:_display_container: 4
2025-05-03 10:10:27,809:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 10:10:27,809:INFO:create_model() successfully completed......................................
2025-05-03 10:10:27,920:INFO:SubProcess create_model() end ==================================
2025-05-03 10:10:27,920:INFO:Creating metrics dataframe
2025-05-03 10:10:27,934:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 10:10:27,944:INFO:Initializing create_model()
2025-05-03 10:10:27,944:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:10:27,944:INFO:Checking exceptions
2025-05-03 10:10:27,944:INFO:Importing libraries
2025-05-03 10:10:27,950:INFO:Copying training dataset
2025-05-03 10:10:27,955:INFO:Defining folds
2025-05-03 10:10:27,955:INFO:Declaring metric variables
2025-05-03 10:10:27,955:INFO:Importing untrained model
2025-05-03 10:10:27,955:INFO:Declaring custom model
2025-05-03 10:10:27,967:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:10:27,967:INFO:Cross validation set to False
2025-05-03 10:10:27,968:INFO:Fitting Model
2025-05-03 10:10:27,984:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:10:27,986:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.
2025-05-03 10:10:27,986:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:10:27,986:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:10:27,987:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:10:27,987:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:10:27,987:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:10:27,987:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:10:28,137:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:10:28,137:INFO:create_model() successfully completed......................................
2025-05-03 10:10:28,302:INFO:_master_model_container: 19
2025-05-03 10:10:28,302:INFO:_display_container: 4
2025-05-03 10:10:28,302:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:10:28,302:INFO:compare_models() successfully completed......................................
2025-05-03 10:11:48,616:INFO:Initializing tune_model()
2025-05-03 10:11:48,617:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lightgbm, fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:11:48,617:INFO:Checking exceptions
2025-05-03 10:13:04,027:INFO:Initializing create_model()
2025-05-03 10:13:04,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:13:04,027:INFO:Checking exceptions
2025-05-03 10:13:04,056:INFO:Importing libraries
2025-05-03 10:13:04,056:INFO:Copying training dataset
2025-05-03 10:13:04,091:INFO:Defining folds
2025-05-03 10:13:04,091:INFO:Declaring metric variables
2025-05-03 10:13:04,105:INFO:Importing untrained model
2025-05-03 10:13:04,116:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:13:04,129:INFO:Starting cross validation
2025-05-03 10:13:04,133:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:13:04,766:INFO:Calculating mean and std
2025-05-03 10:13:04,766:INFO:Creating metrics dataframe
2025-05-03 10:13:04,774:INFO:Finalizing model
2025-05-03 10:13:04,795:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:13:04,799:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.
2025-05-03 10:13:04,799:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:13:04,799:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:13:04,799:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:13:04,799:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:13:04,799:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:13:04,799:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:13:04,933:INFO:Uploading results into container
2025-05-03 10:13:04,934:INFO:Uploading model into container now
2025-05-03 10:13:04,946:INFO:_master_model_container: 20
2025-05-03 10:13:04,946:INFO:_display_container: 5
2025-05-03 10:13:04,947:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:13:04,947:INFO:create_model() successfully completed......................................
2025-05-03 10:13:39,006:INFO:Initializing create_model()
2025-05-03 10:13:39,006:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:13:39,006:INFO:Checking exceptions
2025-05-03 10:13:39,026:INFO:Importing libraries
2025-05-03 10:13:39,026:INFO:Copying training dataset
2025-05-03 10:13:39,051:INFO:Defining folds
2025-05-03 10:13:39,051:INFO:Declaring metric variables
2025-05-03 10:13:39,057:INFO:Importing untrained model
2025-05-03 10:13:39,062:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:13:39,071:INFO:Starting cross validation
2025-05-03 10:13:39,072:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:13:39,455:INFO:Calculating mean and std
2025-05-03 10:13:39,455:INFO:Creating metrics dataframe
2025-05-03 10:13:39,459:INFO:Finalizing model
2025-05-03 10:13:39,631:INFO:Uploading results into container
2025-05-03 10:13:39,631:INFO:Uploading model into container now
2025-05-03 10:13:39,643:INFO:_master_model_container: 21
2025-05-03 10:13:39,643:INFO:_display_container: 6
2025-05-03 10:13:39,644:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:13:39,645:INFO:create_model() successfully completed......................................
2025-05-03 10:13:54,982:INFO:Initializing create_model()
2025-05-03 10:13:54,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:13:54,983:INFO:Checking exceptions
2025-05-03 10:13:55,000:INFO:Importing libraries
2025-05-03 10:13:55,000:INFO:Copying training dataset
2025-05-03 10:13:55,025:INFO:Defining folds
2025-05-03 10:13:55,026:INFO:Declaring metric variables
2025-05-03 10:13:55,030:INFO:Importing untrained model
2025-05-03 10:13:55,034:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:13:55,042:INFO:Starting cross validation
2025-05-03 10:13:55,043:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:13:56,297:INFO:Calculating mean and std
2025-05-03 10:13:56,297:INFO:Creating metrics dataframe
2025-05-03 10:13:56,297:INFO:Finalizing model
2025-05-03 10:13:56,836:INFO:Uploading results into container
2025-05-03 10:13:56,838:INFO:Uploading model into container now
2025-05-03 10:13:56,849:INFO:_master_model_container: 22
2025-05-03 10:13:56,850:INFO:_display_container: 7
2025-05-03 10:13:56,850:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:13:56,850:INFO:create_model() successfully completed......................................
2025-05-03 10:14:21,028:INFO:Initializing tune_model()
2025-05-03 10:14:21,028:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:14:21,028:INFO:Checking exceptions
2025-05-03 10:14:21,028:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:14:21,049:INFO:Copying training dataset
2025-05-03 10:14:21,062:INFO:Checking base model
2025-05-03 10:14:21,062:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 10:14:21,065:INFO:Declaring metric variables
2025-05-03 10:14:21,068:INFO:Defining Hyperparameters
2025-05-03 10:14:21,208:INFO:Tuning with n_jobs=-1
2025-05-03 10:14:21,208:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:16:11,188:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 10:16:11,189:INFO:Hyperparameter search completed
2025-05-03 10:16:11,189:INFO:SubProcess create_model() called ==================================
2025-05-03 10:16:11,189:INFO:Initializing create_model()
2025-05-03 10:16:11,190:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D9C7350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 10:16:11,190:INFO:Checking exceptions
2025-05-03 10:16:11,190:INFO:Importing libraries
2025-05-03 10:16:11,190:INFO:Copying training dataset
2025-05-03 10:16:11,208:INFO:Defining folds
2025-05-03 10:16:11,208:INFO:Declaring metric variables
2025-05-03 10:16:11,211:INFO:Importing untrained model
2025-05-03 10:16:11,211:INFO:Declaring custom model
2025-05-03 10:16:11,215:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:16:11,222:INFO:Starting cross validation
2025-05-03 10:16:11,223:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:16:11,720:INFO:Calculating mean and std
2025-05-03 10:16:11,721:INFO:Creating metrics dataframe
2025-05-03 10:16:11,727:INFO:Finalizing model
2025-05-03 10:16:11,735:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 10:16:11,735:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 10:16:11,735:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 10:16:11,750:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 10:16:11,750:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 10:16:11,750:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 10:16:11,751:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:16:11,755:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.
2025-05-03 10:16:11,755:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:16:11,755:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:16:11,755:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:16:11,755:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:16:11,756:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:16:11,756:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:16:11,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,857:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,858:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,858:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,860:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,865:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,866:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,867:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,867:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,868:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,870:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,872:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,873:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,876:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,877:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,879:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,883:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,884:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,885:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,887:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,887:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,891:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,893:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,897:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,897:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,899:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,902:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,902:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,906:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,910:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,910:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,912:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:16:11,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:16:11,945:INFO:Uploading results into container
2025-05-03 10:16:11,946:INFO:Uploading model into container now
2025-05-03 10:16:11,946:INFO:_master_model_container: 23
2025-05-03 10:16:11,946:INFO:_display_container: 8
2025-05-03 10:16:11,947:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:16:11,947:INFO:create_model() successfully completed......................................
2025-05-03 10:16:12,098:INFO:SubProcess create_model() end ==================================
2025-05-03 10:16:12,098:INFO:choose_better activated
2025-05-03 10:16:12,102:INFO:SubProcess create_model() called ==================================
2025-05-03 10:16:12,103:INFO:Initializing create_model()
2025-05-03 10:16:12,103:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:16:12,103:INFO:Checking exceptions
2025-05-03 10:16:12,104:INFO:Importing libraries
2025-05-03 10:16:12,104:INFO:Copying training dataset
2025-05-03 10:16:12,122:INFO:Defining folds
2025-05-03 10:16:12,122:INFO:Declaring metric variables
2025-05-03 10:16:12,122:INFO:Importing untrained model
2025-05-03 10:16:12,122:INFO:Declaring custom model
2025-05-03 10:16:12,123:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:16:12,123:INFO:Starting cross validation
2025-05-03 10:16:12,123:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:16:12,621:INFO:Calculating mean and std
2025-05-03 10:16:12,621:INFO:Creating metrics dataframe
2025-05-03 10:16:12,623:INFO:Finalizing model
2025-05-03 10:16:12,644:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:16:12,646:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000938 seconds.
2025-05-03 10:16:12,646:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:16:12,648:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:16:12,648:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:16:12,648:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:16:12,648:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:16:12,648:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:16:12,781:INFO:Uploading results into container
2025-05-03 10:16:12,781:INFO:Uploading model into container now
2025-05-03 10:16:12,783:INFO:_master_model_container: 24
2025-05-03 10:16:12,783:INFO:_display_container: 9
2025-05-03 10:16:12,783:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:16:12,783:INFO:create_model() successfully completed......................................
2025-05-03 10:16:12,948:INFO:SubProcess create_model() end ==================================
2025-05-03 10:16:12,950:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 10:16:12,952:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 10:16:12,953:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 10:16:12,953:INFO:choose_better completed
2025-05-03 10:16:12,968:INFO:_master_model_container: 24
2025-05-03 10:16:12,968:INFO:_display_container: 8
2025-05-03 10:16:12,970:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:16:12,970:INFO:tune_model() successfully completed......................................
2025-05-03 10:16:33,834:INFO:Initializing tune_model()
2025-05-03 10:16:33,835:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:16:33,835:INFO:Checking exceptions
2025-05-03 10:16:33,835:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:16:33,854:INFO:Copying training dataset
2025-05-03 10:16:33,866:INFO:Checking base model
2025-05-03 10:16:33,866:INFO:Base model : Extreme Gradient Boosting
2025-05-03 10:16:33,869:INFO:Declaring metric variables
2025-05-03 10:16:33,873:INFO:Defining Hyperparameters
2025-05-03 10:16:34,010:INFO:Tuning with n_jobs=-1
2025-05-03 10:16:34,014:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:18:04,659:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 10:18:04,660:INFO:Hyperparameter search completed
2025-05-03 10:18:04,660:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:04,661:INFO:Initializing create_model()
2025-05-03 10:18:04,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001369D936B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 10:18:04,661:INFO:Checking exceptions
2025-05-03 10:18:04,661:INFO:Importing libraries
2025-05-03 10:18:04,661:INFO:Copying training dataset
2025-05-03 10:18:04,678:INFO:Defining folds
2025-05-03 10:18:04,678:INFO:Declaring metric variables
2025-05-03 10:18:04,681:INFO:Importing untrained model
2025-05-03 10:18:04,681:INFO:Declaring custom model
2025-05-03 10:18:04,685:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:18:04,693:INFO:Starting cross validation
2025-05-03 10:18:04,693:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:05,104:INFO:Calculating mean and std
2025-05-03 10:18:05,105:INFO:Creating metrics dataframe
2025-05-03 10:18:05,109:INFO:Finalizing model
2025-05-03 10:18:05,258:INFO:Uploading results into container
2025-05-03 10:18:05,260:INFO:Uploading model into container now
2025-05-03 10:18:05,261:INFO:_master_model_container: 25
2025-05-03 10:18:05,261:INFO:_display_container: 9
2025-05-03 10:18:05,262:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 10:18:05,262:INFO:create_model() successfully completed......................................
2025-05-03 10:18:05,439:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:05,439:INFO:choose_better activated
2025-05-03 10:18:05,442:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:05,443:INFO:Initializing create_model()
2025-05-03 10:18:05,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:05,443:INFO:Checking exceptions
2025-05-03 10:18:05,444:INFO:Importing libraries
2025-05-03 10:18:05,444:INFO:Copying training dataset
2025-05-03 10:18:05,459:INFO:Defining folds
2025-05-03 10:18:05,459:INFO:Declaring metric variables
2025-05-03 10:18:05,459:INFO:Importing untrained model
2025-05-03 10:18:05,459:INFO:Declaring custom model
2025-05-03 10:18:05,460:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:18:05,461:INFO:Starting cross validation
2025-05-03 10:18:05,461:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:05,758:INFO:Calculating mean and std
2025-05-03 10:18:05,759:INFO:Creating metrics dataframe
2025-05-03 10:18:05,760:INFO:Finalizing model
2025-05-03 10:18:05,875:INFO:Uploading results into container
2025-05-03 10:18:05,875:INFO:Uploading model into container now
2025-05-03 10:18:05,877:INFO:_master_model_container: 26
2025-05-03 10:18:05,877:INFO:_display_container: 10
2025-05-03 10:18:05,877:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:18:05,877:INFO:create_model() successfully completed......................................
2025-05-03 10:18:06,030:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:06,030:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 10:18:06,031:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 10:18:06,032:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 10:18:06,032:INFO:choose_better completed
2025-05-03 10:18:06,038:INFO:_master_model_container: 26
2025-05-03 10:18:06,038:INFO:_display_container: 9
2025-05-03 10:18:06,039:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 10:18:06,039:INFO:tune_model() successfully completed......................................
2025-05-03 10:18:06,179:INFO:Initializing tune_model()
2025-05-03 10:18:06,179:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001369AB70290>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:18:06,179:INFO:Checking exceptions
2025-05-03 10:18:06,179:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:18:06,202:INFO:Copying training dataset
2025-05-03 10:18:06,214:INFO:Checking base model
2025-05-03 10:18:06,215:INFO:Base model : Random Forest Classifier
2025-05-03 10:18:06,217:INFO:Declaring metric variables
2025-05-03 10:18:06,221:INFO:Defining Hyperparameters
2025-05-03 10:18:06,339:INFO:Tuning with n_jobs=-1
2025-05-03 10:18:06,344:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:18:42,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:18:42,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:18:42,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:18:42,127:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 10:18:44,664:INFO:PyCaret ClassificationExperiment
2025-05-03 10:18:44,664:INFO:Logging name: clf-default-name
2025-05-03 10:18:44,664:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 10:18:44,664:INFO:version 3.3.2
2025-05-03 10:18:44,664:INFO:Initializing setup()
2025-05-03 10:18:44,664:INFO:self.USI: d7a0
2025-05-03 10:18:44,664:INFO:self._variable_keys: {'n_jobs_param', 'X_test', 'pipeline', 'X_train', 'html_param', 'exp_id', 'log_plots_param', '_available_plots', 'gpu_n_jobs_param', 'X', 'target_param', 'fix_imbalance', 'USI', 'seed', 'fold_generator', 'logging_param', 'gpu_param', 'data', 'idx', 'fold_groups_param', 'y_train', 'y_test', 'is_multiclass', 'memory', '_ml_usecase', 'y', 'exp_name_log', 'fold_shuffle_param'}
2025-05-03 10:18:44,664:INFO:Checking environment
2025-05-03 10:18:44,664:INFO:python_version: 3.11.11
2025-05-03 10:18:44,664:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 10:18:44,664:INFO:machine: AMD64
2025-05-03 10:18:44,664:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 10:18:44,669:INFO:Memory: svmem(total=16965230592, available=4641161216, percent=72.6, used=12324069376, free=4641161216)
2025-05-03 10:18:44,669:INFO:Physical Core: 4
2025-05-03 10:18:44,669:INFO:Logical Core: 8
2025-05-03 10:18:44,669:INFO:Checking libraries
2025-05-03 10:18:44,669:INFO:System:
2025-05-03 10:18:44,669:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 10:18:44,670:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 10:18:44,670:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 10:18:44,670:INFO:PyCaret required dependencies:
2025-05-03 10:18:44,671:INFO:                 pip: 25.0
2025-05-03 10:18:44,671:INFO:          setuptools: 75.8.0
2025-05-03 10:18:44,671:INFO:             pycaret: 3.3.2
2025-05-03 10:18:44,671:INFO:             IPython: 8.32.0
2025-05-03 10:18:44,671:INFO:          ipywidgets: 8.1.6
2025-05-03 10:18:44,671:INFO:                tqdm: 4.67.1
2025-05-03 10:18:44,671:INFO:               numpy: 1.26.4
2025-05-03 10:18:44,671:INFO:              pandas: 2.1.4
2025-05-03 10:18:44,671:INFO:              jinja2: 3.1.6
2025-05-03 10:18:44,672:INFO:               scipy: 1.11.4
2025-05-03 10:18:44,672:INFO:              joblib: 1.3.2
2025-05-03 10:18:44,672:INFO:             sklearn: 1.4.2
2025-05-03 10:18:44,672:INFO:                pyod: 2.0.5
2025-05-03 10:18:44,672:INFO:            imblearn: 0.13.0
2025-05-03 10:18:44,672:INFO:   category_encoders: 2.7.0
2025-05-03 10:18:44,672:INFO:            lightgbm: 4.6.0
2025-05-03 10:18:44,672:INFO:               numba: 0.61.2
2025-05-03 10:18:44,672:INFO:            requests: 2.32.3
2025-05-03 10:18:44,672:INFO:          matplotlib: 3.7.5
2025-05-03 10:18:44,672:INFO:          scikitplot: 0.3.7
2025-05-03 10:18:44,672:INFO:         yellowbrick: 1.5
2025-05-03 10:18:44,672:INFO:              plotly: 5.24.1
2025-05-03 10:18:44,672:INFO:    plotly-resampler: Not installed
2025-05-03 10:18:44,672:INFO:             kaleido: 0.2.1
2025-05-03 10:18:44,672:INFO:           schemdraw: 0.15
2025-05-03 10:18:44,672:INFO:         statsmodels: 0.14.4
2025-05-03 10:18:44,672:INFO:              sktime: 0.26.0
2025-05-03 10:18:44,672:INFO:               tbats: 1.1.3
2025-05-03 10:18:44,672:INFO:            pmdarima: 2.0.4
2025-05-03 10:18:44,672:INFO:              psutil: 6.1.1
2025-05-03 10:18:44,672:INFO:          markupsafe: 3.0.2
2025-05-03 10:18:44,672:INFO:             pickle5: Not installed
2025-05-03 10:18:44,672:INFO:         cloudpickle: 3.1.1
2025-05-03 10:18:44,672:INFO:         deprecation: 2.1.0
2025-05-03 10:18:44,672:INFO:              xxhash: 3.5.0
2025-05-03 10:18:44,672:INFO:           wurlitzer: Not installed
2025-05-03 10:18:44,672:INFO:PyCaret optional dependencies:
2025-05-03 10:18:44,891:INFO:                shap: 0.47.2
2025-05-03 10:18:44,891:INFO:           interpret: Not installed
2025-05-03 10:18:44,891:INFO:                umap: Not installed
2025-05-03 10:18:44,891:INFO:     ydata_profiling: Not installed
2025-05-03 10:18:44,891:INFO:  explainerdashboard: Not installed
2025-05-03 10:18:44,891:INFO:             autoviz: Not installed
2025-05-03 10:18:44,891:INFO:           fairlearn: Not installed
2025-05-03 10:18:44,891:INFO:          deepchecks: Not installed
2025-05-03 10:18:44,891:INFO:             xgboost: 3.0.0
2025-05-03 10:18:44,891:INFO:            catboost: Not installed
2025-05-03 10:18:44,891:INFO:              kmodes: Not installed
2025-05-03 10:18:44,891:INFO:             mlxtend: Not installed
2025-05-03 10:18:44,891:INFO:       statsforecast: Not installed
2025-05-03 10:18:44,891:INFO:        tune_sklearn: Not installed
2025-05-03 10:18:44,891:INFO:                 ray: Not installed
2025-05-03 10:18:44,891:INFO:            hyperopt: 0.2.7
2025-05-03 10:18:44,891:INFO:              optuna: Not installed
2025-05-03 10:18:44,891:INFO:               skopt: 0.10.2
2025-05-03 10:18:44,891:INFO:              mlflow: 2.22.0
2025-05-03 10:18:44,891:INFO:              gradio: Not installed
2025-05-03 10:18:44,891:INFO:             fastapi: 0.115.12
2025-05-03 10:18:44,891:INFO:             uvicorn: 0.34.2
2025-05-03 10:18:44,891:INFO:              m2cgen: Not installed
2025-05-03 10:18:44,891:INFO:           evidently: Not installed
2025-05-03 10:18:44,891:INFO:               fugue: Not installed
2025-05-03 10:18:44,891:INFO:           streamlit: Not installed
2025-05-03 10:18:44,891:INFO:             prophet: Not installed
2025-05-03 10:18:44,891:INFO:None
2025-05-03 10:18:44,891:INFO:Set up data.
2025-05-03 10:18:44,908:INFO:Set up folding strategy.
2025-05-03 10:18:44,908:INFO:Set up train/test split.
2025-05-03 10:18:44,928:INFO:Set up index.
2025-05-03 10:18:44,928:INFO:Assigning column types.
2025-05-03 10:18:44,941:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 10:18:44,979:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 10:18:44,979:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:18:44,992:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,041:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 10:18:45,041:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:18:45,058:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,058:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,058:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 10:18:45,091:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:18:45,124:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,158:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 10:18:45,185:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,187:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,187:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 10:18:45,241:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,241:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,291:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,308:INFO:Finished creating preprocessing pipeline.
2025-05-03 10:18:45,308:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 10:18:45,308:INFO:Creating final display dataframe.
2025-05-03 10:18:45,374:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 10:18:45,429:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,429:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,474:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 10:18:45,491:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 10:18:45,491:INFO:setup() successfully completed in 0.83s...............
2025-05-03 10:18:45,494:INFO:Initializing compare_models()
2025-05-03 10:18:45,494:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:18:45,494:INFO:Checking exceptions
2025-05-03 10:18:45,507:INFO:Preparing display monitor
2025-05-03 10:18:45,639:INFO:Initializing Logistic Regression
2025-05-03 10:18:45,640:INFO:Total runtime is 1.6756852467854817e-05 minutes
2025-05-03 10:18:45,644:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:45,644:INFO:Initializing create_model()
2025-05-03 10:18:45,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:45,645:INFO:Checking exceptions
2025-05-03 10:18:45,645:INFO:Importing libraries
2025-05-03 10:18:45,645:INFO:Copying training dataset
2025-05-03 10:18:45,659:INFO:Defining folds
2025-05-03 10:18:45,661:INFO:Declaring metric variables
2025-05-03 10:18:45,663:INFO:Importing untrained model
2025-05-03 10:18:45,667:INFO:Logistic Regression Imported successfully
2025-05-03 10:18:45,674:INFO:Starting cross validation
2025-05-03 10:18:45,675:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:49,203:INFO:Calculating mean and std
2025-05-03 10:18:49,205:INFO:Creating metrics dataframe
2025-05-03 10:18:49,205:INFO:Uploading results into container
2025-05-03 10:18:49,210:INFO:Uploading model into container now
2025-05-03 10:18:49,210:INFO:_master_model_container: 1
2025-05-03 10:18:49,210:INFO:_display_container: 2
2025-05-03 10:18:49,210:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 10:18:49,210:INFO:create_model() successfully completed......................................
2025-05-03 10:18:49,336:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:49,336:INFO:Creating metrics dataframe
2025-05-03 10:18:49,336:INFO:Initializing Random Forest Classifier
2025-05-03 10:18:49,336:INFO:Total runtime is 0.06162578264872233 minutes
2025-05-03 10:18:49,355:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:49,355:INFO:Initializing create_model()
2025-05-03 10:18:49,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:49,355:INFO:Checking exceptions
2025-05-03 10:18:49,355:INFO:Importing libraries
2025-05-03 10:18:49,355:INFO:Copying training dataset
2025-05-03 10:18:49,394:INFO:Defining folds
2025-05-03 10:18:49,394:INFO:Declaring metric variables
2025-05-03 10:18:49,394:INFO:Importing untrained model
2025-05-03 10:18:49,404:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:18:49,408:INFO:Starting cross validation
2025-05-03 10:18:49,408:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:53,538:INFO:Calculating mean and std
2025-05-03 10:18:53,540:INFO:Creating metrics dataframe
2025-05-03 10:18:53,542:INFO:Uploading results into container
2025-05-03 10:18:53,544:INFO:Uploading model into container now
2025-05-03 10:18:53,544:INFO:_master_model_container: 2
2025-05-03 10:18:53,546:INFO:_display_container: 2
2025-05-03 10:18:53,547:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:18:53,547:INFO:create_model() successfully completed......................................
2025-05-03 10:18:53,681:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:53,681:INFO:Creating metrics dataframe
2025-05-03 10:18:53,685:INFO:Initializing Extreme Gradient Boosting
2025-05-03 10:18:53,685:INFO:Total runtime is 0.13409624099731446 minutes
2025-05-03 10:18:53,685:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:53,685:INFO:Initializing create_model()
2025-05-03 10:18:53,685:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:53,685:INFO:Checking exceptions
2025-05-03 10:18:53,685:INFO:Importing libraries
2025-05-03 10:18:53,685:INFO:Copying training dataset
2025-05-03 10:18:53,715:INFO:Defining folds
2025-05-03 10:18:53,715:INFO:Declaring metric variables
2025-05-03 10:18:53,719:INFO:Importing untrained model
2025-05-03 10:18:53,721:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:18:53,729:INFO:Starting cross validation
2025-05-03 10:18:53,729:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:56,841:INFO:Calculating mean and std
2025-05-03 10:18:56,841:INFO:Creating metrics dataframe
2025-05-03 10:18:56,846:INFO:Uploading results into container
2025-05-03 10:18:56,846:INFO:Uploading model into container now
2025-05-03 10:18:56,846:INFO:_master_model_container: 3
2025-05-03 10:18:56,846:INFO:_display_container: 2
2025-05-03 10:18:56,846:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:18:56,846:INFO:create_model() successfully completed......................................
2025-05-03 10:18:56,981:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:56,981:INFO:Creating metrics dataframe
2025-05-03 10:18:56,988:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 10:18:56,988:INFO:Total runtime is 0.1891448736190796 minutes
2025-05-03 10:18:56,988:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:56,988:INFO:Initializing create_model()
2025-05-03 10:18:56,988:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:56,988:INFO:Checking exceptions
2025-05-03 10:18:56,988:INFO:Importing libraries
2025-05-03 10:18:56,995:INFO:Copying training dataset
2025-05-03 10:18:57,011:INFO:Defining folds
2025-05-03 10:18:57,011:INFO:Declaring metric variables
2025-05-03 10:18:57,015:INFO:Importing untrained model
2025-05-03 10:18:57,015:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:18:57,026:INFO:Starting cross validation
2025-05-03 10:18:57,026:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:57,492:INFO:Calculating mean and std
2025-05-03 10:18:57,494:INFO:Creating metrics dataframe
2025-05-03 10:18:57,496:INFO:Uploading results into container
2025-05-03 10:18:57,497:INFO:Uploading model into container now
2025-05-03 10:18:57,497:INFO:_master_model_container: 4
2025-05-03 10:18:57,497:INFO:_display_container: 2
2025-05-03 10:18:57,497:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:18:57,497:INFO:create_model() successfully completed......................................
2025-05-03 10:18:57,631:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:57,631:INFO:Creating metrics dataframe
2025-05-03 10:18:57,631:INFO:Initializing Extra Trees Classifier
2025-05-03 10:18:57,631:INFO:Total runtime is 0.19986746708552044 minutes
2025-05-03 10:18:57,644:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:57,644:INFO:Initializing create_model()
2025-05-03 10:18:57,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:57,644:INFO:Checking exceptions
2025-05-03 10:18:57,645:INFO:Importing libraries
2025-05-03 10:18:57,645:INFO:Copying training dataset
2025-05-03 10:18:57,664:INFO:Defining folds
2025-05-03 10:18:57,664:INFO:Declaring metric variables
2025-05-03 10:18:57,664:INFO:Importing untrained model
2025-05-03 10:18:57,664:INFO:Extra Trees Classifier Imported successfully
2025-05-03 10:18:57,676:INFO:Starting cross validation
2025-05-03 10:18:57,680:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:58,797:INFO:Calculating mean and std
2025-05-03 10:18:58,797:INFO:Creating metrics dataframe
2025-05-03 10:18:58,797:INFO:Uploading results into container
2025-05-03 10:18:58,797:INFO:Uploading model into container now
2025-05-03 10:18:58,802:INFO:_master_model_container: 5
2025-05-03 10:18:58,802:INFO:_display_container: 2
2025-05-03 10:18:58,802:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 10:18:58,802:INFO:create_model() successfully completed......................................
2025-05-03 10:18:58,926:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:58,926:INFO:Creating metrics dataframe
2025-05-03 10:18:58,930:INFO:Initializing Ridge Classifier
2025-05-03 10:18:58,930:INFO:Total runtime is 0.2215170462926229 minutes
2025-05-03 10:18:58,944:INFO:SubProcess create_model() called ==================================
2025-05-03 10:18:58,946:INFO:Initializing create_model()
2025-05-03 10:18:58,946:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B10985ACD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:58,946:INFO:Checking exceptions
2025-05-03 10:18:58,946:INFO:Importing libraries
2025-05-03 10:18:58,946:INFO:Copying training dataset
2025-05-03 10:18:58,966:INFO:Defining folds
2025-05-03 10:18:58,966:INFO:Declaring metric variables
2025-05-03 10:18:58,966:INFO:Importing untrained model
2025-05-03 10:18:58,983:INFO:Ridge Classifier Imported successfully
2025-05-03 10:18:59,013:INFO:Starting cross validation
2025-05-03 10:18:59,015:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:18:59,091:INFO:Calculating mean and std
2025-05-03 10:18:59,091:INFO:Creating metrics dataframe
2025-05-03 10:18:59,091:INFO:Uploading results into container
2025-05-03 10:18:59,091:INFO:Uploading model into container now
2025-05-03 10:18:59,091:INFO:_master_model_container: 6
2025-05-03 10:18:59,096:INFO:_display_container: 2
2025-05-03 10:18:59,096:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 10:18:59,096:INFO:create_model() successfully completed......................................
2025-05-03 10:18:59,230:INFO:SubProcess create_model() end ==================================
2025-05-03 10:18:59,230:INFO:Creating metrics dataframe
2025-05-03 10:18:59,246:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 10:18:59,246:INFO:Initializing create_model()
2025-05-03 10:18:59,246:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:59,246:INFO:Checking exceptions
2025-05-03 10:18:59,246:INFO:Importing libraries
2025-05-03 10:18:59,246:INFO:Copying training dataset
2025-05-03 10:18:59,269:INFO:Defining folds
2025-05-03 10:18:59,269:INFO:Declaring metric variables
2025-05-03 10:18:59,269:INFO:Importing untrained model
2025-05-03 10:18:59,269:INFO:Declaring custom model
2025-05-03 10:18:59,273:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:18:59,273:INFO:Cross validation set to False
2025-05-03 10:18:59,273:INFO:Fitting Model
2025-05-03 10:18:59,296:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:18:59,299:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.
2025-05-03 10:18:59,299:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:18:59,299:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:18:59,299:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:18:59,299:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:18:59,299:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:18:59,299:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:18:59,402:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:18:59,402:INFO:create_model() successfully completed......................................
2025-05-03 10:18:59,572:INFO:_master_model_container: 6
2025-05-03 10:18:59,572:INFO:_display_container: 2
2025-05-03 10:18:59,572:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:18:59,572:INFO:compare_models() successfully completed......................................
2025-05-03 10:18:59,595:INFO:Initializing create_model()
2025-05-03 10:18:59,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:18:59,595:INFO:Checking exceptions
2025-05-03 10:18:59,627:INFO:Importing libraries
2025-05-03 10:18:59,627:INFO:Copying training dataset
2025-05-03 10:18:59,650:INFO:Defining folds
2025-05-03 10:18:59,650:INFO:Declaring metric variables
2025-05-03 10:18:59,650:INFO:Importing untrained model
2025-05-03 10:18:59,659:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:18:59,668:INFO:Starting cross validation
2025-05-03 10:18:59,668:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:19:00,202:INFO:Calculating mean and std
2025-05-03 10:19:00,203:INFO:Creating metrics dataframe
2025-05-03 10:19:00,209:INFO:Finalizing model
2025-05-03 10:19:00,228:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:19:00,233:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.
2025-05-03 10:19:00,233:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:19:00,233:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:19:00,233:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:19:00,233:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:19:00,233:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:19:00,233:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:19:00,351:INFO:Uploading results into container
2025-05-03 10:19:00,351:INFO:Uploading model into container now
2025-05-03 10:19:00,361:INFO:_master_model_container: 7
2025-05-03 10:19:00,361:INFO:_display_container: 3
2025-05-03 10:19:00,363:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:19:00,363:INFO:create_model() successfully completed......................................
2025-05-03 10:19:00,513:INFO:Initializing create_model()
2025-05-03 10:19:00,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:19:00,513:INFO:Checking exceptions
2025-05-03 10:19:00,574:INFO:Importing libraries
2025-05-03 10:19:00,574:INFO:Copying training dataset
2025-05-03 10:19:00,605:INFO:Defining folds
2025-05-03 10:19:00,605:INFO:Declaring metric variables
2025-05-03 10:19:00,605:INFO:Importing untrained model
2025-05-03 10:19:00,611:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:19:00,613:INFO:Starting cross validation
2025-05-03 10:19:00,613:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:19:00,942:INFO:Calculating mean and std
2025-05-03 10:19:00,942:INFO:Creating metrics dataframe
2025-05-03 10:19:00,942:INFO:Finalizing model
2025-05-03 10:19:01,077:INFO:Uploading results into container
2025-05-03 10:19:01,079:INFO:Uploading model into container now
2025-05-03 10:19:01,089:INFO:_master_model_container: 8
2025-05-03 10:19:01,089:INFO:_display_container: 4
2025-05-03 10:19:01,091:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:19:01,091:INFO:create_model() successfully completed......................................
2025-05-03 10:19:01,239:INFO:Initializing create_model()
2025-05-03 10:19:01,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:19:01,239:INFO:Checking exceptions
2025-05-03 10:19:01,256:INFO:Importing libraries
2025-05-03 10:19:01,256:INFO:Copying training dataset
2025-05-03 10:19:01,290:INFO:Defining folds
2025-05-03 10:19:01,290:INFO:Declaring metric variables
2025-05-03 10:19:01,292:INFO:Importing untrained model
2025-05-03 10:19:01,292:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:19:01,304:INFO:Starting cross validation
2025-05-03 10:19:01,306:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:19:02,610:INFO:Calculating mean and std
2025-05-03 10:19:02,610:INFO:Creating metrics dataframe
2025-05-03 10:19:02,613:INFO:Finalizing model
2025-05-03 10:19:03,164:INFO:Uploading results into container
2025-05-03 10:19:03,164:INFO:Uploading model into container now
2025-05-03 10:19:03,164:INFO:_master_model_container: 9
2025-05-03 10:19:03,164:INFO:_display_container: 5
2025-05-03 10:19:03,164:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:19:03,164:INFO:create_model() successfully completed......................................
2025-05-03 10:19:03,349:INFO:Initializing tune_model()
2025-05-03 10:19:03,349:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:19:03,349:INFO:Checking exceptions
2025-05-03 10:19:03,349:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:19:03,379:INFO:Copying training dataset
2025-05-03 10:19:03,392:INFO:Checking base model
2025-05-03 10:19:03,392:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 10:19:03,399:INFO:Declaring metric variables
2025-05-03 10:19:03,399:INFO:Defining Hyperparameters
2025-05-03 10:19:03,528:INFO:Tuning with n_jobs=-1
2025-05-03 10:19:03,528:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:21:00,721:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 10:21:00,721:INFO:Hyperparameter search completed
2025-05-03 10:21:00,721:INFO:SubProcess create_model() called ==================================
2025-05-03 10:21:00,721:INFO:Initializing create_model()
2025-05-03 10:21:00,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B109859FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 10:21:00,721:INFO:Checking exceptions
2025-05-03 10:21:00,721:INFO:Importing libraries
2025-05-03 10:21:00,721:INFO:Copying training dataset
2025-05-03 10:21:00,746:INFO:Defining folds
2025-05-03 10:21:00,746:INFO:Declaring metric variables
2025-05-03 10:21:00,746:INFO:Importing untrained model
2025-05-03 10:21:00,746:INFO:Declaring custom model
2025-05-03 10:21:00,746:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:21:00,763:INFO:Starting cross validation
2025-05-03 10:21:00,763:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:21:01,143:INFO:Calculating mean and std
2025-05-03 10:21:01,145:INFO:Creating metrics dataframe
2025-05-03 10:21:01,152:INFO:Finalizing model
2025-05-03 10:21:01,157:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 10:21:01,157:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 10:21:01,157:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 10:21:01,174:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 10:21:01,174:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 10:21:01,174:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 10:21:01,174:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:21:01,176:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.
2025-05-03 10:21:01,176:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:21:01,176:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:21:01,178:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:21:01,178:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:21:01,179:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:21:01,179:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:21:01,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,268:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,272:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,274:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,297:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,314:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,321:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 10:21:01,331:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 10:21:01,339:INFO:Uploading results into container
2025-05-03 10:21:01,341:INFO:Uploading model into container now
2025-05-03 10:21:01,341:INFO:_master_model_container: 10
2025-05-03 10:21:01,341:INFO:_display_container: 6
2025-05-03 10:21:01,343:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:21:01,343:INFO:create_model() successfully completed......................................
2025-05-03 10:21:01,495:INFO:SubProcess create_model() end ==================================
2025-05-03 10:21:01,495:INFO:choose_better activated
2025-05-03 10:21:01,496:INFO:SubProcess create_model() called ==================================
2025-05-03 10:21:01,496:INFO:Initializing create_model()
2025-05-03 10:21:01,496:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:21:01,496:INFO:Checking exceptions
2025-05-03 10:21:01,512:INFO:Importing libraries
2025-05-03 10:21:01,513:INFO:Copying training dataset
2025-05-03 10:21:01,540:INFO:Defining folds
2025-05-03 10:21:01,540:INFO:Declaring metric variables
2025-05-03 10:21:01,540:INFO:Importing untrained model
2025-05-03 10:21:01,540:INFO:Declaring custom model
2025-05-03 10:21:01,540:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:21:01,540:INFO:Starting cross validation
2025-05-03 10:21:01,540:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:21:02,026:INFO:Calculating mean and std
2025-05-03 10:21:02,026:INFO:Creating metrics dataframe
2025-05-03 10:21:02,028:INFO:Finalizing model
2025-05-03 10:21:02,053:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:21:02,057:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.
2025-05-03 10:21:02,057:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:21:02,058:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:21:02,058:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:21:02,058:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:21:02,058:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:21:02,058:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:21:02,182:INFO:Uploading results into container
2025-05-03 10:21:02,184:INFO:Uploading model into container now
2025-05-03 10:21:02,184:INFO:_master_model_container: 11
2025-05-03 10:21:02,184:INFO:_display_container: 7
2025-05-03 10:21:02,184:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:21:02,184:INFO:create_model() successfully completed......................................
2025-05-03 10:21:02,328:INFO:SubProcess create_model() end ==================================
2025-05-03 10:21:02,328:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 10:21:02,328:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 10:21:02,328:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 10:21:02,328:INFO:choose_better completed
2025-05-03 10:21:02,337:INFO:_master_model_container: 11
2025-05-03 10:21:02,337:INFO:_display_container: 6
2025-05-03 10:21:02,337:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:21:02,337:INFO:tune_model() successfully completed......................................
2025-05-03 10:21:02,497:INFO:Initializing tune_model()
2025-05-03 10:21:02,497:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:21:02,497:INFO:Checking exceptions
2025-05-03 10:21:02,497:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:21:02,530:INFO:Copying training dataset
2025-05-03 10:21:02,546:INFO:Checking base model
2025-05-03 10:21:02,546:INFO:Base model : Extreme Gradient Boosting
2025-05-03 10:21:02,546:INFO:Declaring metric variables
2025-05-03 10:21:02,555:INFO:Defining Hyperparameters
2025-05-03 10:21:02,681:INFO:Tuning with n_jobs=-1
2025-05-03 10:21:02,681:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:22:55,605:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 10:22:55,605:INFO:Hyperparameter search completed
2025-05-03 10:22:55,605:INFO:SubProcess create_model() called ==================================
2025-05-03 10:22:55,605:INFO:Initializing create_model()
2025-05-03 10:22:55,605:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B1085F9210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 10:22:55,605:INFO:Checking exceptions
2025-05-03 10:22:55,605:INFO:Importing libraries
2025-05-03 10:22:55,605:INFO:Copying training dataset
2025-05-03 10:22:55,627:INFO:Defining folds
2025-05-03 10:22:55,627:INFO:Declaring metric variables
2025-05-03 10:22:55,629:INFO:Importing untrained model
2025-05-03 10:22:55,629:INFO:Declaring custom model
2025-05-03 10:22:55,635:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:22:55,635:INFO:Starting cross validation
2025-05-03 10:22:55,635:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:22:56,077:INFO:Calculating mean and std
2025-05-03 10:22:56,077:INFO:Creating metrics dataframe
2025-05-03 10:22:56,084:INFO:Finalizing model
2025-05-03 10:22:56,293:INFO:Uploading results into container
2025-05-03 10:22:56,294:INFO:Uploading model into container now
2025-05-03 10:22:56,294:INFO:_master_model_container: 12
2025-05-03 10:22:56,294:INFO:_display_container: 7
2025-05-03 10:22:56,296:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 10:22:56,296:INFO:create_model() successfully completed......................................
2025-05-03 10:22:56,434:INFO:SubProcess create_model() end ==================================
2025-05-03 10:22:56,434:INFO:choose_better activated
2025-05-03 10:22:56,434:INFO:SubProcess create_model() called ==================================
2025-05-03 10:22:56,434:INFO:Initializing create_model()
2025-05-03 10:22:56,434:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:22:56,434:INFO:Checking exceptions
2025-05-03 10:22:56,434:INFO:Importing libraries
2025-05-03 10:22:56,434:INFO:Copying training dataset
2025-05-03 10:22:56,451:INFO:Defining folds
2025-05-03 10:22:56,451:INFO:Declaring metric variables
2025-05-03 10:22:56,451:INFO:Importing untrained model
2025-05-03 10:22:56,451:INFO:Declaring custom model
2025-05-03 10:22:56,466:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:22:56,466:INFO:Starting cross validation
2025-05-03 10:22:56,468:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:22:56,785:INFO:Calculating mean and std
2025-05-03 10:22:56,785:INFO:Creating metrics dataframe
2025-05-03 10:22:56,785:INFO:Finalizing model
2025-05-03 10:22:56,900:INFO:Uploading results into container
2025-05-03 10:22:56,902:INFO:Uploading model into container now
2025-05-03 10:22:56,902:INFO:_master_model_container: 13
2025-05-03 10:22:56,902:INFO:_display_container: 8
2025-05-03 10:22:56,902:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:22:56,902:INFO:create_model() successfully completed......................................
2025-05-03 10:22:57,034:INFO:SubProcess create_model() end ==================================
2025-05-03 10:22:57,034:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 10:22:57,034:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 10:22:57,034:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 10:22:57,034:INFO:choose_better completed
2025-05-03 10:22:57,051:INFO:_master_model_container: 13
2025-05-03 10:22:57,051:INFO:_display_container: 7
2025-05-03 10:22:57,051:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 10:22:57,051:INFO:tune_model() successfully completed......................................
2025-05-03 10:22:57,199:INFO:Initializing tune_model()
2025-05-03 10:22:57,199:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 10:22:57,199:INFO:Checking exceptions
2025-05-03 10:22:57,199:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 10:22:57,229:INFO:Copying training dataset
2025-05-03 10:22:57,245:INFO:Checking base model
2025-05-03 10:22:57,247:INFO:Base model : Random Forest Classifier
2025-05-03 10:22:57,249:INFO:Declaring metric variables
2025-05-03 10:22:57,254:INFO:Defining Hyperparameters
2025-05-03 10:22:57,432:INFO:Tuning with n_jobs=-1
2025-05-03 10:22:57,433:INFO:Initializing skopt.BayesSearchCV
2025-05-03 10:26:43,249:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 10:26:43,249:INFO:Hyperparameter search completed
2025-05-03 10:26:43,249:INFO:SubProcess create_model() called ==================================
2025-05-03 10:26:43,249:INFO:Initializing create_model()
2025-05-03 10:26:43,249:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B1085D39D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 10:26:43,249:INFO:Checking exceptions
2025-05-03 10:26:43,249:INFO:Importing libraries
2025-05-03 10:26:43,249:INFO:Copying training dataset
2025-05-03 10:26:43,266:INFO:Defining folds
2025-05-03 10:26:43,266:INFO:Declaring metric variables
2025-05-03 10:26:43,266:INFO:Importing untrained model
2025-05-03 10:26:43,266:INFO:Declaring custom model
2025-05-03 10:26:43,280:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:26:43,288:INFO:Starting cross validation
2025-05-03 10:26:43,288:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:26:47,174:INFO:Calculating mean and std
2025-05-03 10:26:47,175:INFO:Creating metrics dataframe
2025-05-03 10:26:47,180:INFO:Finalizing model
2025-05-03 10:26:49,516:INFO:Uploading results into container
2025-05-03 10:26:49,517:INFO:Uploading model into container now
2025-05-03 10:26:49,517:INFO:_master_model_container: 14
2025-05-03 10:26:49,517:INFO:_display_container: 8
2025-05-03 10:26:49,519:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 10:26:49,519:INFO:create_model() successfully completed......................................
2025-05-03 10:26:49,647:INFO:SubProcess create_model() end ==================================
2025-05-03 10:26:49,647:INFO:choose_better activated
2025-05-03 10:26:49,647:INFO:SubProcess create_model() called ==================================
2025-05-03 10:26:49,647:INFO:Initializing create_model()
2025-05-03 10:26:49,647:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:26:49,647:INFO:Checking exceptions
2025-05-03 10:26:49,658:INFO:Importing libraries
2025-05-03 10:26:49,658:INFO:Copying training dataset
2025-05-03 10:26:49,694:INFO:Defining folds
2025-05-03 10:26:49,694:INFO:Declaring metric variables
2025-05-03 10:26:49,694:INFO:Importing untrained model
2025-05-03 10:26:49,694:INFO:Declaring custom model
2025-05-03 10:26:49,694:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:26:49,694:INFO:Starting cross validation
2025-05-03 10:26:49,699:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:26:51,067:INFO:Calculating mean and std
2025-05-03 10:26:51,067:INFO:Creating metrics dataframe
2025-05-03 10:26:51,069:INFO:Finalizing model
2025-05-03 10:26:51,682:INFO:Uploading results into container
2025-05-03 10:26:51,682:INFO:Uploading model into container now
2025-05-03 10:26:51,682:INFO:_master_model_container: 15
2025-05-03 10:26:51,682:INFO:_display_container: 9
2025-05-03 10:26:51,682:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:26:51,682:INFO:create_model() successfully completed......................................
2025-05-03 10:26:51,807:INFO:SubProcess create_model() end ==================================
2025-05-03 10:26:51,807:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 10:26:51,807:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 10:26:51,807:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 10:26:51,807:INFO:choose_better completed
2025-05-03 10:26:51,822:INFO:_master_model_container: 15
2025-05-03 10:26:51,823:INFO:_display_container: 8
2025-05-03 10:26:51,824:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 10:26:51,824:INFO:tune_model() successfully completed......................................
2025-05-03 10:38:40,841:INFO:Initializing compare_models()
2025-05-03 10:38:40,841:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 10:38:40,853:INFO:Checking exceptions
2025-05-03 10:38:40,863:INFO:Preparing display monitor
2025-05-03 10:38:40,887:INFO:Initializing Logistic Regression
2025-05-03 10:38:40,887:INFO:Total runtime is 0.0 minutes
2025-05-03 10:38:40,896:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:40,897:INFO:Initializing create_model()
2025-05-03 10:38:40,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:40,897:INFO:Checking exceptions
2025-05-03 10:38:40,897:INFO:Importing libraries
2025-05-03 10:38:40,897:INFO:Copying training dataset
2025-05-03 10:38:40,920:INFO:Defining folds
2025-05-03 10:38:40,920:INFO:Declaring metric variables
2025-05-03 10:38:40,922:INFO:Importing untrained model
2025-05-03 10:38:40,928:INFO:Logistic Regression Imported successfully
2025-05-03 10:38:40,936:INFO:Starting cross validation
2025-05-03 10:38:40,936:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:44,680:INFO:Calculating mean and std
2025-05-03 10:38:44,680:INFO:Creating metrics dataframe
2025-05-03 10:38:44,684:INFO:Uploading results into container
2025-05-03 10:38:44,686:INFO:Uploading model into container now
2025-05-03 10:38:44,687:INFO:_master_model_container: 16
2025-05-03 10:38:44,687:INFO:_display_container: 9
2025-05-03 10:38:44,689:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 10:38:44,689:INFO:create_model() successfully completed......................................
2025-05-03 10:38:44,872:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:44,872:INFO:Creating metrics dataframe
2025-05-03 10:38:44,886:INFO:Initializing Random Forest Classifier
2025-05-03 10:38:44,886:INFO:Total runtime is 0.06665069262186686 minutes
2025-05-03 10:38:44,890:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:44,890:INFO:Initializing create_model()
2025-05-03 10:38:44,890:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:44,890:INFO:Checking exceptions
2025-05-03 10:38:44,890:INFO:Importing libraries
2025-05-03 10:38:44,890:INFO:Copying training dataset
2025-05-03 10:38:44,902:INFO:Defining folds
2025-05-03 10:38:44,902:INFO:Declaring metric variables
2025-05-03 10:38:44,916:INFO:Importing untrained model
2025-05-03 10:38:44,923:INFO:Random Forest Classifier Imported successfully
2025-05-03 10:38:44,930:INFO:Starting cross validation
2025-05-03 10:38:44,930:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:48,965:INFO:Calculating mean and std
2025-05-03 10:38:48,967:INFO:Creating metrics dataframe
2025-05-03 10:38:48,969:INFO:Uploading results into container
2025-05-03 10:38:48,969:INFO:Uploading model into container now
2025-05-03 10:38:48,969:INFO:_master_model_container: 17
2025-05-03 10:38:48,969:INFO:_display_container: 9
2025-05-03 10:38:48,969:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 10:38:48,969:INFO:create_model() successfully completed......................................
2025-05-03 10:38:49,148:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:49,148:INFO:Creating metrics dataframe
2025-05-03 10:38:49,148:INFO:Initializing Extreme Gradient Boosting
2025-05-03 10:38:49,148:INFO:Total runtime is 0.13767617543538413 minutes
2025-05-03 10:38:49,148:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:49,148:INFO:Initializing create_model()
2025-05-03 10:38:49,148:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:49,148:INFO:Checking exceptions
2025-05-03 10:38:49,148:INFO:Importing libraries
2025-05-03 10:38:49,148:INFO:Copying training dataset
2025-05-03 10:38:49,183:INFO:Defining folds
2025-05-03 10:38:49,183:INFO:Declaring metric variables
2025-05-03 10:38:49,189:INFO:Importing untrained model
2025-05-03 10:38:49,193:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 10:38:49,199:INFO:Starting cross validation
2025-05-03 10:38:49,199:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:52,575:INFO:Calculating mean and std
2025-05-03 10:38:52,575:INFO:Creating metrics dataframe
2025-05-03 10:38:52,575:INFO:Uploading results into container
2025-05-03 10:38:52,579:INFO:Uploading model into container now
2025-05-03 10:38:52,579:INFO:_master_model_container: 18
2025-05-03 10:38:52,579:INFO:_display_container: 9
2025-05-03 10:38:52,580:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 10:38:52,580:INFO:create_model() successfully completed......................................
2025-05-03 10:38:52,763:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:52,763:INFO:Creating metrics dataframe
2025-05-03 10:38:52,786:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 10:38:52,786:INFO:Total runtime is 0.1983155131340027 minutes
2025-05-03 10:38:52,792:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:52,792:INFO:Initializing create_model()
2025-05-03 10:38:52,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:52,792:INFO:Checking exceptions
2025-05-03 10:38:52,792:INFO:Importing libraries
2025-05-03 10:38:52,792:INFO:Copying training dataset
2025-05-03 10:38:52,826:INFO:Defining folds
2025-05-03 10:38:52,826:INFO:Declaring metric variables
2025-05-03 10:38:52,830:INFO:Importing untrained model
2025-05-03 10:38:52,840:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:38:52,852:INFO:Starting cross validation
2025-05-03 10:38:52,853:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:53,399:INFO:Calculating mean and std
2025-05-03 10:38:53,401:INFO:Creating metrics dataframe
2025-05-03 10:38:53,403:INFO:Uploading results into container
2025-05-03 10:38:53,404:INFO:Uploading model into container now
2025-05-03 10:38:53,406:INFO:_master_model_container: 19
2025-05-03 10:38:53,406:INFO:_display_container: 9
2025-05-03 10:38:53,408:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:38:53,408:INFO:create_model() successfully completed......................................
2025-05-03 10:38:53,587:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:53,587:INFO:Creating metrics dataframe
2025-05-03 10:38:53,596:INFO:Initializing Extra Trees Classifier
2025-05-03 10:38:53,596:INFO:Total runtime is 0.21180450121561686 minutes
2025-05-03 10:38:53,604:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:53,604:INFO:Initializing create_model()
2025-05-03 10:38:53,604:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:53,604:INFO:Checking exceptions
2025-05-03 10:38:53,604:INFO:Importing libraries
2025-05-03 10:38:53,604:INFO:Copying training dataset
2025-05-03 10:38:53,646:INFO:Defining folds
2025-05-03 10:38:53,646:INFO:Declaring metric variables
2025-05-03 10:38:53,646:INFO:Importing untrained model
2025-05-03 10:38:53,646:INFO:Extra Trees Classifier Imported successfully
2025-05-03 10:38:53,665:INFO:Starting cross validation
2025-05-03 10:38:53,665:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:54,946:INFO:Calculating mean and std
2025-05-03 10:38:54,948:INFO:Creating metrics dataframe
2025-05-03 10:38:54,951:INFO:Uploading results into container
2025-05-03 10:38:54,953:INFO:Uploading model into container now
2025-05-03 10:38:54,953:INFO:_master_model_container: 20
2025-05-03 10:38:54,954:INFO:_display_container: 9
2025-05-03 10:38:54,954:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 10:38:54,954:INFO:create_model() successfully completed......................................
2025-05-03 10:38:55,119:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:55,119:INFO:Creating metrics dataframe
2025-05-03 10:38:55,130:INFO:Initializing Ridge Classifier
2025-05-03 10:38:55,130:INFO:Total runtime is 0.23737852176030477 minutes
2025-05-03 10:38:55,138:INFO:SubProcess create_model() called ==================================
2025-05-03 10:38:55,138:INFO:Initializing create_model()
2025-05-03 10:38:55,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002B11E651150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:55,138:INFO:Checking exceptions
2025-05-03 10:38:55,138:INFO:Importing libraries
2025-05-03 10:38:55,140:INFO:Copying training dataset
2025-05-03 10:38:55,145:INFO:Defining folds
2025-05-03 10:38:55,145:INFO:Declaring metric variables
2025-05-03 10:38:55,164:INFO:Importing untrained model
2025-05-03 10:38:55,168:INFO:Ridge Classifier Imported successfully
2025-05-03 10:38:55,176:INFO:Starting cross validation
2025-05-03 10:38:55,176:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 10:38:55,256:INFO:Calculating mean and std
2025-05-03 10:38:55,256:INFO:Creating metrics dataframe
2025-05-03 10:38:55,256:INFO:Uploading results into container
2025-05-03 10:38:55,260:INFO:Uploading model into container now
2025-05-03 10:38:55,261:INFO:_master_model_container: 21
2025-05-03 10:38:55,261:INFO:_display_container: 9
2025-05-03 10:38:55,261:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 10:38:55,261:INFO:create_model() successfully completed......................................
2025-05-03 10:38:55,431:INFO:SubProcess create_model() end ==================================
2025-05-03 10:38:55,431:INFO:Creating metrics dataframe
2025-05-03 10:38:55,438:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 10:38:55,465:INFO:Initializing create_model()
2025-05-03 10:38:55,465:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 10:38:55,465:INFO:Checking exceptions
2025-05-03 10:38:55,465:INFO:Importing libraries
2025-05-03 10:38:55,465:INFO:Copying training dataset
2025-05-03 10:38:55,495:INFO:Defining folds
2025-05-03 10:38:55,495:INFO:Declaring metric variables
2025-05-03 10:38:55,495:INFO:Importing untrained model
2025-05-03 10:38:55,495:INFO:Declaring custom model
2025-05-03 10:38:55,495:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 10:38:55,495:INFO:Cross validation set to False
2025-05-03 10:38:55,495:INFO:Fitting Model
2025-05-03 10:38:55,529:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 10:38:55,533:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.
2025-05-03 10:38:55,533:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 10:38:55,533:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 10:38:55,533:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 10:38:55,533:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 10:38:55,535:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 10:38:55,535:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 10:38:55,650:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:38:55,650:INFO:create_model() successfully completed......................................
2025-05-03 10:38:55,861:INFO:_master_model_container: 21
2025-05-03 10:38:55,861:INFO:_display_container: 9
2025-05-03 10:38:55,861:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 10:38:55,861:INFO:compare_models() successfully completed......................................
2025-05-03 11:01:22,569:INFO:Initializing save_model()
2025-05-03 11:01:22,573:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 11:01:22,573:INFO:Adding model into prep_pipe
2025-05-03 11:01:22,582:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 11:01:22,587:INFO:Pipeline(memory=Memory(location=None),
         steps=[('placeholder', None),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split',
                                learning_rate=0.49999999999999994, max_depth=-1,
                                min_child_samples=100, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=197, n_jobs=-1,
                                num_leaves=256, objective=None, random_state=42,
                                reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 11:01:22,587:INFO:save_model() successfully completed......................................
2025-05-03 11:01:22,789:INFO:Initializing save_model()
2025-05-03 11:01:22,789:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 11:01:22,789:INFO:Adding model into prep_pipe
2025-05-03 11:01:22,799:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 11:01:22,807:INFO:Pipeline(memory=Memory(location=None),
         steps=[('placeholder', None),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu', early_stopping_rounds=None,
                               enable_categorical=False, eval_metric=None,
                               feature_types=None, feature_weights=None,
                               ga...olicy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 11:01:22,807:INFO:save_model() successfully completed......................................
2025-05-03 11:01:22,986:INFO:Initializing save_model()
2025-05-03 11:01:22,986:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 11:01:22,989:INFO:Adding model into prep_pipe
2025-05-03 11:01:23,142:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 11:01:23,142:INFO:Pipeline(memory=Memory(location=None),
         steps=[('placeholder', None),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 11:01:23,142:INFO:save_model() successfully completed......................................
2025-05-03 11:02:00,063:INFO:Initializing load_model()
2025-05-03 11:02:00,063:INFO:load_model(model_name=../models/LightGBM_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:02:00,086:INFO:Initializing load_model()
2025-05-03 11:02:00,086:INFO:load_model(model_name=../models/XGBoost_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:02:00,131:INFO:Initializing load_model()
2025-05-03 11:02:00,131:INFO:load_model(model_name=../models/RandomForest_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:05:56,035:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)

2025-05-03 11:05:56,035:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)

2025-05-03 11:30:20,894:INFO:Initializing save_model()
2025-05-03 11:30:20,894:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                       steps=[('placeholder', None),
                                              ('trained_model',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_fraction=0.6637577331805016,
                                                              importance_type='split',
                                                              learning_rate=0.499...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                       verbose=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 11:30:20,894:INFO:Adding model into prep_pipe
2025-05-03 11:30:21,360:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 11:30:21,421:INFO:Pipeline(memory=Memory(location=None),
         steps=[('placeholder', None),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                                        steps=[('placeholder',
                                                                None),
                                                               ('trained_model',
                                                                LGBMClassifier(bagging_fraction=0.4,
                                                                               bagging_freq=0,
                                                                               boosting_type='gbdt',
                                                                               class_weight=None,
                                                                               colsample_bytree=...
                                                                                       max_leaf_nodes=None,
                                                                                       max_samples=None,
                                                                                       min_impurity_decrease=4.490672633438402e-06,
                                                                                       min_samples_leaf=2,
                                                                                       min_samples_split=10,
                                                                                       min_weight_fraction_leaf=0.0,
                                                                                       monotonic_cst=None,
                                                                                       n_estimators=300,
                                                                                       n_jobs=-1,
                                                                                       oob_score=False,
                                                                                       random_state=42,
                                                                                       verbose=0,
                                                                                       warm_start=False))],
                                                        verbose=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 11:30:21,421:INFO:save_model() successfully completed......................................
2025-05-03 11:34:20,383:INFO:Initializing save_model()
2025-05-03 11:34:20,383:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                       steps=[('placeholder', None),
                                              ('trained_model',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_fraction=0.6637577331805016,
                                                              importance_type='split',
                                                              learning_rate=0.499...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                       verbose=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 11:34:20,383:INFO:Adding model into prep_pipe
2025-05-03 11:34:20,841:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 11:34:20,894:INFO:Pipeline(memory=Memory(location=None),
         steps=[('placeholder', None),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                                        steps=[('placeholder',
                                                                None),
                                                               ('trained_model',
                                                                LGBMClassifier(bagging_fraction=0.4,
                                                                               bagging_freq=0,
                                                                               boosting_type='gbdt',
                                                                               class_weight=None,
                                                                               colsample_bytree=...
                                                                                       max_leaf_nodes=None,
                                                                                       max_samples=None,
                                                                                       min_impurity_decrease=4.490672633438402e-06,
                                                                                       min_samples_leaf=2,
                                                                                       min_samples_split=10,
                                                                                       min_weight_fraction_leaf=0.0,
                                                                                       monotonic_cst=None,
                                                                                       n_estimators=300,
                                                                                       n_jobs=-1,
                                                                                       oob_score=False,
                                                                                       random_state=42,
                                                                                       verbose=0,
                                                                                       warm_start=False))],
                                                        verbose=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 11:34:20,894:INFO:save_model() successfully completed......................................
2025-05-03 11:42:32,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 11:42:32,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 11:42:32,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 11:42:32,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 11:43:37,099:INFO:PyCaret ClassificationExperiment
2025-05-03 11:43:37,099:INFO:Logging name: clf-default-name
2025-05-03 11:43:37,099:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 11:43:37,099:INFO:version 3.3.2
2025-05-03 11:43:37,099:INFO:Initializing setup()
2025-05-03 11:43:37,099:INFO:self.USI: e479
2025-05-03 11:43:37,099:INFO:self._variable_keys: {'X_train', '_ml_usecase', 'fold_shuffle_param', 'gpu_n_jobs_param', 'exp_id', 'logging_param', 'gpu_param', '_available_plots', 'pipeline', 'fold_groups_param', 'exp_name_log', 'target_param', 'n_jobs_param', 'fix_imbalance', 'idx', 'html_param', 'is_multiclass', 'X_test', 'fold_generator', 'y_test', 'seed', 'memory', 'y', 'X', 'USI', 'data', 'log_plots_param', 'y_train'}
2025-05-03 11:43:37,099:INFO:Checking environment
2025-05-03 11:43:37,099:INFO:python_version: 3.11.11
2025-05-03 11:43:37,099:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 11:43:37,099:INFO:machine: AMD64
2025-05-03 11:43:37,099:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 11:43:37,118:INFO:Memory: svmem(total=16965230592, available=3350740992, percent=80.2, used=13614489600, free=3350740992)
2025-05-03 11:43:37,118:INFO:Physical Core: 4
2025-05-03 11:43:37,118:INFO:Logical Core: 8
2025-05-03 11:43:37,118:INFO:Checking libraries
2025-05-03 11:43:37,118:INFO:System:
2025-05-03 11:43:37,118:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 11:43:37,118:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 11:43:37,118:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 11:43:37,118:INFO:PyCaret required dependencies:
2025-05-03 11:43:37,161:INFO:                 pip: 25.0
2025-05-03 11:43:37,161:INFO:          setuptools: 75.8.0
2025-05-03 11:43:37,161:INFO:             pycaret: 3.3.2
2025-05-03 11:43:37,161:INFO:             IPython: 8.32.0
2025-05-03 11:43:37,161:INFO:          ipywidgets: 8.1.6
2025-05-03 11:43:37,161:INFO:                tqdm: 4.67.1
2025-05-03 11:43:37,161:INFO:               numpy: 1.26.4
2025-05-03 11:43:37,161:INFO:              pandas: 2.1.4
2025-05-03 11:43:37,161:INFO:              jinja2: 3.1.6
2025-05-03 11:43:37,161:INFO:               scipy: 1.11.4
2025-05-03 11:43:37,161:INFO:              joblib: 1.3.2
2025-05-03 11:43:37,161:INFO:             sklearn: 1.4.2
2025-05-03 11:43:37,161:INFO:                pyod: 2.0.5
2025-05-03 11:43:37,161:INFO:            imblearn: 0.13.0
2025-05-03 11:43:37,161:INFO:   category_encoders: 2.7.0
2025-05-03 11:43:37,161:INFO:            lightgbm: 4.6.0
2025-05-03 11:43:37,161:INFO:               numba: 0.61.2
2025-05-03 11:43:37,161:INFO:            requests: 2.32.3
2025-05-03 11:43:37,161:INFO:          matplotlib: 3.7.5
2025-05-03 11:43:37,161:INFO:          scikitplot: 0.3.7
2025-05-03 11:43:37,161:INFO:         yellowbrick: 1.5
2025-05-03 11:43:37,161:INFO:              plotly: 5.24.1
2025-05-03 11:43:37,161:INFO:    plotly-resampler: Not installed
2025-05-03 11:43:37,161:INFO:             kaleido: 0.2.1
2025-05-03 11:43:37,161:INFO:           schemdraw: 0.15
2025-05-03 11:43:37,161:INFO:         statsmodels: 0.14.4
2025-05-03 11:43:37,161:INFO:              sktime: 0.26.0
2025-05-03 11:43:37,161:INFO:               tbats: 1.1.3
2025-05-03 11:43:37,161:INFO:            pmdarima: 2.0.4
2025-05-03 11:43:37,161:INFO:              psutil: 6.1.1
2025-05-03 11:43:37,161:INFO:          markupsafe: 3.0.2
2025-05-03 11:43:37,161:INFO:             pickle5: Not installed
2025-05-03 11:43:37,161:INFO:         cloudpickle: 3.1.1
2025-05-03 11:43:37,161:INFO:         deprecation: 2.1.0
2025-05-03 11:43:37,161:INFO:              xxhash: 3.5.0
2025-05-03 11:43:37,161:INFO:           wurlitzer: Not installed
2025-05-03 11:43:37,161:INFO:PyCaret optional dependencies:
2025-05-03 11:43:37,614:INFO:                shap: 0.47.2
2025-05-03 11:43:37,614:INFO:           interpret: Not installed
2025-05-03 11:43:37,614:INFO:                umap: Not installed
2025-05-03 11:43:37,614:INFO:     ydata_profiling: Not installed
2025-05-03 11:43:37,614:INFO:  explainerdashboard: Not installed
2025-05-03 11:43:37,615:INFO:             autoviz: Not installed
2025-05-03 11:43:37,615:INFO:           fairlearn: Not installed
2025-05-03 11:43:37,615:INFO:          deepchecks: Not installed
2025-05-03 11:43:37,615:INFO:             xgboost: 3.0.0
2025-05-03 11:43:37,615:INFO:            catboost: Not installed
2025-05-03 11:43:37,615:INFO:              kmodes: Not installed
2025-05-03 11:43:37,615:INFO:             mlxtend: Not installed
2025-05-03 11:43:37,615:INFO:       statsforecast: Not installed
2025-05-03 11:43:37,615:INFO:        tune_sklearn: Not installed
2025-05-03 11:43:37,615:INFO:                 ray: Not installed
2025-05-03 11:43:37,615:INFO:            hyperopt: 0.2.7
2025-05-03 11:43:37,615:INFO:              optuna: Not installed
2025-05-03 11:43:37,615:INFO:               skopt: 0.10.2
2025-05-03 11:43:37,615:INFO:              mlflow: 2.22.0
2025-05-03 11:43:37,615:INFO:              gradio: Not installed
2025-05-03 11:43:37,615:INFO:             fastapi: 0.115.12
2025-05-03 11:43:37,615:INFO:             uvicorn: 0.34.2
2025-05-03 11:43:37,615:INFO:              m2cgen: Not installed
2025-05-03 11:43:37,615:INFO:           evidently: Not installed
2025-05-03 11:43:37,615:INFO:               fugue: Not installed
2025-05-03 11:43:37,615:INFO:           streamlit: Not installed
2025-05-03 11:43:37,615:INFO:             prophet: Not installed
2025-05-03 11:43:37,617:INFO:None
2025-05-03 11:43:37,617:INFO:Set up data.
2025-05-03 11:43:37,666:INFO:Set up folding strategy.
2025-05-03 11:43:37,666:INFO:Set up train/test split.
2025-05-03 11:43:37,699:INFO:Set up index.
2025-05-03 11:43:37,699:INFO:Assigning column types.
2025-05-03 11:43:37,699:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 11:43:37,766:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 11:43:37,766:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 11:43:37,812:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:37,814:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:37,866:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 11:43:37,866:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 11:43:37,900:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:37,904:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:37,904:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 11:43:37,965:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 11:43:37,990:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:37,990:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,032:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 11:43:38,069:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:38,071:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,071:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 11:43:38,149:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:38,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,257:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:38,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,271:INFO:Set up column name cleaning.
2025-05-03 11:43:38,287:INFO:Finished creating preprocessing pipeline.
2025-05-03 11:43:38,291:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 11:43:38,291:INFO:Creating final display dataframe.
2025-05-03 11:43:38,399:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features            4
8         Categorical features            7
2025-05-03 11:43:38,493:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:38,498:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,699:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 11:43:38,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 11:43:38,708:INFO:setup() successfully completed in 1.61s...............
2025-05-03 11:43:38,708:INFO:Initializing load_model()
2025-05-03 11:43:38,708:INFO:load_model(model_name=../models/LightGBM_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:43:38,717:INFO:Initializing load_model()
2025-05-03 11:43:38,717:INFO:load_model(model_name=../models/XGBoost_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:43:38,723:INFO:Initializing load_model()
2025-05-03 11:43:38,723:INFO:load_model(model_name=../models/RandomForest_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 11:43:38,865:INFO:Initializing blend_models()
2025-05-03 11:43:38,865:INFO:blend_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A84EC34150>, estimator_list=[Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split',
                                learning_rate=0.49999999999999994, max_depth=-1,
                                min_child_samples=100, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=197, n_jobs=-1,
                                num_leaves=256, objective=None, random_state=42,
                                reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu', early_stopping_rounds=None,
                               enable_categorical=False, eval_metric=None,
                               fe...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False), Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)], fold=None, round=4, choose_better=False, optimize=F1, method=soft, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2025-05-03 11:43:38,865:INFO:Checking exceptions
2025-05-03 11:43:38,898:INFO:Importing libraries
2025-05-03 11:43:38,898:INFO:Copying training dataset
2025-05-03 11:43:38,914:INFO:Getting model names
2025-05-03 11:43:38,920:INFO:SubProcess create_model() called ==================================
2025-05-03 11:43:38,953:INFO:Initializing create_model()
2025-05-03 11:43:38,953:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001A84EC34150>, estimator=VotingClassifier(estimators=[('Light Gradient Boosting Machine',
                              Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                       steps=[('placeholder', None),
                                              ('trained_model',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_fraction=0.6637577331805016,
                                                              importance_type='sp...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                       verbose=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001A84F7E4BD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 11:43:38,953:INFO:Checking exceptions
2025-05-03 11:43:38,953:INFO:Importing libraries
2025-05-03 11:43:38,953:INFO:Copying training dataset
2025-05-03 11:43:38,966:INFO:Defining folds
2025-05-03 11:43:38,966:INFO:Declaring metric variables
2025-05-03 11:43:38,974:INFO:Importing untrained model
2025-05-03 11:43:38,974:INFO:Declaring custom model
2025-05-03 11:43:38,993:INFO:Voting Classifier Imported successfully
2025-05-03 11:43:38,999:INFO:Starting cross validation
2025-05-03 11:43:38,999:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 11:52:24,287:INFO:Initializing plot_model()
2025-05-03 11:52:24,287:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=VotingClassifier(estimators=[('lgbm',
                              Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
                                       steps=[('placeholder', None),
                                              ('trained_model',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_fraction=0.6637577331805016,
                                                              importance_type='split',
                                                              learning_rate=0.499...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                       verbose=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 11:52:24,287:INFO:Checking exceptions
2025-05-03 11:52:45,526:INFO:Initializing plot_model()
2025-05-03 11:52:45,526:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=<class 'sklearn.pipeline.Pipeline'>, plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 11:52:45,526:INFO:Checking exceptions
2025-05-03 11:53:48,863:INFO:Initializing interpret_model()
2025-05-03 11:53:48,863:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split',
                                learning_rate=0.49999999999999994, max_depth=-1,
                                min_child_samples=100, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=197, n_jobs=-1,
                                num_leaves=256, objective=None, random_state=42,
                                reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 11:53:48,863:INFO:Checking exceptions
2025-05-03 11:53:48,863:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 11:54:12,959:INFO:Initializing interpret_model()
2025-05-03 11:54:12,959:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 11:54:12,959:INFO:Checking exceptions
2025-05-03 11:54:12,959:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 11:54:12,980:INFO:plot type: summary
2025-05-03 11:54:12,980:INFO:Creating TreeExplainer
2025-05-03 11:54:13,096:INFO:Compiling shap values
2025-05-03 11:54:14,875:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn(

2025-05-03 11:54:14,875:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)

2025-05-03 11:54:16,423:INFO:Visual Rendered Successfully
2025-05-03 11:54:16,423:INFO:interpret_model() successfully completed......................................
2025-05-03 11:58:17,461:INFO:Initializing plot_model()
2025-05-03 11:58:17,461:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 11:58:17,461:INFO:Checking exceptions
2025-05-03 11:58:37,846:INFO:Initializing plot_model()
2025-05-03 11:58:37,846:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 11:58:37,848:INFO:Checking exceptions
2025-05-03 11:58:37,899:INFO:Preloading libraries
2025-05-03 11:58:38,038:INFO:Copying training dataset
2025-05-03 11:58:38,040:INFO:Plot type: feature
2025-05-03 11:58:38,040:WARNING:No coef_ found. Trying feature_importances_
2025-05-03 11:58:38,258:INFO:Visual Rendered Successfully
2025-05-03 11:58:38,538:INFO:plot_model() successfully completed......................................
2025-05-03 11:59:13,735:INFO:Initializing interpret_model()
2025-05-03 11:59:13,735:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 11:59:13,735:INFO:Checking exceptions
2025-05-03 11:59:13,735:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 11:59:13,762:INFO:plot type: summary
2025-05-03 11:59:13,762:INFO:Creating TreeExplainer
2025-05-03 11:59:20,297:INFO:Initializing interpret_model()
2025-05-03 11:59:20,297:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002B104067750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 11:59:20,297:INFO:Checking exceptions
2025-05-03 11:59:20,297:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 11:59:20,324:INFO:plot type: summary
2025-05-03 11:59:20,324:INFO:Creating TreeExplainer
2025-05-03 11:59:20,423:INFO:Compiling shap values
2025-05-03 12:02:36,179:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:02:36,179:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:02:36,179:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:02:36,179:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:02:40,031:INFO:PyCaret ClassificationExperiment
2025-05-03 12:02:40,031:INFO:Logging name: clf-default-name
2025-05-03 12:02:40,031:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 12:02:40,031:INFO:version 3.3.2
2025-05-03 12:02:40,031:INFO:Initializing setup()
2025-05-03 12:02:40,031:INFO:self.USI: 5c5c
2025-05-03 12:02:40,031:INFO:self._variable_keys: {'X_train', 'USI', 'fold_groups_param', 'logging_param', 'seed', 'memory', 'is_multiclass', 'n_jobs_param', 'target_param', 'fix_imbalance', 'X', 'exp_id', '_available_plots', 'exp_name_log', 'y_test', 'idx', 'data', 'html_param', 'y_train', 'gpu_param', 'y', 'log_plots_param', 'gpu_n_jobs_param', 'X_test', 'fold_generator', '_ml_usecase', 'pipeline', 'fold_shuffle_param'}
2025-05-03 12:02:40,031:INFO:Checking environment
2025-05-03 12:02:40,031:INFO:python_version: 3.11.11
2025-05-03 12:02:40,031:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 12:02:40,031:INFO:machine: AMD64
2025-05-03 12:02:40,031:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 12:02:40,038:INFO:Memory: svmem(total=16965230592, available=3380105216, percent=80.1, used=13585125376, free=3380105216)
2025-05-03 12:02:40,038:INFO:Physical Core: 4
2025-05-03 12:02:40,038:INFO:Logical Core: 8
2025-05-03 12:02:40,038:INFO:Checking libraries
2025-05-03 12:02:40,038:INFO:System:
2025-05-03 12:02:40,038:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 12:02:40,040:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 12:02:40,040:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 12:02:40,040:INFO:PyCaret required dependencies:
2025-05-03 12:02:40,042:INFO:                 pip: 25.0
2025-05-03 12:02:40,042:INFO:          setuptools: 75.8.0
2025-05-03 12:02:40,042:INFO:             pycaret: 3.3.2
2025-05-03 12:02:40,042:INFO:             IPython: 8.32.0
2025-05-03 12:02:40,042:INFO:          ipywidgets: 8.1.6
2025-05-03 12:02:40,042:INFO:                tqdm: 4.67.1
2025-05-03 12:02:40,042:INFO:               numpy: 1.26.4
2025-05-03 12:02:40,042:INFO:              pandas: 2.1.4
2025-05-03 12:02:40,042:INFO:              jinja2: 3.1.6
2025-05-03 12:02:40,042:INFO:               scipy: 1.11.4
2025-05-03 12:02:40,042:INFO:              joblib: 1.3.2
2025-05-03 12:02:40,042:INFO:             sklearn: 1.4.2
2025-05-03 12:02:40,042:INFO:                pyod: 2.0.5
2025-05-03 12:02:40,042:INFO:            imblearn: 0.13.0
2025-05-03 12:02:40,042:INFO:   category_encoders: 2.7.0
2025-05-03 12:02:40,042:INFO:            lightgbm: 4.6.0
2025-05-03 12:02:40,042:INFO:               numba: 0.61.2
2025-05-03 12:02:40,042:INFO:            requests: 2.32.3
2025-05-03 12:02:40,042:INFO:          matplotlib: 3.7.5
2025-05-03 12:02:40,042:INFO:          scikitplot: 0.3.7
2025-05-03 12:02:40,042:INFO:         yellowbrick: 1.5
2025-05-03 12:02:40,042:INFO:              plotly: 5.24.1
2025-05-03 12:02:40,042:INFO:    plotly-resampler: Not installed
2025-05-03 12:02:40,042:INFO:             kaleido: 0.2.1
2025-05-03 12:02:40,042:INFO:           schemdraw: 0.15
2025-05-03 12:02:40,042:INFO:         statsmodels: 0.14.4
2025-05-03 12:02:40,042:INFO:              sktime: 0.26.0
2025-05-03 12:02:40,045:INFO:               tbats: 1.1.3
2025-05-03 12:02:40,045:INFO:            pmdarima: 2.0.4
2025-05-03 12:02:40,045:INFO:              psutil: 6.1.1
2025-05-03 12:02:40,045:INFO:          markupsafe: 3.0.2
2025-05-03 12:02:40,045:INFO:             pickle5: Not installed
2025-05-03 12:02:40,045:INFO:         cloudpickle: 3.1.1
2025-05-03 12:02:40,045:INFO:         deprecation: 2.1.0
2025-05-03 12:02:40,045:INFO:              xxhash: 3.5.0
2025-05-03 12:02:40,045:INFO:           wurlitzer: Not installed
2025-05-03 12:02:40,045:INFO:PyCaret optional dependencies:
2025-05-03 12:02:40,402:INFO:                shap: 0.47.2
2025-05-03 12:02:40,402:INFO:           interpret: Not installed
2025-05-03 12:02:40,402:INFO:                umap: Not installed
2025-05-03 12:02:40,402:INFO:     ydata_profiling: Not installed
2025-05-03 12:02:40,402:INFO:  explainerdashboard: Not installed
2025-05-03 12:02:40,402:INFO:             autoviz: Not installed
2025-05-03 12:02:40,402:INFO:           fairlearn: Not installed
2025-05-03 12:02:40,402:INFO:          deepchecks: Not installed
2025-05-03 12:02:40,402:INFO:             xgboost: 3.0.0
2025-05-03 12:02:40,402:INFO:            catboost: Not installed
2025-05-03 12:02:40,402:INFO:              kmodes: Not installed
2025-05-03 12:02:40,402:INFO:             mlxtend: Not installed
2025-05-03 12:02:40,402:INFO:       statsforecast: Not installed
2025-05-03 12:02:40,402:INFO:        tune_sklearn: Not installed
2025-05-03 12:02:40,402:INFO:                 ray: Not installed
2025-05-03 12:02:40,402:INFO:            hyperopt: 0.2.7
2025-05-03 12:02:40,402:INFO:              optuna: Not installed
2025-05-03 12:02:40,402:INFO:               skopt: 0.10.2
2025-05-03 12:02:40,402:INFO:              mlflow: 2.22.0
2025-05-03 12:02:40,402:INFO:              gradio: Not installed
2025-05-03 12:02:40,402:INFO:             fastapi: 0.115.12
2025-05-03 12:02:40,402:INFO:             uvicorn: 0.34.2
2025-05-03 12:02:40,402:INFO:              m2cgen: Not installed
2025-05-03 12:02:40,402:INFO:           evidently: Not installed
2025-05-03 12:02:40,402:INFO:               fugue: Not installed
2025-05-03 12:02:40,402:INFO:           streamlit: Not installed
2025-05-03 12:02:40,402:INFO:             prophet: Not installed
2025-05-03 12:02:40,402:INFO:None
2025-05-03 12:02:40,402:INFO:Set up data.
2025-05-03 12:02:40,409:INFO:Set up folding strategy.
2025-05-03 12:02:40,409:INFO:Set up train/test split.
2025-05-03 12:02:40,438:INFO:Set up index.
2025-05-03 12:02:40,442:INFO:Assigning column types.
2025-05-03 12:02:40,454:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 12:02:40,501:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,512:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,542:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,542:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,592:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,592:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,629:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,629:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,629:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 12:02:40,692:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,721:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,724:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,768:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:02:40,792:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,792:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,792:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 12:02:40,872:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,875:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,942:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:40,942:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:40,969:INFO:Finished creating preprocessing pipeline.
2025-05-03 12:02:40,969:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 12:02:40,969:INFO:Creating final display dataframe.
2025-05-03 12:02:41,058:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 12:02:41,125:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:41,125:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:41,193:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:02:41,208:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:02:41,208:INFO:setup() successfully completed in 1.18s...............
2025-05-03 12:02:41,225:INFO:Initializing compare_models()
2025-05-03 12:02:41,225:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 12:02:41,225:INFO:Checking exceptions
2025-05-03 12:02:41,230:INFO:Preparing display monitor
2025-05-03 12:02:41,388:INFO:Initializing Logistic Regression
2025-05-03 12:02:41,388:INFO:Total runtime is 1.3907750447591146e-05 minutes
2025-05-03 12:02:41,392:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:41,394:INFO:Initializing create_model()
2025-05-03 12:02:41,394:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:41,394:INFO:Checking exceptions
2025-05-03 12:02:41,394:INFO:Importing libraries
2025-05-03 12:02:41,394:INFO:Copying training dataset
2025-05-03 12:02:41,420:INFO:Defining folds
2025-05-03 12:02:41,420:INFO:Declaring metric variables
2025-05-03 12:02:41,425:INFO:Importing untrained model
2025-05-03 12:02:41,427:INFO:Logistic Regression Imported successfully
2025-05-03 12:02:41,436:INFO:Starting cross validation
2025-05-03 12:02:41,436:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:45,065:INFO:Calculating mean and std
2025-05-03 12:02:45,065:INFO:Creating metrics dataframe
2025-05-03 12:02:45,067:INFO:Uploading results into container
2025-05-03 12:02:45,067:INFO:Uploading model into container now
2025-05-03 12:02:45,067:INFO:_master_model_container: 1
2025-05-03 12:02:45,067:INFO:_display_container: 2
2025-05-03 12:02:45,067:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 12:02:45,067:INFO:create_model() successfully completed......................................
2025-05-03 12:02:45,198:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:45,198:INFO:Creating metrics dataframe
2025-05-03 12:02:45,210:INFO:Initializing Random Forest Classifier
2025-05-03 12:02:45,210:INFO:Total runtime is 0.06371330817540487 minutes
2025-05-03 12:02:45,210:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:45,210:INFO:Initializing create_model()
2025-05-03 12:02:45,210:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:45,210:INFO:Checking exceptions
2025-05-03 12:02:45,210:INFO:Importing libraries
2025-05-03 12:02:45,210:INFO:Copying training dataset
2025-05-03 12:02:45,227:INFO:Defining folds
2025-05-03 12:02:45,227:INFO:Declaring metric variables
2025-05-03 12:02:45,227:INFO:Importing untrained model
2025-05-03 12:02:45,242:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:02:45,242:INFO:Starting cross validation
2025-05-03 12:02:45,242:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:49,277:INFO:Calculating mean and std
2025-05-03 12:02:49,280:INFO:Creating metrics dataframe
2025-05-03 12:02:49,283:INFO:Uploading results into container
2025-05-03 12:02:49,283:INFO:Uploading model into container now
2025-05-03 12:02:49,285:INFO:_master_model_container: 2
2025-05-03 12:02:49,285:INFO:_display_container: 2
2025-05-03 12:02:49,287:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:02:49,287:INFO:create_model() successfully completed......................................
2025-05-03 12:02:49,406:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:49,406:INFO:Creating metrics dataframe
2025-05-03 12:02:49,406:INFO:Initializing Extreme Gradient Boosting
2025-05-03 12:02:49,406:INFO:Total runtime is 0.13364933729171752 minutes
2025-05-03 12:02:49,422:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:49,422:INFO:Initializing create_model()
2025-05-03 12:02:49,422:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:49,422:INFO:Checking exceptions
2025-05-03 12:02:49,422:INFO:Importing libraries
2025-05-03 12:02:49,422:INFO:Copying training dataset
2025-05-03 12:02:49,454:INFO:Defining folds
2025-05-03 12:02:49,454:INFO:Declaring metric variables
2025-05-03 12:02:49,454:INFO:Importing untrained model
2025-05-03 12:02:49,467:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:02:49,469:INFO:Starting cross validation
2025-05-03 12:02:49,469:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:52,630:INFO:Calculating mean and std
2025-05-03 12:02:52,632:INFO:Creating metrics dataframe
2025-05-03 12:02:52,632:INFO:Uploading results into container
2025-05-03 12:02:52,632:INFO:Uploading model into container now
2025-05-03 12:02:52,632:INFO:_master_model_container: 3
2025-05-03 12:02:52,632:INFO:_display_container: 2
2025-05-03 12:02:52,632:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:02:52,632:INFO:create_model() successfully completed......................................
2025-05-03 12:02:52,749:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:52,749:INFO:Creating metrics dataframe
2025-05-03 12:02:52,766:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 12:02:52,766:INFO:Total runtime is 0.18963452180226642 minutes
2025-05-03 12:02:52,766:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:52,766:INFO:Initializing create_model()
2025-05-03 12:02:52,766:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:52,766:INFO:Checking exceptions
2025-05-03 12:02:52,766:INFO:Importing libraries
2025-05-03 12:02:52,766:INFO:Copying training dataset
2025-05-03 12:02:52,788:INFO:Defining folds
2025-05-03 12:02:52,788:INFO:Declaring metric variables
2025-05-03 12:02:52,798:INFO:Importing untrained model
2025-05-03 12:02:52,804:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:02:52,804:INFO:Starting cross validation
2025-05-03 12:02:52,812:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:53,287:INFO:Calculating mean and std
2025-05-03 12:02:53,287:INFO:Creating metrics dataframe
2025-05-03 12:02:53,290:INFO:Uploading results into container
2025-05-03 12:02:53,292:INFO:Uploading model into container now
2025-05-03 12:02:53,292:INFO:_master_model_container: 4
2025-05-03 12:02:53,292:INFO:_display_container: 2
2025-05-03 12:02:53,292:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:02:53,292:INFO:create_model() successfully completed......................................
2025-05-03 12:02:53,431:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:53,431:INFO:Creating metrics dataframe
2025-05-03 12:02:53,448:INFO:Initializing Extra Trees Classifier
2025-05-03 12:02:53,448:INFO:Total runtime is 0.20101420879364013 minutes
2025-05-03 12:02:53,448:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:53,448:INFO:Initializing create_model()
2025-05-03 12:02:53,448:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:53,448:INFO:Checking exceptions
2025-05-03 12:02:53,448:INFO:Importing libraries
2025-05-03 12:02:53,448:INFO:Copying training dataset
2025-05-03 12:02:53,472:INFO:Defining folds
2025-05-03 12:02:53,472:INFO:Declaring metric variables
2025-05-03 12:02:53,481:INFO:Importing untrained model
2025-05-03 12:02:53,487:INFO:Extra Trees Classifier Imported successfully
2025-05-03 12:02:53,489:INFO:Starting cross validation
2025-05-03 12:02:53,489:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:54,615:INFO:Calculating mean and std
2025-05-03 12:02:54,615:INFO:Creating metrics dataframe
2025-05-03 12:02:54,615:INFO:Uploading results into container
2025-05-03 12:02:54,615:INFO:Uploading model into container now
2025-05-03 12:02:54,615:INFO:_master_model_container: 5
2025-05-03 12:02:54,615:INFO:_display_container: 2
2025-05-03 12:02:54,615:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 12:02:54,615:INFO:create_model() successfully completed......................................
2025-05-03 12:02:54,747:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:54,747:INFO:Creating metrics dataframe
2025-05-03 12:02:54,757:INFO:Initializing Ridge Classifier
2025-05-03 12:02:54,757:INFO:Total runtime is 0.2228328506151835 minutes
2025-05-03 12:02:54,763:INFO:SubProcess create_model() called ==================================
2025-05-03 12:02:54,763:INFO:Initializing create_model()
2025-05-03 12:02:54,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AC43EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:54,763:INFO:Checking exceptions
2025-05-03 12:02:54,763:INFO:Importing libraries
2025-05-03 12:02:54,763:INFO:Copying training dataset
2025-05-03 12:02:54,787:INFO:Defining folds
2025-05-03 12:02:54,787:INFO:Declaring metric variables
2025-05-03 12:02:54,794:INFO:Importing untrained model
2025-05-03 12:02:54,797:INFO:Ridge Classifier Imported successfully
2025-05-03 12:02:54,797:INFO:Starting cross validation
2025-05-03 12:02:54,797:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:54,909:INFO:Calculating mean and std
2025-05-03 12:02:54,909:INFO:Creating metrics dataframe
2025-05-03 12:02:54,913:INFO:Uploading results into container
2025-05-03 12:02:54,913:INFO:Uploading model into container now
2025-05-03 12:02:54,913:INFO:_master_model_container: 6
2025-05-03 12:02:54,913:INFO:_display_container: 2
2025-05-03 12:02:54,913:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 12:02:54,913:INFO:create_model() successfully completed......................................
2025-05-03 12:02:55,050:INFO:SubProcess create_model() end ==================================
2025-05-03 12:02:55,050:INFO:Creating metrics dataframe
2025-05-03 12:02:55,063:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 12:02:55,080:INFO:Initializing create_model()
2025-05-03 12:02:55,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:55,080:INFO:Checking exceptions
2025-05-03 12:02:55,086:INFO:Importing libraries
2025-05-03 12:02:55,086:INFO:Copying training dataset
2025-05-03 12:02:55,096:INFO:Defining folds
2025-05-03 12:02:55,096:INFO:Declaring metric variables
2025-05-03 12:02:55,096:INFO:Importing untrained model
2025-05-03 12:02:55,096:INFO:Declaring custom model
2025-05-03 12:02:55,107:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:02:55,107:INFO:Cross validation set to False
2025-05-03 12:02:55,107:INFO:Fitting Model
2025-05-03 12:02:55,130:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:02:55,134:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.
2025-05-03 12:02:55,134:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:02:55,134:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:02:55,134:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:02:55,134:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:02:55,136:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:02:55,136:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:02:55,308:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:02:55,308:INFO:create_model() successfully completed......................................
2025-05-03 12:02:55,473:INFO:_master_model_container: 6
2025-05-03 12:02:55,473:INFO:_display_container: 2
2025-05-03 12:02:55,473:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:02:55,473:INFO:compare_models() successfully completed......................................
2025-05-03 12:02:55,489:INFO:Initializing create_model()
2025-05-03 12:02:55,489:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:55,489:INFO:Checking exceptions
2025-05-03 12:02:55,514:INFO:Importing libraries
2025-05-03 12:02:55,514:INFO:Copying training dataset
2025-05-03 12:02:55,537:INFO:Defining folds
2025-05-03 12:02:55,537:INFO:Declaring metric variables
2025-05-03 12:02:55,541:INFO:Importing untrained model
2025-05-03 12:02:55,550:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:02:55,555:INFO:Starting cross validation
2025-05-03 12:02:55,555:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:56,036:INFO:Calculating mean and std
2025-05-03 12:02:56,036:INFO:Creating metrics dataframe
2025-05-03 12:02:56,042:INFO:Finalizing model
2025-05-03 12:02:56,064:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:02:56,068:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000974 seconds.
2025-05-03 12:02:56,068:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:02:56,068:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:02:56,068:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:02:56,068:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:02:56,068:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:02:56,068:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:02:56,191:INFO:Uploading results into container
2025-05-03 12:02:56,193:INFO:Uploading model into container now
2025-05-03 12:02:56,202:INFO:_master_model_container: 7
2025-05-03 12:02:56,202:INFO:_display_container: 3
2025-05-03 12:02:56,204:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:02:56,204:INFO:create_model() successfully completed......................................
2025-05-03 12:02:56,410:INFO:Initializing create_model()
2025-05-03 12:02:56,410:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:56,410:INFO:Checking exceptions
2025-05-03 12:02:56,428:INFO:Importing libraries
2025-05-03 12:02:56,428:INFO:Copying training dataset
2025-05-03 12:02:56,458:INFO:Defining folds
2025-05-03 12:02:56,458:INFO:Declaring metric variables
2025-05-03 12:02:56,467:INFO:Importing untrained model
2025-05-03 12:02:56,467:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:02:56,489:INFO:Starting cross validation
2025-05-03 12:02:56,490:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:56,828:INFO:Calculating mean and std
2025-05-03 12:02:56,828:INFO:Creating metrics dataframe
2025-05-03 12:02:56,828:INFO:Finalizing model
2025-05-03 12:02:56,962:INFO:Uploading results into container
2025-05-03 12:02:56,962:INFO:Uploading model into container now
2025-05-03 12:02:56,971:INFO:_master_model_container: 8
2025-05-03 12:02:56,971:INFO:_display_container: 4
2025-05-03 12:02:56,973:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:02:56,974:INFO:create_model() successfully completed......................................
2025-05-03 12:02:57,144:INFO:Initializing create_model()
2025-05-03 12:02:57,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:02:57,144:INFO:Checking exceptions
2025-05-03 12:02:57,166:INFO:Importing libraries
2025-05-03 12:02:57,166:INFO:Copying training dataset
2025-05-03 12:02:57,192:INFO:Defining folds
2025-05-03 12:02:57,192:INFO:Declaring metric variables
2025-05-03 12:02:57,192:INFO:Importing untrained model
2025-05-03 12:02:57,199:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:02:57,199:INFO:Starting cross validation
2025-05-03 12:02:57,199:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:02:58,716:INFO:Calculating mean and std
2025-05-03 12:02:58,716:INFO:Creating metrics dataframe
2025-05-03 12:02:58,719:INFO:Finalizing model
2025-05-03 12:02:59,269:INFO:Uploading results into container
2025-05-03 12:02:59,269:INFO:Uploading model into container now
2025-05-03 12:02:59,299:INFO:_master_model_container: 9
2025-05-03 12:02:59,299:INFO:_display_container: 5
2025-05-03 12:02:59,299:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:02:59,299:INFO:create_model() successfully completed......................................
2025-05-03 12:02:59,509:INFO:Initializing tune_model()
2025-05-03 12:02:59,509:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:02:59,509:INFO:Checking exceptions
2025-05-03 12:02:59,509:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:02:59,535:INFO:Copying training dataset
2025-05-03 12:02:59,549:INFO:Checking base model
2025-05-03 12:02:59,558:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 12:02:59,558:INFO:Declaring metric variables
2025-05-03 12:02:59,566:INFO:Defining Hyperparameters
2025-05-03 12:02:59,696:INFO:Tuning with n_jobs=-1
2025-05-03 12:02:59,696:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:05:00,910:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 12:05:00,910:INFO:Hyperparameter search completed
2025-05-03 12:05:00,910:INFO:SubProcess create_model() called ==================================
2025-05-03 12:05:00,910:INFO:Initializing create_model()
2025-05-03 12:05:00,910:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AA86EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 12:05:00,910:INFO:Checking exceptions
2025-05-03 12:05:00,910:INFO:Importing libraries
2025-05-03 12:05:00,910:INFO:Copying training dataset
2025-05-03 12:05:00,926:INFO:Defining folds
2025-05-03 12:05:00,926:INFO:Declaring metric variables
2025-05-03 12:05:00,938:INFO:Importing untrained model
2025-05-03 12:05:00,938:INFO:Declaring custom model
2025-05-03 12:05:00,943:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:05:00,954:INFO:Starting cross validation
2025-05-03 12:05:00,954:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:05:01,332:INFO:Calculating mean and std
2025-05-03 12:05:01,334:INFO:Creating metrics dataframe
2025-05-03 12:05:01,340:INFO:Finalizing model
2025-05-03 12:05:01,347:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:05:01,347:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:05:01,347:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:05:01,363:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:05:01,363:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:05:01,363:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:05:01,363:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:05:01,366:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.
2025-05-03 12:05:01,366:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:05:01,366:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:05:01,366:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:05:01,366:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:05:01,368:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:05:01,368:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:05:01,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,461:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,463:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,467:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,469:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,471:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,473:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,475:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,475:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,482:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,484:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,486:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,488:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,490:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,492:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,494:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,496:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,498:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,500:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,507:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,509:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:05:01,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:05:01,521:INFO:Uploading results into container
2025-05-03 12:05:01,523:INFO:Uploading model into container now
2025-05-03 12:05:01,523:INFO:_master_model_container: 10
2025-05-03 12:05:01,523:INFO:_display_container: 6
2025-05-03 12:05:01,524:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:05:01,524:INFO:create_model() successfully completed......................................
2025-05-03 12:05:01,675:INFO:SubProcess create_model() end ==================================
2025-05-03 12:05:01,675:INFO:choose_better activated
2025-05-03 12:05:01,676:INFO:SubProcess create_model() called ==================================
2025-05-03 12:05:01,676:INFO:Initializing create_model()
2025-05-03 12:05:01,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:05:01,687:INFO:Checking exceptions
2025-05-03 12:05:01,690:INFO:Importing libraries
2025-05-03 12:05:01,690:INFO:Copying training dataset
2025-05-03 12:05:01,721:INFO:Defining folds
2025-05-03 12:05:01,721:INFO:Declaring metric variables
2025-05-03 12:05:01,721:INFO:Importing untrained model
2025-05-03 12:05:01,721:INFO:Declaring custom model
2025-05-03 12:05:01,726:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:05:01,726:INFO:Starting cross validation
2025-05-03 12:05:01,729:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:05:02,232:INFO:Calculating mean and std
2025-05-03 12:05:02,232:INFO:Creating metrics dataframe
2025-05-03 12:05:02,234:INFO:Finalizing model
2025-05-03 12:05:02,258:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:05:02,260:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.
2025-05-03 12:05:02,260:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:05:02,260:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:05:02,260:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:05:02,260:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:05:02,260:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:05:02,260:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:05:02,383:INFO:Uploading results into container
2025-05-03 12:05:02,385:INFO:Uploading model into container now
2025-05-03 12:05:02,385:INFO:_master_model_container: 11
2025-05-03 12:05:02,385:INFO:_display_container: 7
2025-05-03 12:05:02,385:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:05:02,385:INFO:create_model() successfully completed......................................
2025-05-03 12:05:02,525:INFO:SubProcess create_model() end ==================================
2025-05-03 12:05:02,525:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 12:05:02,525:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 12:05:02,525:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 12:05:02,525:INFO:choose_better completed
2025-05-03 12:05:02,534:INFO:_master_model_container: 11
2025-05-03 12:05:02,534:INFO:_display_container: 6
2025-05-03 12:05:02,534:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:05:02,534:INFO:tune_model() successfully completed......................................
2025-05-03 12:05:02,702:INFO:Initializing tune_model()
2025-05-03 12:05:02,702:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:05:02,702:INFO:Checking exceptions
2025-05-03 12:05:02,702:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:05:02,727:INFO:Copying training dataset
2025-05-03 12:05:02,743:INFO:Checking base model
2025-05-03 12:05:02,743:INFO:Base model : Extreme Gradient Boosting
2025-05-03 12:05:02,743:INFO:Declaring metric variables
2025-05-03 12:05:02,751:INFO:Defining Hyperparameters
2025-05-03 12:05:02,873:INFO:Tuning with n_jobs=-1
2025-05-03 12:05:02,873:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:06:55,499:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 12:06:55,499:INFO:Hyperparameter search completed
2025-05-03 12:06:55,499:INFO:SubProcess create_model() called ==================================
2025-05-03 12:06:55,499:INFO:Initializing create_model()
2025-05-03 12:06:55,499:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AB13EA90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 12:06:55,499:INFO:Checking exceptions
2025-05-03 12:06:55,499:INFO:Importing libraries
2025-05-03 12:06:55,499:INFO:Copying training dataset
2025-05-03 12:06:55,515:INFO:Defining folds
2025-05-03 12:06:55,515:INFO:Declaring metric variables
2025-05-03 12:06:55,529:INFO:Importing untrained model
2025-05-03 12:06:55,529:INFO:Declaring custom model
2025-05-03 12:06:55,536:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:06:55,536:INFO:Starting cross validation
2025-05-03 12:06:55,536:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:06:55,962:INFO:Calculating mean and std
2025-05-03 12:06:55,962:INFO:Creating metrics dataframe
2025-05-03 12:06:55,965:INFO:Finalizing model
2025-05-03 12:06:56,159:INFO:Uploading results into container
2025-05-03 12:06:56,159:INFO:Uploading model into container now
2025-05-03 12:06:56,161:INFO:_master_model_container: 12
2025-05-03 12:06:56,161:INFO:_display_container: 7
2025-05-03 12:06:56,162:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 12:06:56,163:INFO:create_model() successfully completed......................................
2025-05-03 12:06:56,298:INFO:SubProcess create_model() end ==================================
2025-05-03 12:06:56,298:INFO:choose_better activated
2025-05-03 12:06:56,298:INFO:SubProcess create_model() called ==================================
2025-05-03 12:06:56,298:INFO:Initializing create_model()
2025-05-03 12:06:56,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:06:56,298:INFO:Checking exceptions
2025-05-03 12:06:56,315:INFO:Importing libraries
2025-05-03 12:06:56,315:INFO:Copying training dataset
2025-05-03 12:06:56,337:INFO:Defining folds
2025-05-03 12:06:56,337:INFO:Declaring metric variables
2025-05-03 12:06:56,337:INFO:Importing untrained model
2025-05-03 12:06:56,337:INFO:Declaring custom model
2025-05-03 12:06:56,337:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:06:56,337:INFO:Starting cross validation
2025-05-03 12:06:56,337:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:06:56,666:INFO:Calculating mean and std
2025-05-03 12:06:56,666:INFO:Creating metrics dataframe
2025-05-03 12:06:56,666:INFO:Finalizing model
2025-05-03 12:06:56,790:INFO:Uploading results into container
2025-05-03 12:06:56,790:INFO:Uploading model into container now
2025-05-03 12:06:56,792:INFO:_master_model_container: 13
2025-05-03 12:06:56,792:INFO:_display_container: 8
2025-05-03 12:06:56,792:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:06:56,792:INFO:create_model() successfully completed......................................
2025-05-03 12:06:56,931:INFO:SubProcess create_model() end ==================================
2025-05-03 12:06:56,931:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 12:06:56,938:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 12:06:56,939:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 12:06:56,939:INFO:choose_better completed
2025-05-03 12:06:56,951:INFO:_master_model_container: 13
2025-05-03 12:06:56,951:INFO:_display_container: 7
2025-05-03 12:06:56,951:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 12:06:56,951:INFO:tune_model() successfully completed......................................
2025-05-03 12:06:57,101:INFO:Initializing tune_model()
2025-05-03 12:06:57,101:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:06:57,101:INFO:Checking exceptions
2025-05-03 12:06:57,101:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:06:57,138:INFO:Copying training dataset
2025-05-03 12:06:57,152:INFO:Checking base model
2025-05-03 12:06:57,152:INFO:Base model : Random Forest Classifier
2025-05-03 12:06:57,156:INFO:Declaring metric variables
2025-05-03 12:06:57,161:INFO:Defining Hyperparameters
2025-05-03 12:06:57,282:INFO:Tuning with n_jobs=-1
2025-05-03 12:06:57,282:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:10:50,497:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 12:10:50,505:INFO:Hyperparameter search completed
2025-05-03 12:10:50,505:INFO:SubProcess create_model() called ==================================
2025-05-03 12:10:50,505:INFO:Initializing create_model()
2025-05-03 12:10:50,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000225AA8A6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 12:10:50,505:INFO:Checking exceptions
2025-05-03 12:10:50,505:INFO:Importing libraries
2025-05-03 12:10:50,505:INFO:Copying training dataset
2025-05-03 12:10:50,522:INFO:Defining folds
2025-05-03 12:10:50,522:INFO:Declaring metric variables
2025-05-03 12:10:50,522:INFO:Importing untrained model
2025-05-03 12:10:50,522:INFO:Declaring custom model
2025-05-03 12:10:50,522:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:10:50,538:INFO:Starting cross validation
2025-05-03 12:10:50,538:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:10:54,393:INFO:Calculating mean and std
2025-05-03 12:10:54,393:INFO:Creating metrics dataframe
2025-05-03 12:10:54,398:INFO:Finalizing model
2025-05-03 12:10:56,683:INFO:Uploading results into container
2025-05-03 12:10:56,683:INFO:Uploading model into container now
2025-05-03 12:10:56,686:INFO:_master_model_container: 14
2025-05-03 12:10:56,686:INFO:_display_container: 8
2025-05-03 12:10:56,686:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 12:10:56,686:INFO:create_model() successfully completed......................................
2025-05-03 12:10:56,817:INFO:SubProcess create_model() end ==================================
2025-05-03 12:10:56,826:INFO:choose_better activated
2025-05-03 12:10:56,833:INFO:SubProcess create_model() called ==================================
2025-05-03 12:10:56,834:INFO:Initializing create_model()
2025-05-03 12:10:56,834:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:10:56,834:INFO:Checking exceptions
2025-05-03 12:10:56,834:INFO:Importing libraries
2025-05-03 12:10:56,834:INFO:Copying training dataset
2025-05-03 12:10:56,867:INFO:Defining folds
2025-05-03 12:10:56,867:INFO:Declaring metric variables
2025-05-03 12:10:56,867:INFO:Importing untrained model
2025-05-03 12:10:56,867:INFO:Declaring custom model
2025-05-03 12:10:56,867:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:10:56,867:INFO:Starting cross validation
2025-05-03 12:10:56,867:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:10:58,125:INFO:Calculating mean and std
2025-05-03 12:10:58,125:INFO:Creating metrics dataframe
2025-05-03 12:10:58,126:INFO:Finalizing model
2025-05-03 12:10:58,724:INFO:Uploading results into container
2025-05-03 12:10:58,724:INFO:Uploading model into container now
2025-05-03 12:10:58,724:INFO:_master_model_container: 15
2025-05-03 12:10:58,724:INFO:_display_container: 9
2025-05-03 12:10:58,724:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:10:58,724:INFO:create_model() successfully completed......................................
2025-05-03 12:10:58,849:INFO:SubProcess create_model() end ==================================
2025-05-03 12:10:58,849:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 12:10:58,849:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 12:10:58,849:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 12:10:58,849:INFO:choose_better completed
2025-05-03 12:10:58,865:INFO:_master_model_container: 15
2025-05-03 12:10:58,865:INFO:_display_container: 8
2025-05-03 12:10:58,865:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 12:10:58,865:INFO:tune_model() successfully completed......................................
2025-05-03 12:10:59,044:INFO:Initializing create_model()
2025-05-03 12:10:59,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:10:59,044:INFO:Checking exceptions
2025-05-03 12:10:59,048:INFO:Importing libraries
2025-05-03 12:10:59,048:INFO:Copying training dataset
2025-05-03 12:10:59,068:INFO:Defining folds
2025-05-03 12:10:59,068:INFO:Declaring metric variables
2025-05-03 12:10:59,068:INFO:Importing untrained model
2025-05-03 12:10:59,068:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:10:59,068:INFO:Starting cross validation
2025-05-03 12:10:59,068:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:10:59,565:INFO:Calculating mean and std
2025-05-03 12:10:59,565:INFO:Creating metrics dataframe
2025-05-03 12:10:59,567:INFO:Finalizing model
2025-05-03 12:10:59,593:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:10:59,597:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.
2025-05-03 12:10:59,597:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:10:59,597:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:10:59,597:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:10:59,597:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:10:59,599:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:10:59,599:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:10:59,738:INFO:Uploading results into container
2025-05-03 12:10:59,740:INFO:Uploading model into container now
2025-05-03 12:10:59,740:INFO:_master_model_container: 16
2025-05-03 12:10:59,740:INFO:_display_container: 9
2025-05-03 12:10:59,740:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:10:59,740:INFO:create_model() successfully completed......................................
2025-05-03 12:10:59,886:INFO:Initializing create_model()
2025-05-03 12:10:59,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:10:59,886:INFO:Checking exceptions
2025-05-03 12:10:59,888:INFO:Importing libraries
2025-05-03 12:10:59,888:INFO:Copying training dataset
2025-05-03 12:10:59,924:INFO:Defining folds
2025-05-03 12:10:59,924:INFO:Declaring metric variables
2025-05-03 12:10:59,924:INFO:Importing untrained model
2025-05-03 12:10:59,924:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:10:59,924:INFO:Starting cross validation
2025-05-03 12:10:59,924:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:11:00,325:INFO:Calculating mean and std
2025-05-03 12:11:00,325:INFO:Creating metrics dataframe
2025-05-03 12:11:00,325:INFO:Finalizing model
2025-05-03 12:11:00,457:INFO:Uploading results into container
2025-05-03 12:11:00,457:INFO:Uploading model into container now
2025-05-03 12:11:00,457:INFO:_master_model_container: 17
2025-05-03 12:11:00,457:INFO:_display_container: 10
2025-05-03 12:11:00,457:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:11:00,459:INFO:create_model() successfully completed......................................
2025-05-03 12:11:00,601:INFO:Initializing create_model()
2025-05-03 12:11:00,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:11:00,601:INFO:Checking exceptions
2025-05-03 12:11:00,603:INFO:Importing libraries
2025-05-03 12:11:00,603:INFO:Copying training dataset
2025-05-03 12:11:00,630:INFO:Defining folds
2025-05-03 12:11:00,630:INFO:Declaring metric variables
2025-05-03 12:11:00,631:INFO:Importing untrained model
2025-05-03 12:11:00,631:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:11:00,631:INFO:Starting cross validation
2025-05-03 12:11:00,631:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:11:01,922:INFO:Calculating mean and std
2025-05-03 12:11:01,922:INFO:Creating metrics dataframe
2025-05-03 12:11:01,922:INFO:Finalizing model
2025-05-03 12:11:02,503:INFO:Uploading results into container
2025-05-03 12:11:02,503:INFO:Uploading model into container now
2025-05-03 12:11:02,503:INFO:_master_model_container: 18
2025-05-03 12:11:02,503:INFO:_display_container: 11
2025-05-03 12:11:02,503:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:11:02,503:INFO:create_model() successfully completed......................................
2025-05-03 12:11:02,678:INFO:Initializing interpret_model()
2025-05-03 12:11:02,678:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:11:02,678:INFO:Checking exceptions
2025-05-03 12:11:02,678:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 12:11:02,718:INFO:plot type: summary
2025-05-03 12:11:02,718:INFO:Creating TreeExplainer
2025-05-03 12:11:02,828:INFO:Compiling shap values
2025-05-03 12:11:04,489:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn(

2025-05-03 12:11:04,489:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)

2025-05-03 12:11:05,959:INFO:Visual Rendered Successfully
2025-05-03 12:11:05,959:INFO:interpret_model() successfully completed......................................
2025-05-03 12:11:06,132:INFO:Initializing interpret_model()
2025-05-03 12:11:06,132:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:11:06,132:INFO:Checking exceptions
2025-05-03 12:11:06,132:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 12:11:06,155:INFO:plot type: summary
2025-05-03 12:11:06,155:INFO:Creating TreeExplainer
2025-05-03 12:11:06,188:INFO:Compiling shap values
2025-05-03 12:11:08,080:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)

2025-05-03 12:11:09,573:INFO:Visual Rendered Successfully
2025-05-03 12:11:09,573:INFO:interpret_model() successfully completed......................................
2025-05-03 12:11:09,733:INFO:Initializing interpret_model()
2025-05-03 12:11:09,733:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000225FF4E3610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:11:09,733:INFO:Checking exceptions
2025-05-03 12:11:09,733:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 12:11:09,759:INFO:plot type: summary
2025-05-03 12:11:09,759:INFO:Creating TreeExplainer
2025-05-03 12:11:09,843:INFO:Compiling shap values
2025-05-03 12:15:31,458:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:15:31,458:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:15:31,458:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:15:31,458:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:15:35,310:INFO:PyCaret ClassificationExperiment
2025-05-03 12:15:35,310:INFO:Logging name: clf-default-name
2025-05-03 12:15:35,310:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 12:15:35,310:INFO:version 3.3.2
2025-05-03 12:15:35,310:INFO:Initializing setup()
2025-05-03 12:15:35,310:INFO:self.USI: c1ec
2025-05-03 12:15:35,310:INFO:self._variable_keys: {'_ml_usecase', 'pipeline', 'html_param', 'exp_name_log', 'fold_generator', 'target_param', 'X_train', 'gpu_n_jobs_param', 'fix_imbalance', 'n_jobs_param', 'log_plots_param', 'fold_groups_param', 'logging_param', 'X_test', 'X', 'gpu_param', 'seed', 'exp_id', 'is_multiclass', 'USI', 'y', 'idx', 'fold_shuffle_param', 'memory', 'y_test', 'y_train', '_available_plots', 'data'}
2025-05-03 12:15:35,310:INFO:Checking environment
2025-05-03 12:15:35,310:INFO:python_version: 3.11.11
2025-05-03 12:15:35,310:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 12:15:35,310:INFO:machine: AMD64
2025-05-03 12:15:35,310:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 12:15:35,327:INFO:Memory: svmem(total=16965230592, available=4281716736, percent=74.8, used=12683513856, free=4281716736)
2025-05-03 12:15:35,327:INFO:Physical Core: 4
2025-05-03 12:15:35,327:INFO:Logical Core: 8
2025-05-03 12:15:35,327:INFO:Checking libraries
2025-05-03 12:15:35,327:INFO:System:
2025-05-03 12:15:35,327:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 12:15:35,327:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 12:15:35,327:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 12:15:35,327:INFO:PyCaret required dependencies:
2025-05-03 12:15:35,327:INFO:                 pip: 25.0
2025-05-03 12:15:35,327:INFO:          setuptools: 75.8.0
2025-05-03 12:15:35,327:INFO:             pycaret: 3.3.2
2025-05-03 12:15:35,327:INFO:             IPython: 8.32.0
2025-05-03 12:15:35,327:INFO:          ipywidgets: 8.1.6
2025-05-03 12:15:35,327:INFO:                tqdm: 4.67.1
2025-05-03 12:15:35,327:INFO:               numpy: 1.26.4
2025-05-03 12:15:35,327:INFO:              pandas: 2.1.4
2025-05-03 12:15:35,327:INFO:              jinja2: 3.1.6
2025-05-03 12:15:35,327:INFO:               scipy: 1.11.4
2025-05-03 12:15:35,327:INFO:              joblib: 1.3.2
2025-05-03 12:15:35,327:INFO:             sklearn: 1.4.2
2025-05-03 12:15:35,327:INFO:                pyod: 2.0.5
2025-05-03 12:15:35,327:INFO:            imblearn: 0.13.0
2025-05-03 12:15:35,327:INFO:   category_encoders: 2.7.0
2025-05-03 12:15:35,327:INFO:            lightgbm: 4.6.0
2025-05-03 12:15:35,327:INFO:               numba: 0.61.2
2025-05-03 12:15:35,327:INFO:            requests: 2.32.3
2025-05-03 12:15:35,327:INFO:          matplotlib: 3.7.5
2025-05-03 12:15:35,327:INFO:          scikitplot: 0.3.7
2025-05-03 12:15:35,327:INFO:         yellowbrick: 1.5
2025-05-03 12:15:35,327:INFO:              plotly: 5.24.1
2025-05-03 12:15:35,327:INFO:    plotly-resampler: Not installed
2025-05-03 12:15:35,327:INFO:             kaleido: 0.2.1
2025-05-03 12:15:35,327:INFO:           schemdraw: 0.15
2025-05-03 12:15:35,327:INFO:         statsmodels: 0.14.4
2025-05-03 12:15:35,327:INFO:              sktime: 0.26.0
2025-05-03 12:15:35,327:INFO:               tbats: 1.1.3
2025-05-03 12:15:35,327:INFO:            pmdarima: 2.0.4
2025-05-03 12:15:35,327:INFO:              psutil: 6.1.1
2025-05-03 12:15:35,327:INFO:          markupsafe: 3.0.2
2025-05-03 12:15:35,327:INFO:             pickle5: Not installed
2025-05-03 12:15:35,327:INFO:         cloudpickle: 3.1.1
2025-05-03 12:15:35,327:INFO:         deprecation: 2.1.0
2025-05-03 12:15:35,327:INFO:              xxhash: 3.5.0
2025-05-03 12:15:35,327:INFO:           wurlitzer: Not installed
2025-05-03 12:15:35,327:INFO:PyCaret optional dependencies:
2025-05-03 12:15:35,658:INFO:                shap: 0.47.2
2025-05-03 12:15:35,658:INFO:           interpret: Not installed
2025-05-03 12:15:35,658:INFO:                umap: Not installed
2025-05-03 12:15:35,658:INFO:     ydata_profiling: Not installed
2025-05-03 12:15:35,658:INFO:  explainerdashboard: Not installed
2025-05-03 12:15:35,658:INFO:             autoviz: Not installed
2025-05-03 12:15:35,658:INFO:           fairlearn: Not installed
2025-05-03 12:15:35,658:INFO:          deepchecks: Not installed
2025-05-03 12:15:35,658:INFO:             xgboost: 3.0.0
2025-05-03 12:15:35,658:INFO:            catboost: Not installed
2025-05-03 12:15:35,658:INFO:              kmodes: Not installed
2025-05-03 12:15:35,658:INFO:             mlxtend: Not installed
2025-05-03 12:15:35,658:INFO:       statsforecast: Not installed
2025-05-03 12:15:35,658:INFO:        tune_sklearn: Not installed
2025-05-03 12:15:35,658:INFO:                 ray: Not installed
2025-05-03 12:15:35,658:INFO:            hyperopt: 0.2.7
2025-05-03 12:15:35,658:INFO:              optuna: Not installed
2025-05-03 12:15:35,658:INFO:               skopt: 0.10.2
2025-05-03 12:15:35,658:INFO:              mlflow: 2.22.0
2025-05-03 12:15:35,658:INFO:              gradio: Not installed
2025-05-03 12:15:35,658:INFO:             fastapi: 0.115.12
2025-05-03 12:15:35,658:INFO:             uvicorn: 0.34.2
2025-05-03 12:15:35,658:INFO:              m2cgen: Not installed
2025-05-03 12:15:35,658:INFO:           evidently: Not installed
2025-05-03 12:15:35,658:INFO:               fugue: Not installed
2025-05-03 12:15:35,658:INFO:           streamlit: Not installed
2025-05-03 12:15:35,658:INFO:             prophet: Not installed
2025-05-03 12:15:35,658:INFO:None
2025-05-03 12:15:35,658:INFO:Set up data.
2025-05-03 12:15:35,677:INFO:Set up folding strategy.
2025-05-03 12:15:35,677:INFO:Set up train/test split.
2025-05-03 12:15:35,690:INFO:Set up index.
2025-05-03 12:15:35,700:INFO:Assigning column types.
2025-05-03 12:15:35,716:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 12:15:35,764:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:15:35,771:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:15:35,804:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:35,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:35,854:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:15:35,854:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:15:35,886:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:35,887:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:35,890:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 12:15:35,938:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:15:35,970:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:35,970:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,018:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:15:36,056:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:36,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,060:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 12:15:36,134:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:36,137:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,220:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:36,220:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,237:INFO:Finished creating preprocessing pipeline.
2025-05-03 12:15:36,237:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 12:15:36,237:INFO:Creating final display dataframe.
2025-05-03 12:15:36,331:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 12:15:36,403:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:36,403:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,492:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:15:36,496:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:15:36,496:INFO:setup() successfully completed in 1.19s...............
2025-05-03 12:15:36,512:INFO:Initializing compare_models()
2025-05-03 12:15:36,512:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 12:15:36,512:INFO:Checking exceptions
2025-05-03 12:15:36,529:INFO:Preparing display monitor
2025-05-03 12:15:36,686:INFO:Initializing Logistic Regression
2025-05-03 12:15:36,686:INFO:Total runtime is 0.0 minutes
2025-05-03 12:15:36,686:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:36,686:INFO:Initializing create_model()
2025-05-03 12:15:36,686:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:36,686:INFO:Checking exceptions
2025-05-03 12:15:36,686:INFO:Importing libraries
2025-05-03 12:15:36,686:INFO:Copying training dataset
2025-05-03 12:15:36,703:INFO:Defining folds
2025-05-03 12:15:36,703:INFO:Declaring metric variables
2025-05-03 12:15:36,703:INFO:Importing untrained model
2025-05-03 12:15:36,722:INFO:Logistic Regression Imported successfully
2025-05-03 12:15:36,741:INFO:Starting cross validation
2025-05-03 12:15:36,743:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:40,461:INFO:Calculating mean and std
2025-05-03 12:15:40,463:INFO:Creating metrics dataframe
2025-05-03 12:15:40,464:INFO:Uploading results into container
2025-05-03 12:15:40,467:INFO:Uploading model into container now
2025-05-03 12:15:40,467:INFO:_master_model_container: 1
2025-05-03 12:15:40,467:INFO:_display_container: 2
2025-05-03 12:15:40,467:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 12:15:40,467:INFO:create_model() successfully completed......................................
2025-05-03 12:15:40,600:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:40,600:INFO:Creating metrics dataframe
2025-05-03 12:15:40,600:INFO:Initializing Random Forest Classifier
2025-05-03 12:15:40,600:INFO:Total runtime is 0.06523540019989013 minutes
2025-05-03 12:15:40,600:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:40,600:INFO:Initializing create_model()
2025-05-03 12:15:40,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:40,600:INFO:Checking exceptions
2025-05-03 12:15:40,600:INFO:Importing libraries
2025-05-03 12:15:40,600:INFO:Copying training dataset
2025-05-03 12:15:40,632:INFO:Defining folds
2025-05-03 12:15:40,632:INFO:Declaring metric variables
2025-05-03 12:15:40,633:INFO:Importing untrained model
2025-05-03 12:15:40,633:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:15:40,649:INFO:Starting cross validation
2025-05-03 12:15:40,650:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:45,061:INFO:Calculating mean and std
2025-05-03 12:15:45,061:INFO:Creating metrics dataframe
2025-05-03 12:15:45,065:INFO:Uploading results into container
2025-05-03 12:15:45,065:INFO:Uploading model into container now
2025-05-03 12:15:45,065:INFO:_master_model_container: 2
2025-05-03 12:15:45,065:INFO:_display_container: 2
2025-05-03 12:15:45,065:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:15:45,065:INFO:create_model() successfully completed......................................
2025-05-03 12:15:45,192:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:45,195:INFO:Creating metrics dataframe
2025-05-03 12:15:45,206:INFO:Initializing Extreme Gradient Boosting
2025-05-03 12:15:45,206:INFO:Total runtime is 0.1420043627421061 minutes
2025-05-03 12:15:45,213:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:45,213:INFO:Initializing create_model()
2025-05-03 12:15:45,213:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:45,213:INFO:Checking exceptions
2025-05-03 12:15:45,213:INFO:Importing libraries
2025-05-03 12:15:45,213:INFO:Copying training dataset
2025-05-03 12:15:45,230:INFO:Defining folds
2025-05-03 12:15:45,230:INFO:Declaring metric variables
2025-05-03 12:15:45,238:INFO:Importing untrained model
2025-05-03 12:15:45,246:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:15:45,254:INFO:Starting cross validation
2025-05-03 12:15:45,254:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:51,036:INFO:Calculating mean and std
2025-05-03 12:15:51,040:INFO:Creating metrics dataframe
2025-05-03 12:15:51,042:INFO:Uploading results into container
2025-05-03 12:15:51,042:INFO:Uploading model into container now
2025-05-03 12:15:51,051:INFO:_master_model_container: 3
2025-05-03 12:15:51,051:INFO:_display_container: 2
2025-05-03 12:15:51,055:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:15:51,057:INFO:create_model() successfully completed......................................
2025-05-03 12:15:51,292:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:51,292:INFO:Creating metrics dataframe
2025-05-03 12:15:51,308:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 12:15:51,308:INFO:Total runtime is 0.2437042474746704 minutes
2025-05-03 12:15:51,325:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:51,325:INFO:Initializing create_model()
2025-05-03 12:15:51,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:51,325:INFO:Checking exceptions
2025-05-03 12:15:51,325:INFO:Importing libraries
2025-05-03 12:15:51,325:INFO:Copying training dataset
2025-05-03 12:15:51,393:INFO:Defining folds
2025-05-03 12:15:51,394:INFO:Declaring metric variables
2025-05-03 12:15:51,402:INFO:Importing untrained model
2025-05-03 12:15:51,410:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:15:51,440:INFO:Starting cross validation
2025-05-03 12:15:51,441:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:53,210:INFO:Calculating mean and std
2025-05-03 12:15:53,210:INFO:Creating metrics dataframe
2025-05-03 12:15:53,220:INFO:Uploading results into container
2025-05-03 12:15:53,222:INFO:Uploading model into container now
2025-05-03 12:15:53,224:INFO:_master_model_container: 4
2025-05-03 12:15:53,224:INFO:_display_container: 2
2025-05-03 12:15:53,224:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:15:53,224:INFO:create_model() successfully completed......................................
2025-05-03 12:15:53,540:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:53,540:INFO:Creating metrics dataframe
2025-05-03 12:15:53,572:INFO:Initializing Extra Trees Classifier
2025-05-03 12:15:53,572:INFO:Total runtime is 0.28143757581710815 minutes
2025-05-03 12:15:53,580:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:53,580:INFO:Initializing create_model()
2025-05-03 12:15:53,580:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:53,588:INFO:Checking exceptions
2025-05-03 12:15:53,588:INFO:Importing libraries
2025-05-03 12:15:53,588:INFO:Copying training dataset
2025-05-03 12:15:53,640:INFO:Defining folds
2025-05-03 12:15:53,640:INFO:Declaring metric variables
2025-05-03 12:15:53,657:INFO:Importing untrained model
2025-05-03 12:15:53,668:INFO:Extra Trees Classifier Imported successfully
2025-05-03 12:15:53,690:INFO:Starting cross validation
2025-05-03 12:15:53,690:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:57,300:INFO:Calculating mean and std
2025-05-03 12:15:57,306:INFO:Creating metrics dataframe
2025-05-03 12:15:57,312:INFO:Uploading results into container
2025-05-03 12:15:57,316:INFO:Uploading model into container now
2025-05-03 12:15:57,316:INFO:_master_model_container: 5
2025-05-03 12:15:57,319:INFO:_display_container: 2
2025-05-03 12:15:57,320:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 12:15:57,320:INFO:create_model() successfully completed......................................
2025-05-03 12:15:57,570:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:57,570:INFO:Creating metrics dataframe
2025-05-03 12:15:57,595:INFO:Initializing Ridge Classifier
2025-05-03 12:15:57,595:INFO:Total runtime is 0.3484733064969381 minutes
2025-05-03 12:15:57,612:INFO:SubProcess create_model() called ==================================
2025-05-03 12:15:57,612:INFO:Initializing create_model()
2025-05-03 12:15:57,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E716096FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:57,612:INFO:Checking exceptions
2025-05-03 12:15:57,612:INFO:Importing libraries
2025-05-03 12:15:57,612:INFO:Copying training dataset
2025-05-03 12:15:57,670:INFO:Defining folds
2025-05-03 12:15:57,670:INFO:Declaring metric variables
2025-05-03 12:15:57,687:INFO:Importing untrained model
2025-05-03 12:15:57,702:INFO:Ridge Classifier Imported successfully
2025-05-03 12:15:57,720:INFO:Starting cross validation
2025-05-03 12:15:57,731:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:15:57,968:INFO:Calculating mean and std
2025-05-03 12:15:57,970:INFO:Creating metrics dataframe
2025-05-03 12:15:57,970:INFO:Uploading results into container
2025-05-03 12:15:57,970:INFO:Uploading model into container now
2025-05-03 12:15:57,970:INFO:_master_model_container: 6
2025-05-03 12:15:57,970:INFO:_display_container: 2
2025-05-03 12:15:57,985:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 12:15:57,985:INFO:create_model() successfully completed......................................
2025-05-03 12:15:58,219:INFO:SubProcess create_model() end ==================================
2025-05-03 12:15:58,219:INFO:Creating metrics dataframe
2025-05-03 12:15:58,268:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 12:15:58,303:INFO:Initializing create_model()
2025-05-03 12:15:58,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:58,303:INFO:Checking exceptions
2025-05-03 12:15:58,303:INFO:Importing libraries
2025-05-03 12:15:58,303:INFO:Copying training dataset
2025-05-03 12:15:58,381:INFO:Defining folds
2025-05-03 12:15:58,381:INFO:Declaring metric variables
2025-05-03 12:15:58,381:INFO:Importing untrained model
2025-05-03 12:15:58,381:INFO:Declaring custom model
2025-05-03 12:15:58,386:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:15:58,386:INFO:Cross validation set to False
2025-05-03 12:15:58,386:INFO:Fitting Model
2025-05-03 12:15:58,460:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:15:58,471:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003393 seconds.
2025-05-03 12:15:58,471:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:15:58,471:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:15:58,473:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:15:58,473:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:15:58,473:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:15:58,475:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:15:59,050:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:15:59,051:INFO:create_model() successfully completed......................................
2025-05-03 12:15:59,368:INFO:_master_model_container: 6
2025-05-03 12:15:59,368:INFO:_display_container: 2
2025-05-03 12:15:59,368:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:15:59,373:INFO:compare_models() successfully completed......................................
2025-05-03 12:15:59,423:INFO:Initializing create_model()
2025-05-03 12:15:59,423:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:15:59,423:INFO:Checking exceptions
2025-05-03 12:15:59,479:INFO:Importing libraries
2025-05-03 12:15:59,479:INFO:Copying training dataset
2025-05-03 12:15:59,553:INFO:Defining folds
2025-05-03 12:15:59,553:INFO:Declaring metric variables
2025-05-03 12:15:59,560:INFO:Importing untrained model
2025-05-03 12:15:59,576:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:15:59,604:INFO:Starting cross validation
2025-05-03 12:15:59,606:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:16:01,371:INFO:Calculating mean and std
2025-05-03 12:16:01,371:INFO:Creating metrics dataframe
2025-05-03 12:16:01,417:INFO:Finalizing model
2025-05-03 12:16:01,560:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:16:01,574:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003774 seconds.
2025-05-03 12:16:01,574:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:16:01,574:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:16:01,576:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:16:01,576:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:16:01,576:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:16:01,578:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:16:02,082:INFO:Uploading results into container
2025-05-03 12:16:02,084:INFO:Uploading model into container now
2025-05-03 12:16:02,114:INFO:_master_model_container: 7
2025-05-03 12:16:02,116:INFO:_display_container: 3
2025-05-03 12:16:02,119:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:16:02,119:INFO:create_model() successfully completed......................................
2025-05-03 12:16:02,391:INFO:Initializing create_model()
2025-05-03 12:16:02,391:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:16:02,398:INFO:Checking exceptions
2025-05-03 12:16:02,448:INFO:Importing libraries
2025-05-03 12:16:02,452:INFO:Copying training dataset
2025-05-03 12:16:02,520:INFO:Defining folds
2025-05-03 12:16:02,520:INFO:Declaring metric variables
2025-05-03 12:16:02,529:INFO:Importing untrained model
2025-05-03 12:16:02,549:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:16:02,569:INFO:Starting cross validation
2025-05-03 12:16:02,569:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:16:03,630:INFO:Calculating mean and std
2025-05-03 12:16:03,630:INFO:Creating metrics dataframe
2025-05-03 12:16:03,640:INFO:Finalizing model
2025-05-03 12:16:04,084:INFO:Uploading results into container
2025-05-03 12:16:04,087:INFO:Uploading model into container now
2025-05-03 12:16:04,117:INFO:_master_model_container: 8
2025-05-03 12:16:04,119:INFO:_display_container: 4
2025-05-03 12:16:04,122:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:16:04,122:INFO:create_model() successfully completed......................................
2025-05-03 12:16:04,395:INFO:Initializing create_model()
2025-05-03 12:16:04,395:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:16:04,397:INFO:Checking exceptions
2025-05-03 12:16:04,451:INFO:Importing libraries
2025-05-03 12:16:04,451:INFO:Copying training dataset
2025-05-03 12:16:04,525:INFO:Defining folds
2025-05-03 12:16:04,525:INFO:Declaring metric variables
2025-05-03 12:16:04,541:INFO:Importing untrained model
2025-05-03 12:16:04,557:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:16:04,583:INFO:Starting cross validation
2025-05-03 12:16:04,583:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:16:08,408:INFO:Calculating mean and std
2025-05-03 12:16:08,410:INFO:Creating metrics dataframe
2025-05-03 12:16:08,428:INFO:Finalizing model
2025-05-03 12:16:10,124:INFO:Uploading results into container
2025-05-03 12:16:10,124:INFO:Uploading model into container now
2025-05-03 12:16:10,160:INFO:_master_model_container: 9
2025-05-03 12:16:10,160:INFO:_display_container: 5
2025-05-03 12:16:10,160:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:16:10,160:INFO:create_model() successfully completed......................................
2025-05-03 12:16:10,444:INFO:Initializing tune_model()
2025-05-03 12:16:10,444:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:16:10,444:INFO:Checking exceptions
2025-05-03 12:16:10,444:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:16:10,520:INFO:Copying training dataset
2025-05-03 12:16:10,563:INFO:Checking base model
2025-05-03 12:16:10,563:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 12:16:10,572:INFO:Declaring metric variables
2025-05-03 12:16:10,587:INFO:Defining Hyperparameters
2025-05-03 12:16:10,825:INFO:Tuning with n_jobs=-1
2025-05-03 12:16:10,839:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:20:22,118:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 12:20:22,118:INFO:Hyperparameter search completed
2025-05-03 12:20:22,118:INFO:SubProcess create_model() called ==================================
2025-05-03 12:20:22,118:INFO:Initializing create_model()
2025-05-03 12:20:22,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E714DAA090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 12:20:22,118:INFO:Checking exceptions
2025-05-03 12:20:22,118:INFO:Importing libraries
2025-05-03 12:20:22,118:INFO:Copying training dataset
2025-05-03 12:20:22,135:INFO:Defining folds
2025-05-03 12:20:22,135:INFO:Declaring metric variables
2025-05-03 12:20:22,145:INFO:Importing untrained model
2025-05-03 12:20:22,145:INFO:Declaring custom model
2025-05-03 12:20:22,151:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:20:22,161:INFO:Starting cross validation
2025-05-03 12:20:22,161:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:20:22,571:INFO:Calculating mean and std
2025-05-03 12:20:22,573:INFO:Creating metrics dataframe
2025-05-03 12:20:22,579:INFO:Finalizing model
2025-05-03 12:20:22,584:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:20:22,586:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:20:22,586:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:20:22,601:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:20:22,601:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:20:22,601:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:20:22,601:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:20:22,605:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000848 seconds.
2025-05-03 12:20:22,605:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:20:22,605:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:20:22,605:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:20:22,605:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:20:22,605:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:20:22,605:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:20:22,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,692:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,700:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,705:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,707:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,733:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,743:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:20:22,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:20:22,769:INFO:Uploading results into container
2025-05-03 12:20:22,769:INFO:Uploading model into container now
2025-05-03 12:20:22,771:INFO:_master_model_container: 10
2025-05-03 12:20:22,771:INFO:_display_container: 6
2025-05-03 12:20:22,773:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:20:22,773:INFO:create_model() successfully completed......................................
2025-05-03 12:20:22,921:INFO:SubProcess create_model() end ==================================
2025-05-03 12:20:22,921:INFO:choose_better activated
2025-05-03 12:20:22,925:INFO:SubProcess create_model() called ==================================
2025-05-03 12:20:22,928:INFO:Initializing create_model()
2025-05-03 12:20:22,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:20:22,928:INFO:Checking exceptions
2025-05-03 12:20:22,929:INFO:Importing libraries
2025-05-03 12:20:22,931:INFO:Copying training dataset
2025-05-03 12:20:22,951:INFO:Defining folds
2025-05-03 12:20:22,951:INFO:Declaring metric variables
2025-05-03 12:20:22,951:INFO:Importing untrained model
2025-05-03 12:20:22,951:INFO:Declaring custom model
2025-05-03 12:20:22,963:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:20:22,963:INFO:Starting cross validation
2025-05-03 12:20:22,963:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:20:23,479:INFO:Calculating mean and std
2025-05-03 12:20:23,479:INFO:Creating metrics dataframe
2025-05-03 12:20:23,481:INFO:Finalizing model
2025-05-03 12:20:23,503:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:20:23,507:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.
2025-05-03 12:20:23,507:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:20:23,507:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:20:23,507:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:20:23,507:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:20:23,507:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:20:23,507:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:20:23,630:INFO:Uploading results into container
2025-05-03 12:20:23,630:INFO:Uploading model into container now
2025-05-03 12:20:23,630:INFO:_master_model_container: 11
2025-05-03 12:20:23,630:INFO:_display_container: 7
2025-05-03 12:20:23,633:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:20:23,633:INFO:create_model() successfully completed......................................
2025-05-03 12:20:23,767:INFO:SubProcess create_model() end ==================================
2025-05-03 12:20:23,778:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 12:20:23,778:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 12:20:23,778:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 12:20:23,778:INFO:choose_better completed
2025-05-03 12:20:23,783:INFO:_master_model_container: 11
2025-05-03 12:20:23,783:INFO:_display_container: 6
2025-05-03 12:20:23,783:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:20:23,783:INFO:tune_model() successfully completed......................................
2025-05-03 12:20:23,957:INFO:Initializing tune_model()
2025-05-03 12:20:23,957:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:20:23,957:INFO:Checking exceptions
2025-05-03 12:20:23,957:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:20:23,989:INFO:Copying training dataset
2025-05-03 12:20:24,003:INFO:Checking base model
2025-05-03 12:20:24,003:INFO:Base model : Extreme Gradient Boosting
2025-05-03 12:20:24,005:INFO:Declaring metric variables
2025-05-03 12:20:24,013:INFO:Defining Hyperparameters
2025-05-03 12:20:24,133:INFO:Tuning with n_jobs=-1
2025-05-03 12:20:24,146:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:22:16,829:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 12:22:16,829:INFO:Hyperparameter search completed
2025-05-03 12:22:16,829:INFO:SubProcess create_model() called ==================================
2025-05-03 12:22:16,829:INFO:Initializing create_model()
2025-05-03 12:22:16,829:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E714BD0F50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 12:22:16,829:INFO:Checking exceptions
2025-05-03 12:22:16,836:INFO:Importing libraries
2025-05-03 12:22:16,836:INFO:Copying training dataset
2025-05-03 12:22:16,852:INFO:Defining folds
2025-05-03 12:22:16,852:INFO:Declaring metric variables
2025-05-03 12:22:16,860:INFO:Importing untrained model
2025-05-03 12:22:16,860:INFO:Declaring custom model
2025-05-03 12:22:16,863:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:22:16,874:INFO:Starting cross validation
2025-05-03 12:22:16,874:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:22:17,382:INFO:Calculating mean and std
2025-05-03 12:22:17,382:INFO:Creating metrics dataframe
2025-05-03 12:22:17,392:INFO:Finalizing model
2025-05-03 12:22:17,568:INFO:Uploading results into container
2025-05-03 12:22:17,568:INFO:Uploading model into container now
2025-05-03 12:22:17,570:INFO:_master_model_container: 12
2025-05-03 12:22:17,570:INFO:_display_container: 7
2025-05-03 12:22:17,570:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 12:22:17,570:INFO:create_model() successfully completed......................................
2025-05-03 12:22:17,715:INFO:SubProcess create_model() end ==================================
2025-05-03 12:22:17,715:INFO:choose_better activated
2025-05-03 12:22:17,723:INFO:SubProcess create_model() called ==================================
2025-05-03 12:22:17,723:INFO:Initializing create_model()
2025-05-03 12:22:17,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:22:17,723:INFO:Checking exceptions
2025-05-03 12:22:17,730:INFO:Importing libraries
2025-05-03 12:22:17,730:INFO:Copying training dataset
2025-05-03 12:22:17,763:INFO:Defining folds
2025-05-03 12:22:17,763:INFO:Declaring metric variables
2025-05-03 12:22:17,763:INFO:Importing untrained model
2025-05-03 12:22:17,763:INFO:Declaring custom model
2025-05-03 12:22:17,763:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:22:17,763:INFO:Starting cross validation
2025-05-03 12:22:17,763:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:22:18,118:INFO:Calculating mean and std
2025-05-03 12:22:18,118:INFO:Creating metrics dataframe
2025-05-03 12:22:18,118:INFO:Finalizing model
2025-05-03 12:22:18,235:INFO:Uploading results into container
2025-05-03 12:22:18,235:INFO:Uploading model into container now
2025-05-03 12:22:18,237:INFO:_master_model_container: 13
2025-05-03 12:22:18,237:INFO:_display_container: 8
2025-05-03 12:22:18,238:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:22:18,238:INFO:create_model() successfully completed......................................
2025-05-03 12:22:18,378:INFO:SubProcess create_model() end ==================================
2025-05-03 12:22:18,378:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 12:22:18,378:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 12:22:18,378:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 12:22:18,378:INFO:choose_better completed
2025-05-03 12:22:18,391:INFO:_master_model_container: 13
2025-05-03 12:22:18,391:INFO:_display_container: 7
2025-05-03 12:22:18,391:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 12:22:18,391:INFO:tune_model() successfully completed......................................
2025-05-03 12:22:18,552:INFO:Initializing tune_model()
2025-05-03 12:22:18,552:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:22:18,552:INFO:Checking exceptions
2025-05-03 12:22:18,552:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:22:18,579:INFO:Copying training dataset
2025-05-03 12:22:18,597:INFO:Checking base model
2025-05-03 12:22:18,597:INFO:Base model : Random Forest Classifier
2025-05-03 12:22:18,603:INFO:Declaring metric variables
2025-05-03 12:22:18,604:INFO:Defining Hyperparameters
2025-05-03 12:22:18,732:INFO:Tuning with n_jobs=-1
2025-05-03 12:22:18,736:INFO:Initializing skopt.BayesSearchCV
2025-05-03 12:26:08,550:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 12:26:08,550:INFO:Hyperparameter search completed
2025-05-03 12:26:08,550:INFO:SubProcess create_model() called ==================================
2025-05-03 12:26:08,550:INFO:Initializing create_model()
2025-05-03 12:26:08,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E7690E2F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 12:26:08,550:INFO:Checking exceptions
2025-05-03 12:26:08,550:INFO:Importing libraries
2025-05-03 12:26:08,550:INFO:Copying training dataset
2025-05-03 12:26:08,579:INFO:Defining folds
2025-05-03 12:26:08,579:INFO:Declaring metric variables
2025-05-03 12:26:08,584:INFO:Importing untrained model
2025-05-03 12:26:08,584:INFO:Declaring custom model
2025-05-03 12:26:08,586:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:26:08,595:INFO:Starting cross validation
2025-05-03 12:26:08,600:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:26:13,010:INFO:Calculating mean and std
2025-05-03 12:26:13,010:INFO:Creating metrics dataframe
2025-05-03 12:26:13,018:INFO:Finalizing model
2025-05-03 12:26:15,340:INFO:Uploading results into container
2025-05-03 12:26:15,343:INFO:Uploading model into container now
2025-05-03 12:26:15,343:INFO:_master_model_container: 14
2025-05-03 12:26:15,343:INFO:_display_container: 8
2025-05-03 12:26:15,345:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 12:26:15,345:INFO:create_model() successfully completed......................................
2025-05-03 12:26:15,474:INFO:SubProcess create_model() end ==================================
2025-05-03 12:26:15,474:INFO:choose_better activated
2025-05-03 12:26:15,474:INFO:SubProcess create_model() called ==================================
2025-05-03 12:26:15,488:INFO:Initializing create_model()
2025-05-03 12:26:15,488:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:26:15,488:INFO:Checking exceptions
2025-05-03 12:26:15,492:INFO:Importing libraries
2025-05-03 12:26:15,492:INFO:Copying training dataset
2025-05-03 12:26:15,535:INFO:Defining folds
2025-05-03 12:26:15,535:INFO:Declaring metric variables
2025-05-03 12:26:15,535:INFO:Importing untrained model
2025-05-03 12:26:15,535:INFO:Declaring custom model
2025-05-03 12:26:15,535:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:26:15,535:INFO:Starting cross validation
2025-05-03 12:26:15,535:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:26:16,975:INFO:Calculating mean and std
2025-05-03 12:26:16,975:INFO:Creating metrics dataframe
2025-05-03 12:26:16,977:INFO:Finalizing model
2025-05-03 12:26:17,591:INFO:Uploading results into container
2025-05-03 12:26:17,591:INFO:Uploading model into container now
2025-05-03 12:26:17,591:INFO:_master_model_container: 15
2025-05-03 12:26:17,591:INFO:_display_container: 9
2025-05-03 12:26:17,591:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:26:17,591:INFO:create_model() successfully completed......................................
2025-05-03 12:26:17,741:INFO:SubProcess create_model() end ==================================
2025-05-03 12:26:17,741:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 12:26:17,741:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 12:26:17,741:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 12:26:17,741:INFO:choose_better completed
2025-05-03 12:26:17,761:INFO:_master_model_container: 15
2025-05-03 12:26:17,761:INFO:_display_container: 8
2025-05-03 12:26:17,767:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 12:26:17,767:INFO:tune_model() successfully completed......................................
2025-05-03 12:26:17,946:INFO:Initializing create_model()
2025-05-03 12:26:17,946:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:26:17,946:INFO:Checking exceptions
2025-05-03 12:26:17,946:INFO:Importing libraries
2025-05-03 12:26:17,946:INFO:Copying training dataset
2025-05-03 12:26:17,959:INFO:Defining folds
2025-05-03 12:26:17,959:INFO:Declaring metric variables
2025-05-03 12:26:17,959:INFO:Importing untrained model
2025-05-03 12:26:17,959:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:26:17,959:INFO:Starting cross validation
2025-05-03 12:26:17,959:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:26:18,492:INFO:Calculating mean and std
2025-05-03 12:26:18,492:INFO:Creating metrics dataframe
2025-05-03 12:26:18,494:INFO:Finalizing model
2025-05-03 12:26:18,518:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:26:18,520:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000967 seconds.
2025-05-03 12:26:18,520:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:26:18,520:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:26:18,520:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:26:18,520:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:26:18,520:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:26:18,520:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:26:18,651:INFO:Uploading results into container
2025-05-03 12:26:18,651:INFO:Uploading model into container now
2025-05-03 12:26:18,651:INFO:_master_model_container: 16
2025-05-03 12:26:18,651:INFO:_display_container: 9
2025-05-03 12:26:18,653:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:26:18,653:INFO:create_model() successfully completed......................................
2025-05-03 12:26:18,791:INFO:Initializing create_model()
2025-05-03 12:26:18,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:26:18,791:INFO:Checking exceptions
2025-05-03 12:26:18,791:INFO:Importing libraries
2025-05-03 12:26:18,791:INFO:Copying training dataset
2025-05-03 12:26:18,808:INFO:Defining folds
2025-05-03 12:26:18,808:INFO:Declaring metric variables
2025-05-03 12:26:18,808:INFO:Importing untrained model
2025-05-03 12:26:18,808:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:26:18,808:INFO:Starting cross validation
2025-05-03 12:26:18,820:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:26:19,147:INFO:Calculating mean and std
2025-05-03 12:26:19,147:INFO:Creating metrics dataframe
2025-05-03 12:26:19,149:INFO:Finalizing model
2025-05-03 12:26:19,267:INFO:Uploading results into container
2025-05-03 12:26:19,267:INFO:Uploading model into container now
2025-05-03 12:26:19,267:INFO:_master_model_container: 17
2025-05-03 12:26:19,267:INFO:_display_container: 10
2025-05-03 12:26:19,269:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:26:19,269:INFO:create_model() successfully completed......................................
2025-05-03 12:26:19,407:INFO:Initializing create_model()
2025-05-03 12:26:19,407:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:26:19,407:INFO:Checking exceptions
2025-05-03 12:26:19,407:INFO:Importing libraries
2025-05-03 12:26:19,407:INFO:Copying training dataset
2025-05-03 12:26:19,451:INFO:Defining folds
2025-05-03 12:26:19,451:INFO:Declaring metric variables
2025-05-03 12:26:19,451:INFO:Importing untrained model
2025-05-03 12:26:19,451:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:26:19,451:INFO:Starting cross validation
2025-05-03 12:26:19,457:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:26:20,720:INFO:Calculating mean and std
2025-05-03 12:26:20,720:INFO:Creating metrics dataframe
2025-05-03 12:26:20,721:INFO:Finalizing model
2025-05-03 12:26:21,298:INFO:Uploading results into container
2025-05-03 12:26:21,298:INFO:Uploading model into container now
2025-05-03 12:26:21,298:INFO:_master_model_container: 18
2025-05-03 12:26:21,298:INFO:_display_container: 11
2025-05-03 12:26:21,298:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:26:21,298:INFO:create_model() successfully completed......................................
2025-05-03 12:26:21,465:INFO:Initializing interpret_model()
2025-05-03 12:26:21,465:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:26:21,465:INFO:Checking exceptions
2025-05-03 12:30:07,307:INFO:Initializing create_model()
2025-05-03 12:30:07,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 12:30:07,307:INFO:Checking exceptions
2025-05-03 12:30:07,327:INFO:Importing libraries
2025-05-03 12:30:07,327:INFO:Copying training dataset
2025-05-03 12:30:07,354:INFO:Defining folds
2025-05-03 12:30:07,354:INFO:Declaring metric variables
2025-05-03 12:30:07,362:INFO:Importing untrained model
2025-05-03 12:30:07,362:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:30:07,370:INFO:Starting cross validation
2025-05-03 12:30:07,378:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:30:07,793:INFO:Calculating mean and std
2025-05-03 12:30:07,793:INFO:Creating metrics dataframe
2025-05-03 12:30:07,799:INFO:Finalizing model
2025-05-03 12:30:07,807:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:30:07,807:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:30:07,807:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:30:07,826:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 12:30:07,826:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 12:30:07,826:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 12:30:07,826:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:30:07,830:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000748 seconds.
2025-05-03 12:30:07,830:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:30:07,830:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:30:07,830:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:30:07,830:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:30:07,832:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:30:07,832:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:30:07,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,946:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,962:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,966:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,968:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,970:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,980:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,990:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,994:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,996:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:07,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,003:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,003:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 12:30:08,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 12:30:08,022:INFO:Uploading results into container
2025-05-03 12:30:08,026:INFO:Uploading model into container now
2025-05-03 12:30:08,035:INFO:_master_model_container: 19
2025-05-03 12:30:08,035:INFO:_display_container: 12
2025-05-03 12:30:08,039:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:30:08,039:INFO:create_model() successfully completed......................................
2025-05-03 12:30:08,250:INFO:Initializing create_model()
2025-05-03 12:30:08,250:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 12:30:08,250:INFO:Checking exceptions
2025-05-03 12:30:08,273:INFO:Importing libraries
2025-05-03 12:30:08,273:INFO:Copying training dataset
2025-05-03 12:30:08,290:INFO:Defining folds
2025-05-03 12:30:08,290:INFO:Declaring metric variables
2025-05-03 12:30:08,297:INFO:Importing untrained model
2025-05-03 12:30:08,297:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:30:08,315:INFO:Starting cross validation
2025-05-03 12:30:08,315:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:30:08,789:INFO:Calculating mean and std
2025-05-03 12:30:08,789:INFO:Creating metrics dataframe
2025-05-03 12:30:08,789:INFO:Finalizing model
2025-05-03 12:30:08,983:INFO:Uploading results into container
2025-05-03 12:30:08,985:INFO:Uploading model into container now
2025-05-03 12:30:08,994:INFO:_master_model_container: 20
2025-05-03 12:30:08,994:INFO:_display_container: 13
2025-05-03 12:30:08,997:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 12:30:08,997:INFO:create_model() successfully completed......................................
2025-05-03 12:30:09,178:INFO:Initializing create_model()
2025-05-03 12:30:09,178:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 12:30:09,178:INFO:Checking exceptions
2025-05-03 12:30:09,203:INFO:Importing libraries
2025-05-03 12:30:09,203:INFO:Copying training dataset
2025-05-03 12:30:09,226:INFO:Defining folds
2025-05-03 12:30:09,226:INFO:Declaring metric variables
2025-05-03 12:30:09,235:INFO:Importing untrained model
2025-05-03 12:30:09,235:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:30:09,243:INFO:Starting cross validation
2025-05-03 12:30:09,243:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:30:13,429:INFO:Calculating mean and std
2025-05-03 12:30:13,429:INFO:Creating metrics dataframe
2025-05-03 12:30:13,435:INFO:Finalizing model
2025-05-03 12:30:19,930:INFO:Uploading results into container
2025-05-03 12:30:19,930:INFO:Uploading model into container now
2025-05-03 12:30:19,964:INFO:_master_model_container: 21
2025-05-03 12:30:19,964:INFO:_display_container: 14
2025-05-03 12:30:19,964:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 12:30:19,964:INFO:create_model() successfully completed......................................
2025-05-03 12:30:41,051:INFO:Initializing interpret_model()
2025-05-03 12:30:41,051:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:30:41,056:INFO:Checking exceptions
2025-05-03 12:31:33,827:INFO:Initializing interpret_model()
2025-05-03 12:31:33,827:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:31:33,827:INFO:Checking exceptions
2025-05-03 12:31:33,827:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 12:31:33,845:INFO:plot type: summary
2025-05-03 12:31:33,845:INFO:Creating TreeExplainer
2025-05-03 12:31:33,881:INFO:Compiling shap values
2025-05-03 12:31:34,626:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn(

2025-05-03 12:31:34,626:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)

2025-05-03 12:31:36,080:INFO:Visual Rendered Successfully
2025-05-03 12:31:36,080:INFO:interpret_model() successfully completed......................................
2025-05-03 12:32:17,605:INFO:Initializing interpret_model()
2025-05-03 12:32:17,605:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:32:17,608:INFO:Checking exceptions
2025-05-03 12:32:17,608:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 12:32:17,624:INFO:plot type: summary
2025-05-03 12:32:17,626:INFO:Creating TreeExplainer
2025-05-03 12:32:17,664:INFO:Compiling shap values
2025-05-03 12:32:18,371:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn(

2025-05-03 12:32:18,373:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.
  shap_plot = shap.summary_plot(shap_values, test_X, show=show, **kwargs)

2025-05-03 12:32:19,750:INFO:Visual Rendered Successfully
2025-05-03 12:32:19,750:INFO:interpret_model() successfully completed......................................
2025-05-03 12:33:31,727:INFO:Initializing interpret_model()
2025-05-03 12:33:31,727:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:33:31,727:INFO:Checking exceptions
2025-05-03 12:41:16,246:INFO:Initializing interpret_model()
2025-05-03 12:41:16,246:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:41:16,246:INFO:Checking exceptions
2025-05-03 12:41:16,246:ERROR:
'interpret' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2025-05-03 12:49:42,126:INFO:Initializing interpret_model()
2025-05-03 12:49:42,129:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E714517E90>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 12:49:42,129:INFO:Checking exceptions
2025-05-03 12:49:42,130:ERROR:
'interpret' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2025-05-03 12:50:00,944:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:50:00,944:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:50:00,944:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:50:00,944:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:55:02,702:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:55:02,702:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:55:02,702:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:55:02,702:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 12:55:06,133:INFO:PyCaret ClassificationExperiment
2025-05-03 12:55:06,133:INFO:Logging name: clf-default-name
2025-05-03 12:55:06,133:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 12:55:06,133:INFO:version 3.3.2
2025-05-03 12:55:06,133:INFO:Initializing setup()
2025-05-03 12:55:06,133:INFO:self.USI: 88f3
2025-05-03 12:55:06,133:INFO:self._variable_keys: {'gpu_param', 'fold_shuffle_param', 'fix_imbalance', 'html_param', 'y', '_ml_usecase', 'is_multiclass', 'log_plots_param', 'y_train', 'pipeline', 'fold_generator', 'fold_groups_param', 'X', 'USI', 'exp_name_log', '_available_plots', 'target_param', 'n_jobs_param', 'idx', 'seed', 'X_test', 'X_train', 'exp_id', 'logging_param', 'gpu_n_jobs_param', 'memory', 'data', 'y_test'}
2025-05-03 12:55:06,133:INFO:Checking environment
2025-05-03 12:55:06,133:INFO:python_version: 3.11.11
2025-05-03 12:55:06,133:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 12:55:06,133:INFO:machine: AMD64
2025-05-03 12:55:06,133:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 12:55:06,139:INFO:Memory: svmem(total=16965230592, available=4170690560, percent=75.4, used=12794540032, free=4170690560)
2025-05-03 12:55:06,139:INFO:Physical Core: 4
2025-05-03 12:55:06,139:INFO:Logical Core: 8
2025-05-03 12:55:06,139:INFO:Checking libraries
2025-05-03 12:55:06,139:INFO:System:
2025-05-03 12:55:06,139:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 12:55:06,139:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 12:55:06,139:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 12:55:06,139:INFO:PyCaret required dependencies:
2025-05-03 12:55:06,141:INFO:                 pip: 25.0
2025-05-03 12:55:06,141:INFO:          setuptools: 75.8.0
2025-05-03 12:55:06,141:INFO:             pycaret: 3.3.2
2025-05-03 12:55:06,141:INFO:             IPython: 8.32.0
2025-05-03 12:55:06,141:INFO:          ipywidgets: 8.1.6
2025-05-03 12:55:06,141:INFO:                tqdm: 4.67.1
2025-05-03 12:55:06,141:INFO:               numpy: 1.26.4
2025-05-03 12:55:06,141:INFO:              pandas: 2.1.4
2025-05-03 12:55:06,141:INFO:              jinja2: 3.1.6
2025-05-03 12:55:06,141:INFO:               scipy: 1.11.4
2025-05-03 12:55:06,141:INFO:              joblib: 1.3.2
2025-05-03 12:55:06,141:INFO:             sklearn: 1.4.2
2025-05-03 12:55:06,141:INFO:                pyod: 2.0.5
2025-05-03 12:55:06,141:INFO:            imblearn: 0.13.0
2025-05-03 12:55:06,141:INFO:   category_encoders: 2.7.0
2025-05-03 12:55:06,141:INFO:            lightgbm: 4.6.0
2025-05-03 12:55:06,141:INFO:               numba: 0.61.2
2025-05-03 12:55:06,141:INFO:            requests: 2.32.3
2025-05-03 12:55:06,141:INFO:          matplotlib: 3.7.5
2025-05-03 12:55:06,141:INFO:          scikitplot: 0.3.7
2025-05-03 12:55:06,141:INFO:         yellowbrick: 1.5
2025-05-03 12:55:06,141:INFO:              plotly: 5.24.1
2025-05-03 12:55:06,141:INFO:    plotly-resampler: Not installed
2025-05-03 12:55:06,141:INFO:             kaleido: 0.2.1
2025-05-03 12:55:06,141:INFO:           schemdraw: 0.15
2025-05-03 12:55:06,141:INFO:         statsmodels: 0.14.4
2025-05-03 12:55:06,141:INFO:              sktime: 0.26.0
2025-05-03 12:55:06,141:INFO:               tbats: 1.1.3
2025-05-03 12:55:06,141:INFO:            pmdarima: 2.0.4
2025-05-03 12:55:06,141:INFO:              psutil: 6.1.1
2025-05-03 12:55:06,141:INFO:          markupsafe: 3.0.2
2025-05-03 12:55:06,141:INFO:             pickle5: Not installed
2025-05-03 12:55:06,141:INFO:         cloudpickle: 3.1.1
2025-05-03 12:55:06,141:INFO:         deprecation: 2.1.0
2025-05-03 12:55:06,141:INFO:              xxhash: 3.5.0
2025-05-03 12:55:06,141:INFO:           wurlitzer: Not installed
2025-05-03 12:55:06,141:INFO:PyCaret optional dependencies:
2025-05-03 12:55:06,470:INFO:                shap: 0.47.2
2025-05-03 12:55:06,470:INFO:           interpret: Not installed
2025-05-03 12:55:06,470:INFO:                umap: Not installed
2025-05-03 12:55:06,470:INFO:     ydata_profiling: Not installed
2025-05-03 12:55:06,470:INFO:  explainerdashboard: Not installed
2025-05-03 12:55:06,470:INFO:             autoviz: Not installed
2025-05-03 12:55:06,470:INFO:           fairlearn: Not installed
2025-05-03 12:55:06,470:INFO:          deepchecks: Not installed
2025-05-03 12:55:06,470:INFO:             xgboost: 3.0.0
2025-05-03 12:55:06,470:INFO:            catboost: Not installed
2025-05-03 12:55:06,470:INFO:              kmodes: Not installed
2025-05-03 12:55:06,470:INFO:             mlxtend: Not installed
2025-05-03 12:55:06,470:INFO:       statsforecast: Not installed
2025-05-03 12:55:06,470:INFO:        tune_sklearn: Not installed
2025-05-03 12:55:06,470:INFO:                 ray: Not installed
2025-05-03 12:55:06,470:INFO:            hyperopt: 0.2.7
2025-05-03 12:55:06,470:INFO:              optuna: Not installed
2025-05-03 12:55:06,470:INFO:               skopt: 0.10.2
2025-05-03 12:55:06,470:INFO:              mlflow: 2.22.0
2025-05-03 12:55:06,470:INFO:              gradio: Not installed
2025-05-03 12:55:06,470:INFO:             fastapi: 0.115.12
2025-05-03 12:55:06,470:INFO:             uvicorn: 0.34.2
2025-05-03 12:55:06,470:INFO:              m2cgen: Not installed
2025-05-03 12:55:06,470:INFO:           evidently: Not installed
2025-05-03 12:55:06,470:INFO:               fugue: Not installed
2025-05-03 12:55:06,470:INFO:           streamlit: Not installed
2025-05-03 12:55:06,470:INFO:             prophet: Not installed
2025-05-03 12:55:06,470:INFO:None
2025-05-03 12:55:06,470:INFO:Set up data.
2025-05-03 12:55:06,488:INFO:Set up folding strategy.
2025-05-03 12:55:06,488:INFO:Set up train/test split.
2025-05-03 12:55:06,523:INFO:Set up index.
2025-05-03 12:55:06,525:INFO:Assigning column types.
2025-05-03 12:55:06,536:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 12:55:06,586:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,586:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,619:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:06,619:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:06,669:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,669:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,708:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:06,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:06,716:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 12:55:06,760:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,786:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:06,796:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:06,842:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 12:55:06,883:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:06,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:06,888:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 12:55:06,963:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:06,963:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:07,038:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:07,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:07,062:INFO:Finished creating preprocessing pipeline.
2025-05-03 12:55:07,062:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 12:55:07,062:INFO:Creating final display dataframe.
2025-05-03 12:55:07,154:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 12:55:07,238:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:07,240:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:07,318:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 12:55:07,323:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 12:55:07,323:INFO:setup() successfully completed in 1.2s...............
2025-05-03 12:55:07,335:INFO:Initializing compare_models()
2025-05-03 12:55:07,335:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 12:55:07,335:INFO:Checking exceptions
2025-05-03 12:55:07,351:INFO:Preparing display monitor
2025-05-03 12:55:07,510:INFO:Initializing Logistic Regression
2025-05-03 12:55:07,510:INFO:Total runtime is 0.0 minutes
2025-05-03 12:55:07,523:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:07,523:INFO:Initializing create_model()
2025-05-03 12:55:07,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:07,523:INFO:Checking exceptions
2025-05-03 12:55:07,523:INFO:Importing libraries
2025-05-03 12:55:07,523:INFO:Copying training dataset
2025-05-03 12:55:07,558:INFO:Defining folds
2025-05-03 12:55:07,558:INFO:Declaring metric variables
2025-05-03 12:55:07,563:INFO:Importing untrained model
2025-05-03 12:55:07,569:INFO:Logistic Regression Imported successfully
2025-05-03 12:55:07,577:INFO:Starting cross validation
2025-05-03 12:55:07,579:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:11,383:INFO:Calculating mean and std
2025-05-03 12:55:11,385:INFO:Creating metrics dataframe
2025-05-03 12:55:11,385:INFO:Uploading results into container
2025-05-03 12:55:11,385:INFO:Uploading model into container now
2025-05-03 12:55:11,385:INFO:_master_model_container: 1
2025-05-03 12:55:11,385:INFO:_display_container: 2
2025-05-03 12:55:11,385:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 12:55:11,385:INFO:create_model() successfully completed......................................
2025-05-03 12:55:11,527:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:11,527:INFO:Creating metrics dataframe
2025-05-03 12:55:11,551:INFO:Initializing Random Forest Classifier
2025-05-03 12:55:11,551:INFO:Total runtime is 0.0673435648282369 minutes
2025-05-03 12:55:11,551:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:11,551:INFO:Initializing create_model()
2025-05-03 12:55:11,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:11,551:INFO:Checking exceptions
2025-05-03 12:55:11,551:INFO:Importing libraries
2025-05-03 12:55:11,551:INFO:Copying training dataset
2025-05-03 12:55:11,576:INFO:Defining folds
2025-05-03 12:55:11,576:INFO:Declaring metric variables
2025-05-03 12:55:11,581:INFO:Importing untrained model
2025-05-03 12:55:11,581:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:55:11,592:INFO:Starting cross validation
2025-05-03 12:55:11,592:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:15,777:INFO:Calculating mean and std
2025-05-03 12:55:15,780:INFO:Creating metrics dataframe
2025-05-03 12:55:15,782:INFO:Uploading results into container
2025-05-03 12:55:15,784:INFO:Uploading model into container now
2025-05-03 12:55:15,785:INFO:_master_model_container: 2
2025-05-03 12:55:15,785:INFO:_display_container: 2
2025-05-03 12:55:15,785:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:55:15,785:INFO:create_model() successfully completed......................................
2025-05-03 12:55:15,944:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:15,944:INFO:Creating metrics dataframe
2025-05-03 12:55:15,951:INFO:Initializing Extreme Gradient Boosting
2025-05-03 12:55:15,951:INFO:Total runtime is 0.14067137638727825 minutes
2025-05-03 12:55:15,959:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:15,959:INFO:Initializing create_model()
2025-05-03 12:55:15,959:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:15,959:INFO:Checking exceptions
2025-05-03 12:55:15,959:INFO:Importing libraries
2025-05-03 12:55:15,959:INFO:Copying training dataset
2025-05-03 12:55:15,982:INFO:Defining folds
2025-05-03 12:55:15,982:INFO:Declaring metric variables
2025-05-03 12:55:15,988:INFO:Importing untrained model
2025-05-03 12:55:15,995:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:55:16,003:INFO:Starting cross validation
2025-05-03 12:55:16,003:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:19,764:INFO:Calculating mean and std
2025-05-03 12:55:19,766:INFO:Creating metrics dataframe
2025-05-03 12:55:19,766:INFO:Uploading results into container
2025-05-03 12:55:19,766:INFO:Uploading model into container now
2025-05-03 12:55:19,766:INFO:_master_model_container: 3
2025-05-03 12:55:19,766:INFO:_display_container: 2
2025-05-03 12:55:19,766:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:55:19,766:INFO:create_model() successfully completed......................................
2025-05-03 12:55:19,890:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:19,890:INFO:Creating metrics dataframe
2025-05-03 12:55:19,902:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 12:55:19,902:INFO:Total runtime is 0.20652323961257935 minutes
2025-05-03 12:55:19,906:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:19,906:INFO:Initializing create_model()
2025-05-03 12:55:19,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:19,906:INFO:Checking exceptions
2025-05-03 12:55:19,906:INFO:Importing libraries
2025-05-03 12:55:19,906:INFO:Copying training dataset
2025-05-03 12:55:19,923:INFO:Defining folds
2025-05-03 12:55:19,923:INFO:Declaring metric variables
2025-05-03 12:55:19,932:INFO:Importing untrained model
2025-05-03 12:55:19,932:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:55:19,945:INFO:Starting cross validation
2025-05-03 12:55:19,945:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:20,431:INFO:Calculating mean and std
2025-05-03 12:55:20,431:INFO:Creating metrics dataframe
2025-05-03 12:55:20,435:INFO:Uploading results into container
2025-05-03 12:55:20,435:INFO:Uploading model into container now
2025-05-03 12:55:20,435:INFO:_master_model_container: 4
2025-05-03 12:55:20,435:INFO:_display_container: 2
2025-05-03 12:55:20,437:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:55:20,437:INFO:create_model() successfully completed......................................
2025-05-03 12:55:20,576:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:20,577:INFO:Creating metrics dataframe
2025-05-03 12:55:20,586:INFO:Initializing Extra Trees Classifier
2025-05-03 12:55:20,586:INFO:Total runtime is 0.21792579889297486 minutes
2025-05-03 12:55:20,594:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:20,594:INFO:Initializing create_model()
2025-05-03 12:55:20,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:20,594:INFO:Checking exceptions
2025-05-03 12:55:20,594:INFO:Importing libraries
2025-05-03 12:55:20,594:INFO:Copying training dataset
2025-05-03 12:55:20,636:INFO:Defining folds
2025-05-03 12:55:20,636:INFO:Declaring metric variables
2025-05-03 12:55:20,644:INFO:Importing untrained model
2025-05-03 12:55:20,652:INFO:Extra Trees Classifier Imported successfully
2025-05-03 12:55:20,660:INFO:Starting cross validation
2025-05-03 12:55:20,660:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:23,461:INFO:Calculating mean and std
2025-05-03 12:55:23,461:INFO:Creating metrics dataframe
2025-05-03 12:55:23,479:INFO:Uploading results into container
2025-05-03 12:55:23,483:INFO:Uploading model into container now
2025-05-03 12:55:23,487:INFO:_master_model_container: 5
2025-05-03 12:55:23,489:INFO:_display_container: 2
2025-05-03 12:55:23,490:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 12:55:23,494:INFO:create_model() successfully completed......................................
2025-05-03 12:55:23,779:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:23,779:INFO:Creating metrics dataframe
2025-05-03 12:55:23,804:INFO:Initializing Ridge Classifier
2025-05-03 12:55:23,804:INFO:Total runtime is 0.2715631008148193 minutes
2025-05-03 12:55:23,821:INFO:SubProcess create_model() called ==================================
2025-05-03 12:55:23,822:INFO:Initializing create_model()
2025-05-03 12:55:23,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F499839550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:23,823:INFO:Checking exceptions
2025-05-03 12:55:23,823:INFO:Importing libraries
2025-05-03 12:55:23,823:INFO:Copying training dataset
2025-05-03 12:55:23,880:INFO:Defining folds
2025-05-03 12:55:23,880:INFO:Declaring metric variables
2025-05-03 12:55:23,895:INFO:Importing untrained model
2025-05-03 12:55:23,903:INFO:Ridge Classifier Imported successfully
2025-05-03 12:55:23,928:INFO:Starting cross validation
2025-05-03 12:55:23,928:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:24,169:INFO:Calculating mean and std
2025-05-03 12:55:24,173:INFO:Creating metrics dataframe
2025-05-03 12:55:24,182:INFO:Uploading results into container
2025-05-03 12:55:24,182:INFO:Uploading model into container now
2025-05-03 12:55:24,182:INFO:_master_model_container: 6
2025-05-03 12:55:24,182:INFO:_display_container: 2
2025-05-03 12:55:24,182:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 12:55:24,188:INFO:create_model() successfully completed......................................
2025-05-03 12:55:24,445:INFO:SubProcess create_model() end ==================================
2025-05-03 12:55:24,445:INFO:Creating metrics dataframe
2025-05-03 12:55:24,485:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 12:55:24,519:INFO:Initializing create_model()
2025-05-03 12:55:24,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:24,519:INFO:Checking exceptions
2025-05-03 12:55:24,524:INFO:Importing libraries
2025-05-03 12:55:24,528:INFO:Copying training dataset
2025-05-03 12:55:24,594:INFO:Defining folds
2025-05-03 12:55:24,602:INFO:Declaring metric variables
2025-05-03 12:55:24,603:INFO:Importing untrained model
2025-05-03 12:55:24,603:INFO:Declaring custom model
2025-05-03 12:55:24,603:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:55:24,603:INFO:Cross validation set to False
2025-05-03 12:55:24,611:INFO:Fitting Model
2025-05-03 12:55:24,677:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:55:24,686:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003211 seconds.
2025-05-03 12:55:24,688:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:55:24,688:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:55:24,688:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:55:24,688:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:55:24,690:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:55:24,690:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:55:25,331:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:55:25,332:INFO:create_model() successfully completed......................................
2025-05-03 12:55:25,680:INFO:_master_model_container: 6
2025-05-03 12:55:25,680:INFO:_display_container: 2
2025-05-03 12:55:25,682:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:55:25,682:INFO:compare_models() successfully completed......................................
2025-05-03 12:55:25,727:INFO:Initializing create_model()
2025-05-03 12:55:25,727:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:25,727:INFO:Checking exceptions
2025-05-03 12:55:25,771:INFO:Importing libraries
2025-05-03 12:55:25,772:INFO:Copying training dataset
2025-05-03 12:55:25,882:INFO:Defining folds
2025-05-03 12:55:25,882:INFO:Declaring metric variables
2025-05-03 12:55:25,895:INFO:Importing untrained model
2025-05-03 12:55:25,904:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 12:55:25,928:INFO:Starting cross validation
2025-05-03 12:55:25,928:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:27,756:INFO:Calculating mean and std
2025-05-03 12:55:27,759:INFO:Creating metrics dataframe
2025-05-03 12:55:27,783:INFO:Finalizing model
2025-05-03 12:55:27,843:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 12:55:27,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002165 seconds.
2025-05-03 12:55:27,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 12:55:27,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 12:55:27,853:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 12:55:27,855:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 12:55:27,855:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 12:55:27,855:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 12:55:28,462:INFO:Uploading results into container
2025-05-03 12:55:28,463:INFO:Uploading model into container now
2025-05-03 12:55:28,498:INFO:_master_model_container: 7
2025-05-03 12:55:28,498:INFO:_display_container: 3
2025-05-03 12:55:28,521:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 12:55:28,537:INFO:create_model() successfully completed......................................
2025-05-03 12:55:28,937:INFO:Initializing create_model()
2025-05-03 12:55:28,939:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:28,939:INFO:Checking exceptions
2025-05-03 12:55:29,001:INFO:Importing libraries
2025-05-03 12:55:29,006:INFO:Copying training dataset
2025-05-03 12:55:29,055:INFO:Defining folds
2025-05-03 12:55:29,055:INFO:Declaring metric variables
2025-05-03 12:55:29,057:INFO:Importing untrained model
2025-05-03 12:55:29,064:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 12:55:29,071:INFO:Starting cross validation
2025-05-03 12:55:29,071:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:29,431:INFO:Calculating mean and std
2025-05-03 12:55:29,431:INFO:Creating metrics dataframe
2025-05-03 12:55:29,431:INFO:Finalizing model
2025-05-03 12:55:29,672:INFO:Uploading results into container
2025-05-03 12:55:29,675:INFO:Uploading model into container now
2025-05-03 12:55:29,758:INFO:_master_model_container: 8
2025-05-03 12:55:29,758:INFO:_display_container: 4
2025-05-03 12:55:29,763:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 12:55:29,763:INFO:create_model() successfully completed......................................
2025-05-03 12:55:30,064:INFO:Initializing create_model()
2025-05-03 12:55:30,067:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 12:55:30,067:INFO:Checking exceptions
2025-05-03 12:55:30,115:INFO:Importing libraries
2025-05-03 12:55:30,115:INFO:Copying training dataset
2025-05-03 12:55:30,179:INFO:Defining folds
2025-05-03 12:55:30,179:INFO:Declaring metric variables
2025-05-03 12:55:30,191:INFO:Importing untrained model
2025-05-03 12:55:30,200:INFO:Random Forest Classifier Imported successfully
2025-05-03 12:55:30,225:INFO:Starting cross validation
2025-05-03 12:55:30,229:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 12:55:33,898:INFO:Calculating mean and std
2025-05-03 12:55:33,902:INFO:Creating metrics dataframe
2025-05-03 12:55:33,916:INFO:Finalizing model
2025-05-03 12:55:35,516:INFO:Uploading results into container
2025-05-03 12:55:35,518:INFO:Uploading model into container now
2025-05-03 12:55:35,544:INFO:_master_model_container: 9
2025-05-03 12:55:35,544:INFO:_display_container: 5
2025-05-03 12:55:35,546:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 12:55:35,547:INFO:create_model() successfully completed......................................
2025-05-03 12:55:35,843:INFO:Initializing tune_model()
2025-05-03 12:55:35,843:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 12:55:35,843:INFO:Checking exceptions
2025-05-03 12:55:35,843:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 12:55:35,925:INFO:Copying training dataset
2025-05-03 12:55:35,976:INFO:Checking base model
2025-05-03 12:55:35,979:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 12:55:35,994:INFO:Declaring metric variables
2025-05-03 12:55:36,007:INFO:Defining Hyperparameters
2025-05-03 12:55:36,264:INFO:Tuning with n_jobs=-1
2025-05-03 12:55:36,281:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:00:07,094:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 13:00:07,094:INFO:Hyperparameter search completed
2025-05-03 13:00:07,094:INFO:SubProcess create_model() called ==================================
2025-05-03 13:00:07,102:INFO:Initializing create_model()
2025-05-03 13:00:07,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F4937B6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 13:00:07,102:INFO:Checking exceptions
2025-05-03 13:00:07,102:INFO:Importing libraries
2025-05-03 13:00:07,102:INFO:Copying training dataset
2025-05-03 13:00:07,161:INFO:Defining folds
2025-05-03 13:00:07,161:INFO:Declaring metric variables
2025-05-03 13:00:07,169:INFO:Importing untrained model
2025-05-03 13:00:07,169:INFO:Declaring custom model
2025-05-03 13:00:07,189:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:00:07,213:INFO:Starting cross validation
2025-05-03 13:00:07,213:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:00:08,430:INFO:Calculating mean and std
2025-05-03 13:00:08,431:INFO:Creating metrics dataframe
2025-05-03 13:00:08,448:INFO:Finalizing model
2025-05-03 13:00:08,465:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:00:08,465:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:00:08,465:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:00:08,510:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:00:08,510:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:00:08,510:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:00:08,510:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:00:08,518:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002094 seconds.
2025-05-03 13:00:08,520:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:00:08,520:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:00:08,520:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:00:08,521:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:00:08,522:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:00:08,522:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:00:08,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,771:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,788:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,792:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,794:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,795:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,799:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,801:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,808:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,812:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,818:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,821:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,823:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,825:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,828:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,830:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,834:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,837:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,838:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,841:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,843:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,843:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,844:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,847:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,847:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,849:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,851:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,853:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,854:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,856:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,858:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,858:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,860:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,863:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,863:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,865:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,867:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,869:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,871:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,873:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,873:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,880:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,880:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,884:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,886:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,886:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,902:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,906:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,906:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,910:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,910:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,912:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,938:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,942:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,944:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,946:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,946:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,948:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,950:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,952:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,954:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,958:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,966:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,968:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,968:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,972:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,974:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,979:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:08,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:00:08,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:00:09,013:INFO:Uploading results into container
2025-05-03 13:00:09,016:INFO:Uploading model into container now
2025-05-03 13:00:09,018:INFO:_master_model_container: 10
2025-05-03 13:00:09,018:INFO:_display_container: 6
2025-05-03 13:00:09,020:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:00:09,020:INFO:create_model() successfully completed......................................
2025-05-03 13:00:09,253:INFO:SubProcess create_model() end ==================================
2025-05-03 13:00:09,253:INFO:choose_better activated
2025-05-03 13:00:09,270:INFO:SubProcess create_model() called ==================================
2025-05-03 13:00:09,270:INFO:Initializing create_model()
2025-05-03 13:00:09,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:00:09,270:INFO:Checking exceptions
2025-05-03 13:00:09,275:INFO:Importing libraries
2025-05-03 13:00:09,275:INFO:Copying training dataset
2025-05-03 13:00:09,326:INFO:Defining folds
2025-05-03 13:00:09,326:INFO:Declaring metric variables
2025-05-03 13:00:09,326:INFO:Importing untrained model
2025-05-03 13:00:09,326:INFO:Declaring custom model
2025-05-03 13:00:09,335:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:00:09,335:INFO:Starting cross validation
2025-05-03 13:00:09,337:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:00:10,931:INFO:Calculating mean and std
2025-05-03 13:00:10,931:INFO:Creating metrics dataframe
2025-05-03 13:00:10,937:INFO:Finalizing model
2025-05-03 13:00:10,990:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:00:10,997:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002980 seconds.
2025-05-03 13:00:10,999:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:00:10,999:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:00:10,999:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:00:10,999:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:00:11,001:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:00:11,001:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:00:11,461:INFO:Uploading results into container
2025-05-03 13:00:11,463:INFO:Uploading model into container now
2025-05-03 13:00:11,465:INFO:_master_model_container: 11
2025-05-03 13:00:11,465:INFO:_display_container: 7
2025-05-03 13:00:11,469:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:00:11,469:INFO:create_model() successfully completed......................................
2025-05-03 13:00:11,707:INFO:SubProcess create_model() end ==================================
2025-05-03 13:00:11,715:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 13:00:11,717:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 13:00:11,723:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 13:00:11,723:INFO:choose_better completed
2025-05-03 13:00:11,755:INFO:_master_model_container: 11
2025-05-03 13:00:11,755:INFO:_display_container: 6
2025-05-03 13:00:11,755:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:00:11,755:INFO:tune_model() successfully completed......................................
2025-05-03 13:00:12,046:INFO:Initializing tune_model()
2025-05-03 13:00:12,046:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 13:00:12,046:INFO:Checking exceptions
2025-05-03 13:00:12,052:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 13:00:12,127:INFO:Copying training dataset
2025-05-03 13:00:12,183:INFO:Checking base model
2025-05-03 13:00:12,183:INFO:Base model : Extreme Gradient Boosting
2025-05-03 13:00:12,219:INFO:Declaring metric variables
2025-05-03 13:00:12,241:INFO:Defining Hyperparameters
2025-05-03 13:00:12,537:INFO:Tuning with n_jobs=-1
2025-05-03 13:00:12,554:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:02:58,321:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 13:02:58,336:INFO:Hyperparameter search completed
2025-05-03 13:02:58,336:INFO:SubProcess create_model() called ==================================
2025-05-03 13:02:58,336:INFO:Initializing create_model()
2025-05-03 13:02:58,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F4937B6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 13:02:58,336:INFO:Checking exceptions
2025-05-03 13:02:58,336:INFO:Importing libraries
2025-05-03 13:02:58,336:INFO:Copying training dataset
2025-05-03 13:02:58,352:INFO:Defining folds
2025-05-03 13:02:58,352:INFO:Declaring metric variables
2025-05-03 13:02:58,352:INFO:Importing untrained model
2025-05-03 13:02:58,368:INFO:Declaring custom model
2025-05-03 13:02:58,368:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:02:58,368:INFO:Starting cross validation
2025-05-03 13:02:58,384:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:02:58,810:INFO:Calculating mean and std
2025-05-03 13:02:58,810:INFO:Creating metrics dataframe
2025-05-03 13:02:58,816:INFO:Finalizing model
2025-05-03 13:02:59,066:INFO:Uploading results into container
2025-05-03 13:02:59,066:INFO:Uploading model into container now
2025-05-03 13:02:59,066:INFO:_master_model_container: 12
2025-05-03 13:02:59,066:INFO:_display_container: 7
2025-05-03 13:02:59,068:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:02:59,068:INFO:create_model() successfully completed......................................
2025-05-03 13:02:59,209:INFO:SubProcess create_model() end ==================================
2025-05-03 13:02:59,209:INFO:choose_better activated
2025-05-03 13:02:59,209:INFO:SubProcess create_model() called ==================================
2025-05-03 13:02:59,209:INFO:Initializing create_model()
2025-05-03 13:02:59,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:02:59,215:INFO:Checking exceptions
2025-05-03 13:02:59,215:INFO:Importing libraries
2025-05-03 13:02:59,215:INFO:Copying training dataset
2025-05-03 13:02:59,233:INFO:Defining folds
2025-05-03 13:02:59,233:INFO:Declaring metric variables
2025-05-03 13:02:59,233:INFO:Importing untrained model
2025-05-03 13:02:59,233:INFO:Declaring custom model
2025-05-03 13:02:59,233:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:02:59,233:INFO:Starting cross validation
2025-05-03 13:02:59,233:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:02:59,591:INFO:Calculating mean and std
2025-05-03 13:02:59,591:INFO:Creating metrics dataframe
2025-05-03 13:02:59,592:INFO:Finalizing model
2025-05-03 13:02:59,726:INFO:Uploading results into container
2025-05-03 13:02:59,727:INFO:Uploading model into container now
2025-05-03 13:02:59,727:INFO:_master_model_container: 13
2025-05-03 13:02:59,727:INFO:_display_container: 8
2025-05-03 13:02:59,729:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:02:59,729:INFO:create_model() successfully completed......................................
2025-05-03 13:02:59,865:INFO:SubProcess create_model() end ==================================
2025-05-03 13:02:59,868:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 13:02:59,868:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 13:02:59,868:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 13:02:59,868:INFO:choose_better completed
2025-05-03 13:02:59,879:INFO:_master_model_container: 13
2025-05-03 13:02:59,879:INFO:_display_container: 7
2025-05-03 13:02:59,881:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:02:59,881:INFO:tune_model() successfully completed......................................
2025-05-03 13:03:00,025:INFO:Initializing tune_model()
2025-05-03 13:03:00,025:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 13:03:00,025:INFO:Checking exceptions
2025-05-03 13:03:00,025:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 13:03:00,044:INFO:Copying training dataset
2025-05-03 13:03:00,066:INFO:Checking base model
2025-05-03 13:03:00,066:INFO:Base model : Random Forest Classifier
2025-05-03 13:03:00,069:INFO:Declaring metric variables
2025-05-03 13:03:00,076:INFO:Defining Hyperparameters
2025-05-03 13:03:00,219:INFO:Tuning with n_jobs=-1
2025-05-03 13:03:00,228:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:07:03,590:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 13:07:03,594:INFO:Hyperparameter search completed
2025-05-03 13:07:03,594:INFO:SubProcess create_model() called ==================================
2025-05-03 13:07:03,595:INFO:Initializing create_model()
2025-05-03 13:07:03,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F4937B6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 13:07:03,596:INFO:Checking exceptions
2025-05-03 13:07:03,596:INFO:Importing libraries
2025-05-03 13:07:03,596:INFO:Copying training dataset
2025-05-03 13:07:03,618:INFO:Defining folds
2025-05-03 13:07:03,618:INFO:Declaring metric variables
2025-05-03 13:07:03,618:INFO:Importing untrained model
2025-05-03 13:07:03,618:INFO:Declaring custom model
2025-05-03 13:07:03,628:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:07:03,637:INFO:Starting cross validation
2025-05-03 13:07:03,638:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:07:08,280:INFO:Calculating mean and std
2025-05-03 13:07:08,280:INFO:Creating metrics dataframe
2025-05-03 13:07:08,290:INFO:Finalizing model
2025-05-03 13:07:10,889:INFO:Uploading results into container
2025-05-03 13:07:10,889:INFO:Uploading model into container now
2025-05-03 13:07:10,889:INFO:_master_model_container: 14
2025-05-03 13:07:10,889:INFO:_display_container: 8
2025-05-03 13:07:10,889:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 13:07:10,889:INFO:create_model() successfully completed......................................
2025-05-03 13:07:10,989:INFO:SubProcess create_model() end ==================================
2025-05-03 13:07:10,989:INFO:choose_better activated
2025-05-03 13:07:10,989:INFO:SubProcess create_model() called ==================================
2025-05-03 13:07:10,989:INFO:Initializing create_model()
2025-05-03 13:07:10,989:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:07:10,989:INFO:Checking exceptions
2025-05-03 13:07:10,989:INFO:Importing libraries
2025-05-03 13:07:10,989:INFO:Copying training dataset
2025-05-03 13:07:11,006:INFO:Defining folds
2025-05-03 13:07:11,006:INFO:Declaring metric variables
2025-05-03 13:07:11,006:INFO:Importing untrained model
2025-05-03 13:07:11,006:INFO:Declaring custom model
2025-05-03 13:07:11,006:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:07:11,006:INFO:Starting cross validation
2025-05-03 13:07:11,006:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:07:12,408:INFO:Calculating mean and std
2025-05-03 13:07:12,408:INFO:Creating metrics dataframe
2025-05-03 13:07:12,408:INFO:Finalizing model
2025-05-03 13:07:13,080:INFO:Uploading results into container
2025-05-03 13:07:13,081:INFO:Uploading model into container now
2025-05-03 13:07:13,082:INFO:_master_model_container: 15
2025-05-03 13:07:13,082:INFO:_display_container: 9
2025-05-03 13:07:13,082:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:07:13,083:INFO:create_model() successfully completed......................................
2025-05-03 13:07:13,221:INFO:SubProcess create_model() end ==================================
2025-05-03 13:07:13,223:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 13:07:13,224:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 13:07:13,225:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 13:07:13,225:INFO:choose_better completed
2025-05-03 13:07:13,237:INFO:_master_model_container: 15
2025-05-03 13:07:13,237:INFO:_display_container: 8
2025-05-03 13:07:13,239:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 13:07:13,239:INFO:tune_model() successfully completed......................................
2025-05-03 13:07:13,405:INFO:Initializing create_model()
2025-05-03 13:07:13,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 13:07:13,405:INFO:Checking exceptions
2025-05-03 13:07:13,425:INFO:Importing libraries
2025-05-03 13:07:13,425:INFO:Copying training dataset
2025-05-03 13:07:13,457:INFO:Defining folds
2025-05-03 13:07:13,460:INFO:Declaring metric variables
2025-05-03 13:07:13,481:INFO:Importing untrained model
2025-05-03 13:07:13,515:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:07:13,530:INFO:Starting cross validation
2025-05-03 13:07:13,530:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:07:14,052:INFO:Calculating mean and std
2025-05-03 13:07:14,052:INFO:Creating metrics dataframe
2025-05-03 13:07:14,058:INFO:Finalizing model
2025-05-03 13:07:14,064:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:07:14,064:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:07:14,064:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:07:14,081:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:07:14,081:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:07:14,081:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:07:14,081:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:07:14,086:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002079 seconds.
2025-05-03 13:07:14,086:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 13:07:14,086:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:07:14,086:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:07:14,086:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:07:14,086:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:07:14,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,207:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,221:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,222:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,223:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,240:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,242:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,242:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,245:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,247:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,251:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,251:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,251:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,253:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,253:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,257:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,257:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,257:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,259:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,261:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,264:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,266:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,268:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,268:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,272:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,272:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,284:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:07:14,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:07:14,320:INFO:Uploading results into container
2025-05-03 13:07:14,320:INFO:Uploading model into container now
2025-05-03 13:07:14,330:INFO:_master_model_container: 16
2025-05-03 13:07:14,331:INFO:_display_container: 9
2025-05-03 13:07:14,331:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:07:14,331:INFO:create_model() successfully completed......................................
2025-05-03 13:07:14,457:INFO:Initializing create_model()
2025-05-03 13:07:14,457:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 13:07:14,457:INFO:Checking exceptions
2025-05-03 13:07:14,472:INFO:Importing libraries
2025-05-03 13:07:14,472:INFO:Copying training dataset
2025-05-03 13:07:14,491:INFO:Defining folds
2025-05-03 13:07:14,491:INFO:Declaring metric variables
2025-05-03 13:07:14,495:INFO:Importing untrained model
2025-05-03 13:07:14,497:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:07:14,502:INFO:Starting cross validation
2025-05-03 13:07:14,503:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:07:14,918:INFO:Calculating mean and std
2025-05-03 13:07:14,918:INFO:Creating metrics dataframe
2025-05-03 13:07:14,923:INFO:Finalizing model
2025-05-03 13:07:15,074:INFO:Uploading results into container
2025-05-03 13:07:15,075:INFO:Uploading model into container now
2025-05-03 13:07:15,082:INFO:_master_model_container: 17
2025-05-03 13:07:15,084:INFO:_display_container: 10
2025-05-03 13:07:15,086:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:07:15,086:INFO:create_model() successfully completed......................................
2025-05-03 13:07:15,204:INFO:Initializing create_model()
2025-05-03 13:07:15,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 13:07:15,204:INFO:Checking exceptions
2025-05-03 13:07:15,224:INFO:Importing libraries
2025-05-03 13:07:15,224:INFO:Copying training dataset
2025-05-03 13:07:15,241:INFO:Defining folds
2025-05-03 13:07:15,241:INFO:Declaring metric variables
2025-05-03 13:07:15,244:INFO:Importing untrained model
2025-05-03 13:07:15,246:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:07:15,266:INFO:Starting cross validation
2025-05-03 13:07:15,266:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:07:19,778:INFO:Calculating mean and std
2025-05-03 13:07:19,778:INFO:Creating metrics dataframe
2025-05-03 13:07:19,782:INFO:Finalizing model
2025-05-03 13:07:22,103:INFO:Uploading results into container
2025-05-03 13:07:22,103:INFO:Uploading model into container now
2025-05-03 13:07:22,113:INFO:_master_model_container: 18
2025-05-03 13:07:22,113:INFO:_display_container: 11
2025-05-03 13:07:22,113:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 13:07:22,113:INFO:create_model() successfully completed......................................
2025-05-03 13:07:22,284:INFO:Initializing interpret_model()
2025-05-03 13:07:22,284:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:07:22,284:INFO:Checking exceptions
2025-05-03 13:07:22,284:ERROR:
'interpret' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2025-05-03 13:10:13,628:INFO:Initializing interpret_model()
2025-05-03 13:10:13,628:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F497C07050>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=       age         workclass  fnlwgt     education  educational-num  \
0       42           Private  145175  Some-college               10   
1       52  Self-emp-not-inc  175029          10th                6   
2       34         Local-gov  172664       HS-grad                9   
3       28           Private  125791       HS-grad                9   
4       46           Private   28419     Assoc-voc               11   
...    ...               ...     ...           ...              ...   
39068   36           Private  635913       HS-grad                9   
39069   34           Private  107624  Some-college               10   
39070   28           Private  250135  Some-college               10   
39071   46         State-gov   96652     Assoc-voc               11   
39072   59           Private  176118       Masters               14   

              marital-status         occupation   relationship   race     sex  \
0         Married-civ-spouse  Machine-op-inspct        Husband  White    Male   
1         Married-civ-spouse       Craft-repair        Husband  White    Male   
2         Married-civ-spouse       Craft-repair        Husband  White    Male   
3              Never-married       Adm-clerical  Not-in-family  White  Female   
4              Never-married   Transport-moving  Not-in-family  White    Male   
...                      ...                ...            ...    ...     ...   
39068  Married-spouse-absent      Other-service  Not-in-family  Black    Male   
39069     Married-civ-spouse       Craft-repair        Husband  White    Male   
39070               Divorced    Exec-managerial  Not-in-family  White  Female   
39071              Separated       Adm-clerical      Unmarried  Black  Female   
39072     Married-civ-spouse     Prof-specialty        Husband  White    Male   

       hours-per-week  es_estadounidense  has_capital_gain  has_capital_loss  
0                  40               True             False             False  
1                  35               True             False             False  
2                  40               True             False             False  
3                  40               True             False             False  
4                  50               True             False             False  
...               ...                ...               ...               ...  
39068              40               True             False             False  
39069              50               True             False             False  
39070              40               True             False             False  
39071              40               True             False             False  
39072               7               True             False             False  

[39073 rows x 14 columns], y_new_sample=       income
0           0
1           0
2           0
3           0
4           0
...       ...
39068       0
39069       0
39070       0
39071       0
39072       1

[39073 rows x 1 columns], save=False, kwargs={})
2025-05-03 13:10:13,639:INFO:Checking exceptions
2025-05-03 13:10:13,639:ERROR:
'interpret' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret` to install.
Alternately, you can install this by running `pip install pycaret[analysis]`
NoneType: None
2025-05-03 13:14:33,181:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:14:33,181:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:14:33,181:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:14:33,181:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:14:37,305:INFO:PyCaret ClassificationExperiment
2025-05-03 13:14:37,305:INFO:Logging name: clf-default-name
2025-05-03 13:14:37,305:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 13:14:37,305:INFO:version 3.3.2
2025-05-03 13:14:37,305:INFO:Initializing setup()
2025-05-03 13:14:37,305:INFO:self.USI: afe4
2025-05-03 13:14:37,305:INFO:self._variable_keys: {'exp_id', 'seed', 'exp_name_log', 'data', 'target_param', 'USI', 'gpu_n_jobs_param', 'idx', 'fold_generator', 'X_train', 'pipeline', 'memory', 'log_plots_param', 'X', 'is_multiclass', 'fold_shuffle_param', 'fold_groups_param', 'html_param', 'gpu_param', 'y', 'fix_imbalance', '_ml_usecase', 'n_jobs_param', '_available_plots', 'logging_param', 'X_test', 'y_test', 'y_train'}
2025-05-03 13:14:37,305:INFO:Checking environment
2025-05-03 13:14:37,305:INFO:python_version: 3.11.11
2025-05-03 13:14:37,305:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 13:14:37,305:INFO:machine: AMD64
2025-05-03 13:14:37,305:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 13:14:37,311:INFO:Memory: svmem(total=16965230592, available=4090355712, percent=75.9, used=12874874880, free=4090355712)
2025-05-03 13:14:37,311:INFO:Physical Core: 4
2025-05-03 13:14:37,311:INFO:Logical Core: 8
2025-05-03 13:14:37,311:INFO:Checking libraries
2025-05-03 13:14:37,311:INFO:System:
2025-05-03 13:14:37,311:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 13:14:37,311:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 13:14:37,311:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 13:14:37,311:INFO:PyCaret required dependencies:
2025-05-03 13:14:37,313:INFO:                 pip: 25.0
2025-05-03 13:14:37,313:INFO:          setuptools: 75.8.0
2025-05-03 13:14:37,313:INFO:             pycaret: 3.3.2
2025-05-03 13:14:37,313:INFO:             IPython: 8.32.0
2025-05-03 13:14:37,313:INFO:          ipywidgets: 8.1.6
2025-05-03 13:14:37,313:INFO:                tqdm: 4.67.1
2025-05-03 13:14:37,313:INFO:               numpy: 1.26.4
2025-05-03 13:14:37,313:INFO:              pandas: 2.1.4
2025-05-03 13:14:37,313:INFO:              jinja2: 3.1.6
2025-05-03 13:14:37,313:INFO:               scipy: 1.11.4
2025-05-03 13:14:37,313:INFO:              joblib: 1.3.2
2025-05-03 13:14:37,313:INFO:             sklearn: 1.4.2
2025-05-03 13:14:37,313:INFO:                pyod: 2.0.5
2025-05-03 13:14:37,313:INFO:            imblearn: 0.13.0
2025-05-03 13:14:37,313:INFO:   category_encoders: 2.7.0
2025-05-03 13:14:37,313:INFO:            lightgbm: 4.6.0
2025-05-03 13:14:37,313:INFO:               numba: 0.61.0
2025-05-03 13:14:37,313:INFO:            requests: 2.32.3
2025-05-03 13:14:37,313:INFO:          matplotlib: 3.7.5
2025-05-03 13:14:37,313:INFO:          scikitplot: 0.3.7
2025-05-03 13:14:37,313:INFO:         yellowbrick: 1.5
2025-05-03 13:14:37,313:INFO:              plotly: 5.24.1
2025-05-03 13:14:37,313:INFO:    plotly-resampler: Not installed
2025-05-03 13:14:37,314:INFO:             kaleido: 0.2.1
2025-05-03 13:14:37,314:INFO:           schemdraw: 0.15
2025-05-03 13:14:37,314:INFO:         statsmodels: 0.14.4
2025-05-03 13:14:37,314:INFO:              sktime: 0.26.0
2025-05-03 13:14:37,314:INFO:               tbats: 1.1.3
2025-05-03 13:14:37,314:INFO:            pmdarima: 2.0.4
2025-05-03 13:14:37,314:INFO:              psutil: 6.1.1
2025-05-03 13:14:37,314:INFO:          markupsafe: 3.0.2
2025-05-03 13:14:37,314:INFO:             pickle5: Not installed
2025-05-03 13:14:37,314:INFO:         cloudpickle: 3.1.1
2025-05-03 13:14:37,314:INFO:         deprecation: 2.1.0
2025-05-03 13:14:37,314:INFO:              xxhash: 3.5.0
2025-05-03 13:14:37,314:INFO:           wurlitzer: Not installed
2025-05-03 13:14:37,314:INFO:PyCaret optional dependencies:
2025-05-03 13:14:37,648:INFO:                shap: 0.47.2
2025-05-03 13:14:37,648:INFO:           interpret: 0.6.10
2025-05-03 13:14:37,648:INFO:                umap: Not installed
2025-05-03 13:14:37,649:INFO:     ydata_profiling: Not installed
2025-05-03 13:14:37,649:INFO:  explainerdashboard: Not installed
2025-05-03 13:14:37,649:INFO:             autoviz: Not installed
2025-05-03 13:14:37,649:INFO:           fairlearn: Not installed
2025-05-03 13:14:37,649:INFO:          deepchecks: Not installed
2025-05-03 13:14:37,649:INFO:             xgboost: 3.0.0
2025-05-03 13:14:37,649:INFO:            catboost: Not installed
2025-05-03 13:14:37,649:INFO:              kmodes: Not installed
2025-05-03 13:14:37,649:INFO:             mlxtend: Not installed
2025-05-03 13:14:37,649:INFO:       statsforecast: Not installed
2025-05-03 13:14:37,649:INFO:        tune_sklearn: Not installed
2025-05-03 13:14:37,649:INFO:                 ray: Not installed
2025-05-03 13:14:37,649:INFO:            hyperopt: 0.2.7
2025-05-03 13:14:37,649:INFO:              optuna: Not installed
2025-05-03 13:14:37,649:INFO:               skopt: 0.10.2
2025-05-03 13:14:37,649:INFO:              mlflow: 2.22.0
2025-05-03 13:14:37,649:INFO:              gradio: Not installed
2025-05-03 13:14:37,649:INFO:             fastapi: 0.115.12
2025-05-03 13:14:37,649:INFO:             uvicorn: 0.34.2
2025-05-03 13:14:37,649:INFO:              m2cgen: Not installed
2025-05-03 13:14:37,649:INFO:           evidently: Not installed
2025-05-03 13:14:37,649:INFO:               fugue: Not installed
2025-05-03 13:14:37,649:INFO:           streamlit: Not installed
2025-05-03 13:14:37,649:INFO:             prophet: Not installed
2025-05-03 13:14:37,649:INFO:None
2025-05-03 13:14:37,649:INFO:Set up data.
2025-05-03 13:14:37,663:INFO:Set up folding strategy.
2025-05-03 13:14:37,663:INFO:Set up train/test split.
2025-05-03 13:14:37,679:INFO:Set up index.
2025-05-03 13:14:37,680:INFO:Assigning column types.
2025-05-03 13:14:37,691:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 13:14:37,727:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,732:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,754:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:37,757:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:37,790:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,791:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,812:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:37,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:37,814:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 13:14:37,849:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,869:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:37,871:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:37,906:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:14:37,927:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:37,929:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:37,930:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 13:14:37,984:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:37,986:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:38,042:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:38,044:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:38,056:INFO:Finished creating preprocessing pipeline.
2025-05-03 13:14:38,058:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 13:14:38,058:INFO:Creating final display dataframe.
2025-05-03 13:14:38,114:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 13:14:38,169:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:38,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:38,232:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:14:38,235:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:14:38,236:INFO:setup() successfully completed in 0.93s...............
2025-05-03 13:14:38,248:INFO:Initializing compare_models()
2025-05-03 13:14:38,248:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 13:14:38,248:INFO:Checking exceptions
2025-05-03 13:14:38,259:INFO:Preparing display monitor
2025-05-03 13:14:38,282:INFO:Initializing Logistic Regression
2025-05-03 13:14:38,282:INFO:Total runtime is 0.0 minutes
2025-05-03 13:14:38,285:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:38,286:INFO:Initializing create_model()
2025-05-03 13:14:38,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:38,286:INFO:Checking exceptions
2025-05-03 13:14:38,286:INFO:Importing libraries
2025-05-03 13:14:38,286:INFO:Copying training dataset
2025-05-03 13:14:38,302:INFO:Defining folds
2025-05-03 13:14:38,302:INFO:Declaring metric variables
2025-05-03 13:14:38,305:INFO:Importing untrained model
2025-05-03 13:14:38,309:INFO:Logistic Regression Imported successfully
2025-05-03 13:14:38,314:INFO:Starting cross validation
2025-05-03 13:14:38,315:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:41,818:INFO:Calculating mean and std
2025-05-03 13:14:41,819:INFO:Creating metrics dataframe
2025-05-03 13:14:41,821:INFO:Uploading results into container
2025-05-03 13:14:41,821:INFO:Uploading model into container now
2025-05-03 13:14:41,822:INFO:_master_model_container: 1
2025-05-03 13:14:41,822:INFO:_display_container: 2
2025-05-03 13:14:41,822:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 13:14:41,822:INFO:create_model() successfully completed......................................
2025-05-03 13:14:41,927:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:41,928:INFO:Creating metrics dataframe
2025-05-03 13:14:41,933:INFO:Initializing Random Forest Classifier
2025-05-03 13:14:41,933:INFO:Total runtime is 0.060855499903361004 minutes
2025-05-03 13:14:41,937:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:41,937:INFO:Initializing create_model()
2025-05-03 13:14:41,938:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:41,938:INFO:Checking exceptions
2025-05-03 13:14:41,938:INFO:Importing libraries
2025-05-03 13:14:41,938:INFO:Copying training dataset
2025-05-03 13:14:41,952:INFO:Defining folds
2025-05-03 13:14:41,952:INFO:Declaring metric variables
2025-05-03 13:14:41,955:INFO:Importing untrained model
2025-05-03 13:14:41,959:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:14:41,964:INFO:Starting cross validation
2025-05-03 13:14:41,964:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:45,854:INFO:Calculating mean and std
2025-05-03 13:14:45,856:INFO:Creating metrics dataframe
2025-05-03 13:14:45,858:INFO:Uploading results into container
2025-05-03 13:14:45,858:INFO:Uploading model into container now
2025-05-03 13:14:45,859:INFO:_master_model_container: 2
2025-05-03 13:14:45,859:INFO:_display_container: 2
2025-05-03 13:14:45,859:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:14:45,859:INFO:create_model() successfully completed......................................
2025-05-03 13:14:45,973:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:45,973:INFO:Creating metrics dataframe
2025-05-03 13:14:45,981:INFO:Initializing Extreme Gradient Boosting
2025-05-03 13:14:45,982:INFO:Total runtime is 0.1283425529797872 minutes
2025-05-03 13:14:45,986:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:45,986:INFO:Initializing create_model()
2025-05-03 13:14:45,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:45,986:INFO:Checking exceptions
2025-05-03 13:14:45,987:INFO:Importing libraries
2025-05-03 13:14:45,987:INFO:Copying training dataset
2025-05-03 13:14:46,000:INFO:Defining folds
2025-05-03 13:14:46,002:INFO:Declaring metric variables
2025-05-03 13:14:46,006:INFO:Importing untrained model
2025-05-03 13:14:46,008:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:14:46,015:INFO:Starting cross validation
2025-05-03 13:14:46,016:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:48,719:INFO:Calculating mean and std
2025-05-03 13:14:48,720:INFO:Creating metrics dataframe
2025-05-03 13:14:48,722:INFO:Uploading results into container
2025-05-03 13:14:48,722:INFO:Uploading model into container now
2025-05-03 13:14:48,723:INFO:_master_model_container: 3
2025-05-03 13:14:48,723:INFO:_display_container: 2
2025-05-03 13:14:48,724:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:14:48,724:INFO:create_model() successfully completed......................................
2025-05-03 13:14:48,838:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:48,838:INFO:Creating metrics dataframe
2025-05-03 13:14:48,844:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 13:14:48,844:INFO:Total runtime is 0.17603705326716107 minutes
2025-05-03 13:14:48,847:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:48,848:INFO:Initializing create_model()
2025-05-03 13:14:48,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:48,848:INFO:Checking exceptions
2025-05-03 13:14:48,848:INFO:Importing libraries
2025-05-03 13:14:48,848:INFO:Copying training dataset
2025-05-03 13:14:48,863:INFO:Defining folds
2025-05-03 13:14:48,863:INFO:Declaring metric variables
2025-05-03 13:14:48,867:INFO:Importing untrained model
2025-05-03 13:14:48,871:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:14:48,877:INFO:Starting cross validation
2025-05-03 13:14:48,877:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:49,361:INFO:Calculating mean and std
2025-05-03 13:14:49,363:INFO:Creating metrics dataframe
2025-05-03 13:14:49,366:INFO:Uploading results into container
2025-05-03 13:14:49,369:INFO:Uploading model into container now
2025-05-03 13:14:49,370:INFO:_master_model_container: 4
2025-05-03 13:14:49,370:INFO:_display_container: 2
2025-05-03 13:14:49,371:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:14:49,371:INFO:create_model() successfully completed......................................
2025-05-03 13:14:49,521:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:49,521:INFO:Creating metrics dataframe
2025-05-03 13:14:49,527:INFO:Initializing Extra Trees Classifier
2025-05-03 13:14:49,527:INFO:Total runtime is 0.18742132981618248 minutes
2025-05-03 13:14:49,529:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:49,530:INFO:Initializing create_model()
2025-05-03 13:14:49,530:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:49,530:INFO:Checking exceptions
2025-05-03 13:14:49,530:INFO:Importing libraries
2025-05-03 13:14:49,530:INFO:Copying training dataset
2025-05-03 13:14:49,545:INFO:Defining folds
2025-05-03 13:14:49,546:INFO:Declaring metric variables
2025-05-03 13:14:49,549:INFO:Importing untrained model
2025-05-03 13:14:49,552:INFO:Extra Trees Classifier Imported successfully
2025-05-03 13:14:49,558:INFO:Starting cross validation
2025-05-03 13:14:49,558:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:50,659:INFO:Calculating mean and std
2025-05-03 13:14:50,661:INFO:Creating metrics dataframe
2025-05-03 13:14:50,663:INFO:Uploading results into container
2025-05-03 13:14:50,663:INFO:Uploading model into container now
2025-05-03 13:14:50,663:INFO:_master_model_container: 5
2025-05-03 13:14:50,663:INFO:_display_container: 2
2025-05-03 13:14:50,665:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 13:14:50,665:INFO:create_model() successfully completed......................................
2025-05-03 13:14:50,770:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:50,770:INFO:Creating metrics dataframe
2025-05-03 13:14:50,776:INFO:Initializing Ridge Classifier
2025-05-03 13:14:50,776:INFO:Total runtime is 0.20822820266087852 minutes
2025-05-03 13:14:50,778:INFO:SubProcess create_model() called ==================================
2025-05-03 13:14:50,780:INFO:Initializing create_model()
2025-05-03 13:14:50,780:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C773F88D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:50,780:INFO:Checking exceptions
2025-05-03 13:14:50,780:INFO:Importing libraries
2025-05-03 13:14:50,780:INFO:Copying training dataset
2025-05-03 13:14:50,794:INFO:Defining folds
2025-05-03 13:14:50,795:INFO:Declaring metric variables
2025-05-03 13:14:50,799:INFO:Importing untrained model
2025-05-03 13:14:50,801:INFO:Ridge Classifier Imported successfully
2025-05-03 13:14:50,807:INFO:Starting cross validation
2025-05-03 13:14:50,807:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:50,864:INFO:Calculating mean and std
2025-05-03 13:14:50,864:INFO:Creating metrics dataframe
2025-05-03 13:14:50,865:INFO:Uploading results into container
2025-05-03 13:14:50,865:INFO:Uploading model into container now
2025-05-03 13:14:50,866:INFO:_master_model_container: 6
2025-05-03 13:14:50,866:INFO:_display_container: 2
2025-05-03 13:14:50,866:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 13:14:50,866:INFO:create_model() successfully completed......................................
2025-05-03 13:14:50,966:INFO:SubProcess create_model() end ==================================
2025-05-03 13:14:50,966:INFO:Creating metrics dataframe
2025-05-03 13:14:50,978:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 13:14:50,985:INFO:Initializing create_model()
2025-05-03 13:14:50,985:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:50,985:INFO:Checking exceptions
2025-05-03 13:14:50,987:INFO:Importing libraries
2025-05-03 13:14:50,987:INFO:Copying training dataset
2025-05-03 13:14:51,001:INFO:Defining folds
2025-05-03 13:14:51,001:INFO:Declaring metric variables
2025-05-03 13:14:51,001:INFO:Importing untrained model
2025-05-03 13:14:51,001:INFO:Declaring custom model
2025-05-03 13:14:51,002:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:14:51,002:INFO:Cross validation set to False
2025-05-03 13:14:51,002:INFO:Fitting Model
2025-05-03 13:14:51,021:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:14:51,022:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000834 seconds.
2025-05-03 13:14:51,022:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:14:51,023:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:14:51,023:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:14:51,023:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:14:51,023:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:14:51,023:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:14:51,162:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:14:51,162:INFO:create_model() successfully completed......................................
2025-05-03 13:14:51,312:INFO:_master_model_container: 6
2025-05-03 13:14:51,313:INFO:_display_container: 2
2025-05-03 13:14:51,313:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:14:51,313:INFO:compare_models() successfully completed......................................
2025-05-03 13:14:51,325:INFO:Initializing create_model()
2025-05-03 13:14:51,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:51,325:INFO:Checking exceptions
2025-05-03 13:14:51,338:INFO:Importing libraries
2025-05-03 13:14:51,338:INFO:Copying training dataset
2025-05-03 13:14:51,357:INFO:Defining folds
2025-05-03 13:14:51,357:INFO:Declaring metric variables
2025-05-03 13:14:51,360:INFO:Importing untrained model
2025-05-03 13:14:51,365:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:14:51,371:INFO:Starting cross validation
2025-05-03 13:14:51,371:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:51,839:INFO:Calculating mean and std
2025-05-03 13:14:51,840:INFO:Creating metrics dataframe
2025-05-03 13:14:51,846:INFO:Finalizing model
2025-05-03 13:14:51,864:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:14:51,867:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.
2025-05-03 13:14:51,867:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:14:51,867:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:14:51,867:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:14:51,868:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:14:51,868:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:14:51,868:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:14:51,989:INFO:Uploading results into container
2025-05-03 13:14:51,990:INFO:Uploading model into container now
2025-05-03 13:14:51,999:INFO:_master_model_container: 7
2025-05-03 13:14:51,999:INFO:_display_container: 3
2025-05-03 13:14:52,000:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:14:52,001:INFO:create_model() successfully completed......................................
2025-05-03 13:14:52,138:INFO:Initializing create_model()
2025-05-03 13:14:52,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:52,138:INFO:Checking exceptions
2025-05-03 13:14:52,150:INFO:Importing libraries
2025-05-03 13:14:52,150:INFO:Copying training dataset
2025-05-03 13:14:52,174:INFO:Defining folds
2025-05-03 13:14:52,174:INFO:Declaring metric variables
2025-05-03 13:14:52,203:INFO:Importing untrained model
2025-05-03 13:14:52,209:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:14:52,229:INFO:Starting cross validation
2025-05-03 13:14:52,230:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:52,542:INFO:Calculating mean and std
2025-05-03 13:14:52,542:INFO:Creating metrics dataframe
2025-05-03 13:14:52,547:INFO:Finalizing model
2025-05-03 13:14:52,662:INFO:Uploading results into container
2025-05-03 13:14:52,663:INFO:Uploading model into container now
2025-05-03 13:14:52,671:INFO:_master_model_container: 8
2025-05-03 13:14:52,672:INFO:_display_container: 4
2025-05-03 13:14:52,673:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:14:52,673:INFO:create_model() successfully completed......................................
2025-05-03 13:14:52,813:INFO:Initializing create_model()
2025-05-03 13:14:52,813:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:14:52,813:INFO:Checking exceptions
2025-05-03 13:14:52,826:INFO:Importing libraries
2025-05-03 13:14:52,826:INFO:Copying training dataset
2025-05-03 13:14:52,845:INFO:Defining folds
2025-05-03 13:14:52,845:INFO:Declaring metric variables
2025-05-03 13:14:52,847:INFO:Importing untrained model
2025-05-03 13:14:52,851:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:14:52,857:INFO:Starting cross validation
2025-05-03 13:14:52,858:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:14:54,111:INFO:Calculating mean and std
2025-05-03 13:14:54,112:INFO:Creating metrics dataframe
2025-05-03 13:14:54,116:INFO:Finalizing model
2025-05-03 13:14:54,702:INFO:Uploading results into container
2025-05-03 13:14:54,704:INFO:Uploading model into container now
2025-05-03 13:14:54,712:INFO:_master_model_container: 9
2025-05-03 13:14:54,713:INFO:_display_container: 5
2025-05-03 13:14:54,713:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:14:54,713:INFO:create_model() successfully completed......................................
2025-05-03 13:14:54,829:INFO:Initializing tune_model()
2025-05-03 13:14:54,830:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 13:14:54,830:INFO:Checking exceptions
2025-05-03 13:14:54,830:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 13:14:54,850:INFO:Copying training dataset
2025-05-03 13:14:54,862:INFO:Checking base model
2025-05-03 13:14:54,862:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 13:14:54,866:INFO:Declaring metric variables
2025-05-03 13:14:54,870:INFO:Defining Hyperparameters
2025-05-03 13:14:54,974:INFO:Tuning with n_jobs=-1
2025-05-03 13:14:54,979:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:16:32,169:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 13:16:32,170:INFO:Hyperparameter search completed
2025-05-03 13:16:32,170:INFO:SubProcess create_model() called ==================================
2025-05-03 13:16:32,171:INFO:Initializing create_model()
2025-05-03 13:16:32,171:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C772306D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 13:16:32,171:INFO:Checking exceptions
2025-05-03 13:16:32,171:INFO:Importing libraries
2025-05-03 13:16:32,171:INFO:Copying training dataset
2025-05-03 13:16:32,186:INFO:Defining folds
2025-05-03 13:16:32,187:INFO:Declaring metric variables
2025-05-03 13:16:32,190:INFO:Importing untrained model
2025-05-03 13:16:32,190:INFO:Declaring custom model
2025-05-03 13:16:32,194:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:16:32,200:INFO:Starting cross validation
2025-05-03 13:16:32,201:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:16:32,591:INFO:Calculating mean and std
2025-05-03 13:16:32,592:INFO:Creating metrics dataframe
2025-05-03 13:16:32,599:INFO:Finalizing model
2025-05-03 13:16:32,604:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:16:32,604:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:16:32,604:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:16:32,618:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:16:32,618:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:16:32,618:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:16:32,619:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:16:32,621:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.
2025-05-03 13:16:32,621:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:16:32,621:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:16:32,621:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:16:32,622:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:16:32,622:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:16:32,622:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:16:32,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,730:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,737:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,741:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,742:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,745:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,747:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,751:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,753:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,755:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,756:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,757:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,758:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,758:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,758:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,763:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,763:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,766:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,767:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,767:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,771:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,772:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,773:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,773:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,774:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,778:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,779:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,779:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,779:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,781:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,784:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,785:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,785:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,786:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,787:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,787:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,788:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,788:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,789:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,791:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,792:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:16:32,792:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:16:32,804:INFO:Uploading results into container
2025-05-03 13:16:32,805:INFO:Uploading model into container now
2025-05-03 13:16:32,805:INFO:_master_model_container: 10
2025-05-03 13:16:32,805:INFO:_display_container: 6
2025-05-03 13:16:32,807:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:16:32,807:INFO:create_model() successfully completed......................................
2025-05-03 13:16:32,965:INFO:SubProcess create_model() end ==================================
2025-05-03 13:16:32,965:INFO:choose_better activated
2025-05-03 13:16:32,967:INFO:SubProcess create_model() called ==================================
2025-05-03 13:16:32,968:INFO:Initializing create_model()
2025-05-03 13:16:32,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:16:32,968:INFO:Checking exceptions
2025-05-03 13:16:32,969:INFO:Importing libraries
2025-05-03 13:16:32,969:INFO:Copying training dataset
2025-05-03 13:16:32,984:INFO:Defining folds
2025-05-03 13:16:32,985:INFO:Declaring metric variables
2025-05-03 13:16:32,985:INFO:Importing untrained model
2025-05-03 13:16:32,985:INFO:Declaring custom model
2025-05-03 13:16:32,986:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:16:32,986:INFO:Starting cross validation
2025-05-03 13:16:32,986:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:16:33,457:INFO:Calculating mean and std
2025-05-03 13:16:33,458:INFO:Creating metrics dataframe
2025-05-03 13:16:33,460:INFO:Finalizing model
2025-05-03 13:16:33,480:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:16:33,483:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000881 seconds.
2025-05-03 13:16:33,483:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:16:33,483:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:16:33,483:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:16:33,483:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:16:33,484:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:16:33,484:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:16:33,610:INFO:Uploading results into container
2025-05-03 13:16:33,611:INFO:Uploading model into container now
2025-05-03 13:16:33,611:INFO:_master_model_container: 11
2025-05-03 13:16:33,611:INFO:_display_container: 7
2025-05-03 13:16:33,612:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:16:33,612:INFO:create_model() successfully completed......................................
2025-05-03 13:16:33,746:INFO:SubProcess create_model() end ==================================
2025-05-03 13:16:33,747:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 13:16:33,747:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 13:16:33,747:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 13:16:33,747:INFO:choose_better completed
2025-05-03 13:16:33,755:INFO:_master_model_container: 11
2025-05-03 13:16:33,755:INFO:_display_container: 6
2025-05-03 13:16:33,756:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:16:33,756:INFO:tune_model() successfully completed......................................
2025-05-03 13:16:33,872:INFO:Initializing tune_model()
2025-05-03 13:16:33,872:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 13:16:33,873:INFO:Checking exceptions
2025-05-03 13:16:33,873:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 13:16:33,893:INFO:Copying training dataset
2025-05-03 13:16:33,905:INFO:Checking base model
2025-05-03 13:16:33,905:INFO:Base model : Extreme Gradient Boosting
2025-05-03 13:16:33,908:INFO:Declaring metric variables
2025-05-03 13:16:33,912:INFO:Defining Hyperparameters
2025-05-03 13:16:34,017:INFO:Tuning with n_jobs=-1
2025-05-03 13:16:34,021:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:18:00,296:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 13:18:00,297:INFO:Hyperparameter search completed
2025-05-03 13:18:00,297:INFO:SubProcess create_model() called ==================================
2025-05-03 13:18:00,299:INFO:Initializing create_model()
2025-05-03 13:18:00,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029C772306D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 13:18:00,299:INFO:Checking exceptions
2025-05-03 13:18:00,299:INFO:Importing libraries
2025-05-03 13:18:00,299:INFO:Copying training dataset
2025-05-03 13:18:00,326:INFO:Defining folds
2025-05-03 13:18:00,326:INFO:Declaring metric variables
2025-05-03 13:18:00,331:INFO:Importing untrained model
2025-05-03 13:18:00,331:INFO:Declaring custom model
2025-05-03 13:18:00,339:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:18:00,359:INFO:Starting cross validation
2025-05-03 13:18:00,360:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:18:00,787:INFO:Calculating mean and std
2025-05-03 13:18:00,788:INFO:Creating metrics dataframe
2025-05-03 13:18:00,791:INFO:Finalizing model
2025-05-03 13:18:00,939:INFO:Uploading results into container
2025-05-03 13:18:00,939:INFO:Uploading model into container now
2025-05-03 13:18:00,940:INFO:_master_model_container: 12
2025-05-03 13:18:00,940:INFO:_display_container: 7
2025-05-03 13:18:00,941:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:18:00,941:INFO:create_model() successfully completed......................................
2025-05-03 13:18:01,077:INFO:SubProcess create_model() end ==================================
2025-05-03 13:18:01,077:INFO:choose_better activated
2025-05-03 13:18:01,079:INFO:SubProcess create_model() called ==================================
2025-05-03 13:18:01,080:INFO:Initializing create_model()
2025-05-03 13:18:01,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:18:01,080:INFO:Checking exceptions
2025-05-03 13:18:01,082:INFO:Importing libraries
2025-05-03 13:18:01,082:INFO:Copying training dataset
2025-05-03 13:18:01,095:INFO:Defining folds
2025-05-03 13:18:01,096:INFO:Declaring metric variables
2025-05-03 13:18:01,096:INFO:Importing untrained model
2025-05-03 13:18:01,096:INFO:Declaring custom model
2025-05-03 13:18:01,097:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:18:01,097:INFO:Starting cross validation
2025-05-03 13:18:01,097:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:18:01,382:INFO:Calculating mean and std
2025-05-03 13:18:01,382:INFO:Creating metrics dataframe
2025-05-03 13:18:01,385:INFO:Finalizing model
2025-05-03 13:18:01,652:INFO:Uploading results into container
2025-05-03 13:18:01,652:INFO:Uploading model into container now
2025-05-03 13:18:01,653:INFO:_master_model_container: 13
2025-05-03 13:18:01,653:INFO:_display_container: 8
2025-05-03 13:18:01,654:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:18:01,654:INFO:create_model() successfully completed......................................
2025-05-03 13:18:01,786:INFO:SubProcess create_model() end ==================================
2025-05-03 13:18:01,787:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 13:18:01,788:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 13:18:01,788:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 13:18:01,788:INFO:choose_better completed
2025-05-03 13:18:01,796:INFO:_master_model_container: 13
2025-05-03 13:18:01,796:INFO:_display_container: 7
2025-05-03 13:18:01,797:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:18:01,797:INFO:tune_model() successfully completed......................................
2025-05-03 13:18:01,913:INFO:Initializing tune_model()
2025-05-03 13:18:01,913:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C7454CB10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 13:18:01,913:INFO:Checking exceptions
2025-05-03 13:18:01,913:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 13:18:01,932:INFO:Copying training dataset
2025-05-03 13:18:01,943:INFO:Checking base model
2025-05-03 13:18:01,943:INFO:Base model : Random Forest Classifier
2025-05-03 13:18:01,946:INFO:Declaring metric variables
2025-05-03 13:18:01,949:INFO:Defining Hyperparameters
2025-05-03 13:18:02,053:INFO:Tuning with n_jobs=-1
2025-05-03 13:18:02,058:INFO:Initializing skopt.BayesSearchCV
2025-05-03 13:21:20,749:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:21:20,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:21:20,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:21:20,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:21:24,002:INFO:PyCaret ClassificationExperiment
2025-05-03 13:21:24,002:INFO:Logging name: clf-default-name
2025-05-03 13:21:24,002:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 13:21:24,002:INFO:version 3.3.2
2025-05-03 13:21:24,002:INFO:Initializing setup()
2025-05-03 13:21:24,002:INFO:self.USI: d1ee
2025-05-03 13:21:24,002:INFO:self._variable_keys: {'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'X', 'memory', 'seed', 'X_train', 'fix_imbalance', 'idx', 'data', 'fold_generator', 'log_plots_param', 'USI', 'logging_param', '_ml_usecase', 'target_param', 'html_param', 'pipeline', 'fold_groups_param', 'y_test', 'fold_shuffle_param', 'y', 'X_test', 'y_train', 'gpu_param', 'exp_name_log', '_available_plots', 'exp_id'}
2025-05-03 13:21:24,002:INFO:Checking environment
2025-05-03 13:21:24,002:INFO:python_version: 3.11.11
2025-05-03 13:21:24,002:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 13:21:24,002:INFO:machine: AMD64
2025-05-03 13:21:24,002:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 13:21:24,007:INFO:Memory: svmem(total=16965230592, available=4104654848, percent=75.8, used=12860575744, free=4104654848)
2025-05-03 13:21:24,008:INFO:Physical Core: 4
2025-05-03 13:21:24,008:INFO:Logical Core: 8
2025-05-03 13:21:24,008:INFO:Checking libraries
2025-05-03 13:21:24,008:INFO:System:
2025-05-03 13:21:24,008:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 13:21:24,008:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 13:21:24,008:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 13:21:24,008:INFO:PyCaret required dependencies:
2025-05-03 13:21:24,009:INFO:                 pip: 25.0
2025-05-03 13:21:24,009:INFO:          setuptools: 75.8.0
2025-05-03 13:21:24,009:INFO:             pycaret: 3.3.2
2025-05-03 13:21:24,010:INFO:             IPython: 8.32.0
2025-05-03 13:21:24,010:INFO:          ipywidgets: 8.1.6
2025-05-03 13:21:24,010:INFO:                tqdm: 4.67.1
2025-05-03 13:21:24,010:INFO:               numpy: 1.26.4
2025-05-03 13:21:24,010:INFO:              pandas: 2.1.4
2025-05-03 13:21:24,010:INFO:              jinja2: 3.1.6
2025-05-03 13:21:24,010:INFO:               scipy: 1.11.4
2025-05-03 13:21:24,010:INFO:              joblib: 1.3.2
2025-05-03 13:21:24,010:INFO:             sklearn: 1.4.2
2025-05-03 13:21:24,010:INFO:                pyod: 2.0.5
2025-05-03 13:21:24,010:INFO:            imblearn: 0.13.0
2025-05-03 13:21:24,010:INFO:   category_encoders: 2.7.0
2025-05-03 13:21:24,010:INFO:            lightgbm: 4.6.0
2025-05-03 13:21:24,010:INFO:               numba: 0.61.0
2025-05-03 13:21:24,010:INFO:            requests: 2.32.3
2025-05-03 13:21:24,010:INFO:          matplotlib: 3.7.5
2025-05-03 13:21:24,010:INFO:          scikitplot: 0.3.7
2025-05-03 13:21:24,010:INFO:         yellowbrick: 1.5
2025-05-03 13:21:24,010:INFO:              plotly: 5.24.1
2025-05-03 13:21:24,010:INFO:    plotly-resampler: Not installed
2025-05-03 13:21:24,010:INFO:             kaleido: 0.2.1
2025-05-03 13:21:24,010:INFO:           schemdraw: 0.15
2025-05-03 13:21:24,010:INFO:         statsmodels: 0.14.4
2025-05-03 13:21:24,010:INFO:              sktime: 0.26.0
2025-05-03 13:21:24,010:INFO:               tbats: 1.1.3
2025-05-03 13:21:24,010:INFO:            pmdarima: 2.0.4
2025-05-03 13:21:24,010:INFO:              psutil: 6.1.1
2025-05-03 13:21:24,010:INFO:          markupsafe: 3.0.2
2025-05-03 13:21:24,010:INFO:             pickle5: Not installed
2025-05-03 13:21:24,010:INFO:         cloudpickle: 3.1.1
2025-05-03 13:21:24,010:INFO:         deprecation: 2.1.0
2025-05-03 13:21:24,010:INFO:              xxhash: 3.5.0
2025-05-03 13:21:24,010:INFO:           wurlitzer: Not installed
2025-05-03 13:21:24,010:INFO:PyCaret optional dependencies:
2025-05-03 13:21:24,388:INFO:                shap: 0.47.2
2025-05-03 13:21:24,388:INFO:           interpret: 0.6.10
2025-05-03 13:21:24,388:INFO:                umap: Not installed
2025-05-03 13:21:24,388:INFO:     ydata_profiling: Not installed
2025-05-03 13:21:24,388:INFO:  explainerdashboard: Not installed
2025-05-03 13:21:24,388:INFO:             autoviz: Not installed
2025-05-03 13:21:24,388:INFO:           fairlearn: Not installed
2025-05-03 13:21:24,389:INFO:          deepchecks: Not installed
2025-05-03 13:21:24,389:INFO:             xgboost: 3.0.0
2025-05-03 13:21:24,389:INFO:            catboost: Not installed
2025-05-03 13:21:24,389:INFO:              kmodes: Not installed
2025-05-03 13:21:24,389:INFO:             mlxtend: Not installed
2025-05-03 13:21:24,389:INFO:       statsforecast: Not installed
2025-05-03 13:21:24,389:INFO:        tune_sklearn: Not installed
2025-05-03 13:21:24,389:INFO:                 ray: Not installed
2025-05-03 13:21:24,389:INFO:            hyperopt: 0.2.7
2025-05-03 13:21:24,389:INFO:              optuna: Not installed
2025-05-03 13:21:24,389:INFO:               skopt: 0.10.2
2025-05-03 13:21:24,389:INFO:              mlflow: 2.22.0
2025-05-03 13:21:24,389:INFO:              gradio: Not installed
2025-05-03 13:21:24,389:INFO:             fastapi: 0.115.12
2025-05-03 13:21:24,389:INFO:             uvicorn: 0.34.2
2025-05-03 13:21:24,389:INFO:              m2cgen: Not installed
2025-05-03 13:21:24,389:INFO:           evidently: Not installed
2025-05-03 13:21:24,389:INFO:               fugue: Not installed
2025-05-03 13:21:24,390:INFO:           streamlit: Not installed
2025-05-03 13:21:24,390:INFO:             prophet: Not installed
2025-05-03 13:21:24,390:INFO:None
2025-05-03 13:21:24,390:INFO:Set up data.
2025-05-03 13:21:24,404:INFO:Set up folding strategy.
2025-05-03 13:21:24,404:INFO:Set up train/test split.
2025-05-03 13:21:24,424:INFO:Set up index.
2025-05-03 13:21:24,426:INFO:Assigning column types.
2025-05-03 13:21:24,438:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 13:21:24,476:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,479:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,506:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,508:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,545:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,546:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,566:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,568:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,569:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 13:21:24,604:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,625:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,665:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:21:24,688:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,691:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,691:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 13:21:24,752:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,754:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,816:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:24,832:INFO:Finished creating preprocessing pipeline.
2025-05-03 13:21:24,833:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('placeholder', None)], verbose=False)
2025-05-03 13:21:24,833:INFO:Creating final display dataframe.
2025-05-03 13:21:24,900:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 13:21:24,968:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:24,972:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:25,045:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:21:25,047:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:21:25,049:INFO:setup() successfully completed in 1.05s...............
2025-05-03 13:21:25,062:INFO:Initializing compare_models()
2025-05-03 13:21:25,063:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 13:21:25,063:INFO:Checking exceptions
2025-05-03 13:21:25,078:INFO:Preparing display monitor
2025-05-03 13:21:25,102:INFO:Initializing Logistic Regression
2025-05-03 13:21:25,102:INFO:Total runtime is 0.0 minutes
2025-05-03 13:21:25,106:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:25,107:INFO:Initializing create_model()
2025-05-03 13:21:25,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:25,107:INFO:Checking exceptions
2025-05-03 13:21:25,107:INFO:Importing libraries
2025-05-03 13:21:25,107:INFO:Copying training dataset
2025-05-03 13:21:25,124:INFO:Defining folds
2025-05-03 13:21:25,124:INFO:Declaring metric variables
2025-05-03 13:21:25,128:INFO:Importing untrained model
2025-05-03 13:21:25,132:INFO:Logistic Regression Imported successfully
2025-05-03 13:21:25,139:INFO:Starting cross validation
2025-05-03 13:21:25,140:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:28,701:INFO:Calculating mean and std
2025-05-03 13:21:28,702:INFO:Creating metrics dataframe
2025-05-03 13:21:28,705:INFO:Uploading results into container
2025-05-03 13:21:28,705:INFO:Uploading model into container now
2025-05-03 13:21:28,706:INFO:_master_model_container: 1
2025-05-03 13:21:28,707:INFO:_display_container: 2
2025-05-03 13:21:28,708:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 13:21:28,708:INFO:create_model() successfully completed......................................
2025-05-03 13:21:28,873:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:28,874:INFO:Creating metrics dataframe
2025-05-03 13:21:28,878:INFO:Initializing Random Forest Classifier
2025-05-03 13:21:28,878:INFO:Total runtime is 0.06292429367701212 minutes
2025-05-03 13:21:28,881:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:28,881:INFO:Initializing create_model()
2025-05-03 13:21:28,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:28,882:INFO:Checking exceptions
2025-05-03 13:21:28,882:INFO:Importing libraries
2025-05-03 13:21:28,882:INFO:Copying training dataset
2025-05-03 13:21:28,896:INFO:Defining folds
2025-05-03 13:21:28,897:INFO:Declaring metric variables
2025-05-03 13:21:28,900:INFO:Importing untrained model
2025-05-03 13:21:28,904:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:21:28,909:INFO:Starting cross validation
2025-05-03 13:21:28,909:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:33,070:INFO:Calculating mean and std
2025-05-03 13:21:33,070:INFO:Creating metrics dataframe
2025-05-03 13:21:33,074:INFO:Uploading results into container
2025-05-03 13:21:33,074:INFO:Uploading model into container now
2025-05-03 13:21:33,075:INFO:_master_model_container: 2
2025-05-03 13:21:33,075:INFO:_display_container: 2
2025-05-03 13:21:33,076:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:21:33,076:INFO:create_model() successfully completed......................................
2025-05-03 13:21:33,194:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:33,194:INFO:Creating metrics dataframe
2025-05-03 13:21:33,200:INFO:Initializing Extreme Gradient Boosting
2025-05-03 13:21:33,200:INFO:Total runtime is 0.13496752580006915 minutes
2025-05-03 13:21:33,203:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:33,204:INFO:Initializing create_model()
2025-05-03 13:21:33,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:33,204:INFO:Checking exceptions
2025-05-03 13:21:33,204:INFO:Importing libraries
2025-05-03 13:21:33,204:INFO:Copying training dataset
2025-05-03 13:21:33,224:INFO:Defining folds
2025-05-03 13:21:33,224:INFO:Declaring metric variables
2025-05-03 13:21:33,226:INFO:Importing untrained model
2025-05-03 13:21:33,231:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:21:33,237:INFO:Starting cross validation
2025-05-03 13:21:33,238:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:36,094:INFO:Calculating mean and std
2025-05-03 13:21:36,095:INFO:Creating metrics dataframe
2025-05-03 13:21:36,098:INFO:Uploading results into container
2025-05-03 13:21:36,098:INFO:Uploading model into container now
2025-05-03 13:21:36,099:INFO:_master_model_container: 3
2025-05-03 13:21:36,099:INFO:_display_container: 2
2025-05-03 13:21:36,100:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:21:36,100:INFO:create_model() successfully completed......................................
2025-05-03 13:21:36,219:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:36,219:INFO:Creating metrics dataframe
2025-05-03 13:21:36,225:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 13:21:36,225:INFO:Total runtime is 0.18537620703379312 minutes
2025-05-03 13:21:36,228:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:36,228:INFO:Initializing create_model()
2025-05-03 13:21:36,228:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:36,230:INFO:Checking exceptions
2025-05-03 13:21:36,230:INFO:Importing libraries
2025-05-03 13:21:36,230:INFO:Copying training dataset
2025-05-03 13:21:36,243:INFO:Defining folds
2025-05-03 13:21:36,243:INFO:Declaring metric variables
2025-05-03 13:21:36,248:INFO:Importing untrained model
2025-05-03 13:21:36,252:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:21:36,258:INFO:Starting cross validation
2025-05-03 13:21:36,258:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:36,747:INFO:Calculating mean and std
2025-05-03 13:21:36,748:INFO:Creating metrics dataframe
2025-05-03 13:21:36,750:INFO:Uploading results into container
2025-05-03 13:21:36,751:INFO:Uploading model into container now
2025-05-03 13:21:36,751:INFO:_master_model_container: 4
2025-05-03 13:21:36,752:INFO:_display_container: 2
2025-05-03 13:21:36,753:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:21:36,753:INFO:create_model() successfully completed......................................
2025-05-03 13:21:36,875:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:36,875:INFO:Creating metrics dataframe
2025-05-03 13:21:36,881:INFO:Initializing Extra Trees Classifier
2025-05-03 13:21:36,881:INFO:Total runtime is 0.19631187915802 minutes
2025-05-03 13:21:36,884:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:36,884:INFO:Initializing create_model()
2025-05-03 13:21:36,884:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:36,884:INFO:Checking exceptions
2025-05-03 13:21:36,884:INFO:Importing libraries
2025-05-03 13:21:36,884:INFO:Copying training dataset
2025-05-03 13:21:36,900:INFO:Defining folds
2025-05-03 13:21:36,900:INFO:Declaring metric variables
2025-05-03 13:21:36,902:INFO:Importing untrained model
2025-05-03 13:21:36,905:INFO:Extra Trees Classifier Imported successfully
2025-05-03 13:21:36,911:INFO:Starting cross validation
2025-05-03 13:21:36,912:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:38,079:INFO:Calculating mean and std
2025-05-03 13:21:38,080:INFO:Creating metrics dataframe
2025-05-03 13:21:38,081:INFO:Uploading results into container
2025-05-03 13:21:38,082:INFO:Uploading model into container now
2025-05-03 13:21:38,082:INFO:_master_model_container: 5
2025-05-03 13:21:38,082:INFO:_display_container: 2
2025-05-03 13:21:38,083:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 13:21:38,083:INFO:create_model() successfully completed......................................
2025-05-03 13:21:38,191:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:38,192:INFO:Creating metrics dataframe
2025-05-03 13:21:38,198:INFO:Initializing Ridge Classifier
2025-05-03 13:21:38,198:INFO:Total runtime is 0.21826210021972656 minutes
2025-05-03 13:21:38,200:INFO:SubProcess create_model() called ==================================
2025-05-03 13:21:38,201:INFO:Initializing create_model()
2025-05-03 13:21:38,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D536286750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:38,201:INFO:Checking exceptions
2025-05-03 13:21:38,201:INFO:Importing libraries
2025-05-03 13:21:38,201:INFO:Copying training dataset
2025-05-03 13:21:38,216:INFO:Defining folds
2025-05-03 13:21:38,216:INFO:Declaring metric variables
2025-05-03 13:21:38,218:INFO:Importing untrained model
2025-05-03 13:21:38,222:INFO:Ridge Classifier Imported successfully
2025-05-03 13:21:38,228:INFO:Starting cross validation
2025-05-03 13:21:38,229:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:38,349:INFO:Calculating mean and std
2025-05-03 13:21:38,350:INFO:Creating metrics dataframe
2025-05-03 13:21:38,351:INFO:Uploading results into container
2025-05-03 13:21:38,351:INFO:Uploading model into container now
2025-05-03 13:21:38,352:INFO:_master_model_container: 6
2025-05-03 13:21:38,352:INFO:_display_container: 2
2025-05-03 13:21:38,352:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 13:21:38,352:INFO:create_model() successfully completed......................................
2025-05-03 13:21:38,452:INFO:SubProcess create_model() end ==================================
2025-05-03 13:21:38,452:INFO:Creating metrics dataframe
2025-05-03 13:21:38,462:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 13:21:38,469:INFO:Initializing create_model()
2025-05-03 13:21:38,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:38,469:INFO:Checking exceptions
2025-05-03 13:21:38,471:INFO:Importing libraries
2025-05-03 13:21:38,471:INFO:Copying training dataset
2025-05-03 13:21:38,484:INFO:Defining folds
2025-05-03 13:21:38,484:INFO:Declaring metric variables
2025-05-03 13:21:38,484:INFO:Importing untrained model
2025-05-03 13:21:38,484:INFO:Declaring custom model
2025-05-03 13:21:38,485:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:21:38,485:INFO:Cross validation set to False
2025-05-03 13:21:38,485:INFO:Fitting Model
2025-05-03 13:21:38,501:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:21:38,503:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000641 seconds.
2025-05-03 13:21:38,503:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:21:38,503:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:21:38,503:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:21:38,503:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:21:38,504:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:21:38,504:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:21:38,615:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:21:38,615:INFO:create_model() successfully completed......................................
2025-05-03 13:21:38,764:INFO:_master_model_container: 6
2025-05-03 13:21:38,764:INFO:_display_container: 2
2025-05-03 13:21:38,765:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:21:38,765:INFO:compare_models() successfully completed......................................
2025-05-03 13:21:38,777:INFO:Initializing create_model()
2025-05-03 13:21:38,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:38,777:INFO:Checking exceptions
2025-05-03 13:21:38,793:INFO:Importing libraries
2025-05-03 13:21:38,793:INFO:Copying training dataset
2025-05-03 13:21:38,811:INFO:Defining folds
2025-05-03 13:21:38,811:INFO:Declaring metric variables
2025-05-03 13:21:38,814:INFO:Importing untrained model
2025-05-03 13:21:38,819:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:21:38,827:INFO:Starting cross validation
2025-05-03 13:21:38,828:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:39,335:INFO:Calculating mean and std
2025-05-03 13:21:39,336:INFO:Creating metrics dataframe
2025-05-03 13:21:39,342:INFO:Finalizing model
2025-05-03 13:21:39,364:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:21:39,367:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.
2025-05-03 13:21:39,367:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:21:39,367:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:21:39,367:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:21:39,367:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:21:39,367:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:21:39,367:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:21:39,491:INFO:Uploading results into container
2025-05-03 13:21:39,492:INFO:Uploading model into container now
2025-05-03 13:21:39,501:INFO:_master_model_container: 7
2025-05-03 13:21:39,501:INFO:_display_container: 3
2025-05-03 13:21:39,503:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:21:39,504:INFO:create_model() successfully completed......................................
2025-05-03 13:21:39,643:INFO:Initializing create_model()
2025-05-03 13:21:39,643:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:39,643:INFO:Checking exceptions
2025-05-03 13:21:39,654:INFO:Importing libraries
2025-05-03 13:21:39,654:INFO:Copying training dataset
2025-05-03 13:21:39,672:INFO:Defining folds
2025-05-03 13:21:39,673:INFO:Declaring metric variables
2025-05-03 13:21:39,676:INFO:Importing untrained model
2025-05-03 13:21:39,680:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:21:39,686:INFO:Starting cross validation
2025-05-03 13:21:39,686:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:39,993:INFO:Calculating mean and std
2025-05-03 13:21:39,993:INFO:Creating metrics dataframe
2025-05-03 13:21:39,997:INFO:Finalizing model
2025-05-03 13:21:40,122:INFO:Uploading results into container
2025-05-03 13:21:40,123:INFO:Uploading model into container now
2025-05-03 13:21:40,133:INFO:_master_model_container: 8
2025-05-03 13:21:40,133:INFO:_display_container: 4
2025-05-03 13:21:40,135:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:21:40,135:INFO:create_model() successfully completed......................................
2025-05-03 13:21:40,298:INFO:Initializing create_model()
2025-05-03 13:21:40,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:21:40,299:INFO:Checking exceptions
2025-05-03 13:21:40,317:INFO:Importing libraries
2025-05-03 13:21:40,317:INFO:Copying training dataset
2025-05-03 13:21:40,351:INFO:Defining folds
2025-05-03 13:21:40,351:INFO:Declaring metric variables
2025-05-03 13:21:40,354:INFO:Importing untrained model
2025-05-03 13:21:40,360:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:21:40,369:INFO:Starting cross validation
2025-05-03 13:21:40,370:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:41,709:INFO:Calculating mean and std
2025-05-03 13:21:41,711:INFO:Creating metrics dataframe
2025-05-03 13:21:41,720:INFO:Finalizing model
2025-05-03 13:21:42,316:INFO:Uploading results into container
2025-05-03 13:21:42,317:INFO:Uploading model into container now
2025-05-03 13:21:42,324:INFO:_master_model_container: 9
2025-05-03 13:21:42,324:INFO:_display_container: 5
2025-05-03 13:21:42,324:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:21:42,325:INFO:create_model() successfully completed......................................
2025-05-03 13:21:42,439:INFO:Initializing create_model()
2025-05-03 13:21:42,440:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 13:21:42,440:INFO:Checking exceptions
2025-05-03 13:21:42,453:INFO:Importing libraries
2025-05-03 13:21:42,453:INFO:Copying training dataset
2025-05-03 13:21:42,476:INFO:Defining folds
2025-05-03 13:21:42,476:INFO:Declaring metric variables
2025-05-03 13:21:42,480:INFO:Importing untrained model
2025-05-03 13:21:42,485:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:21:42,491:INFO:Starting cross validation
2025-05-03 13:21:42,492:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:42,939:INFO:Calculating mean and std
2025-05-03 13:21:42,939:INFO:Creating metrics dataframe
2025-05-03 13:21:42,945:INFO:Finalizing model
2025-05-03 13:21:42,951:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:21:42,951:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:21:42,951:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:21:42,966:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:21:42,967:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:21:42,967:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:21:42,967:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:21:42,969:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.
2025-05-03 13:21:42,969:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:21:42,969:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:21:42,969:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:21:42,970:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:21:42,970:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:21:42,970:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:21:42,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:42,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,069:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,069:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,079:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,080:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,080:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,083:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,092:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,092:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,095:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,095:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,096:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,096:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,100:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,100:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,103:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,103:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,104:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,105:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,105:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,109:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,109:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,112:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,112:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,114:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,115:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,115:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,121:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,123:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,123:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,125:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,127:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,127:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,129:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,129:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,131:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,131:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,133:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,133:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,135:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,137:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,137:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,139:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,142:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:21:43,142:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:21:43,152:INFO:Uploading results into container
2025-05-03 13:21:43,153:INFO:Uploading model into container now
2025-05-03 13:21:43,165:INFO:_master_model_container: 10
2025-05-03 13:21:43,165:INFO:_display_container: 6
2025-05-03 13:21:43,166:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:21:43,167:INFO:create_model() successfully completed......................................
2025-05-03 13:21:43,298:INFO:Initializing create_model()
2025-05-03 13:21:43,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 13:21:43,298:INFO:Checking exceptions
2025-05-03 13:21:43,309:INFO:Importing libraries
2025-05-03 13:21:43,309:INFO:Copying training dataset
2025-05-03 13:21:43,389:INFO:Defining folds
2025-05-03 13:21:43,389:INFO:Declaring metric variables
2025-05-03 13:21:43,392:INFO:Importing untrained model
2025-05-03 13:21:43,405:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:21:43,413:INFO:Starting cross validation
2025-05-03 13:21:43,413:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:43,876:INFO:Calculating mean and std
2025-05-03 13:21:43,877:INFO:Creating metrics dataframe
2025-05-03 13:21:43,880:INFO:Finalizing model
2025-05-03 13:21:44,042:INFO:Uploading results into container
2025-05-03 13:21:44,043:INFO:Uploading model into container now
2025-05-03 13:21:44,052:INFO:_master_model_container: 11
2025-05-03 13:21:44,052:INFO:_display_container: 7
2025-05-03 13:21:44,054:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:21:44,055:INFO:create_model() successfully completed......................................
2025-05-03 13:21:44,184:INFO:Initializing create_model()
2025-05-03 13:21:44,184:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 13:21:44,184:INFO:Checking exceptions
2025-05-03 13:21:44,194:INFO:Importing libraries
2025-05-03 13:21:44,194:INFO:Copying training dataset
2025-05-03 13:21:44,212:INFO:Defining folds
2025-05-03 13:21:44,212:INFO:Declaring metric variables
2025-05-03 13:21:44,216:INFO:Importing untrained model
2025-05-03 13:21:44,219:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:21:44,225:INFO:Starting cross validation
2025-05-03 13:21:44,226:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:21:48,808:INFO:Calculating mean and std
2025-05-03 13:21:48,809:INFO:Creating metrics dataframe
2025-05-03 13:21:48,815:INFO:Finalizing model
2025-05-03 13:21:51,250:INFO:Uploading results into container
2025-05-03 13:21:51,251:INFO:Uploading model into container now
2025-05-03 13:21:51,259:INFO:_master_model_container: 12
2025-05-03 13:21:51,259:INFO:_display_container: 8
2025-05-03 13:21:51,259:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 13:21:51,259:INFO:create_model() successfully completed......................................
2025-05-03 13:21:56,101:INFO:Initializing interpret_model()
2025-05-03 13:21:56,101:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D5359FCC10>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=       age         workclass  fnlwgt     education  educational-num  \
0       42           Private  145175  Some-college               10   
1       52  Self-emp-not-inc  175029          10th                6   
2       34         Local-gov  172664       HS-grad                9   
3       28           Private  125791       HS-grad                9   
4       46           Private   28419     Assoc-voc               11   
...    ...               ...     ...           ...              ...   
39068   36           Private  635913       HS-grad                9   
39069   34           Private  107624  Some-college               10   
39070   28           Private  250135  Some-college               10   
39071   46         State-gov   96652     Assoc-voc               11   
39072   59           Private  176118       Masters               14   

              marital-status         occupation   relationship   race     sex  \
0         Married-civ-spouse  Machine-op-inspct        Husband  White    Male   
1         Married-civ-spouse       Craft-repair        Husband  White    Male   
2         Married-civ-spouse       Craft-repair        Husband  White    Male   
3              Never-married       Adm-clerical  Not-in-family  White  Female   
4              Never-married   Transport-moving  Not-in-family  White    Male   
...                      ...                ...            ...    ...     ...   
39068  Married-spouse-absent      Other-service  Not-in-family  Black    Male   
39069     Married-civ-spouse       Craft-repair        Husband  White    Male   
39070               Divorced    Exec-managerial  Not-in-family  White  Female   
39071              Separated       Adm-clerical      Unmarried  Black  Female   
39072     Married-civ-spouse     Prof-specialty        Husband  White    Male   

       hours-per-week  es_estadounidense  has_capital_gain  has_capital_loss  
0                  40               True             False             False  
1                  35               True             False             False  
2                  40               True             False             False  
3                  40               True             False             False  
4                  50               True             False             False  
...               ...                ...               ...               ...  
39068              40               True             False             False  
39069              50               True             False             False  
39070              40               True             False             False  
39071              40               True             False             False  
39072               7               True             False             False  

[39073 rows x 14 columns], y_new_sample=       income
0           0
1           0
2           0
3           0
4           0
...       ...
39068       0
39069       0
39070       0
39071       0
39072       1

[39073 rows x 1 columns], save=False, kwargs={})
2025-05-03 13:21:56,102:INFO:Checking exceptions
2025-05-03 13:21:56,102:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 13:21:56,123:INFO:plot type: msa
2025-05-03 13:33:45,621:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:33:45,621:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:33:45,621:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:33:45,621:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:34:26,238:INFO:PyCaret ClassificationExperiment
2025-05-03 13:34:26,238:INFO:Logging name: clf-default-name
2025-05-03 13:34:26,238:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 13:34:26,238:INFO:version 3.3.2
2025-05-03 13:34:26,238:INFO:Initializing setup()
2025-05-03 13:34:26,239:INFO:self.USI: c213
2025-05-03 13:34:26,239:INFO:self._variable_keys: {'y_train', 'fix_imbalance', 'html_param', 'y_test', 'fold_generator', 'target_param', 'fold_shuffle_param', '_ml_usecase', 'exp_name_log', 'memory', 'X', 'gpu_n_jobs_param', 'log_plots_param', 'logging_param', 'seed', 'exp_id', 'y', 'X_train', 'pipeline', 'n_jobs_param', 'idx', 'X_test', 'fold_groups_param', 'is_multiclass', '_available_plots', 'data', 'USI', 'gpu_param'}
2025-05-03 13:34:26,239:INFO:Checking environment
2025-05-03 13:34:26,239:INFO:python_version: 3.11.11
2025-05-03 13:34:26,239:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 13:34:26,239:INFO:machine: AMD64
2025-05-03 13:34:26,239:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 13:34:26,246:INFO:Memory: svmem(total=16965230592, available=3463831552, percent=79.6, used=13501399040, free=3463831552)
2025-05-03 13:34:26,246:INFO:Physical Core: 4
2025-05-03 13:34:26,246:INFO:Logical Core: 8
2025-05-03 13:34:26,246:INFO:Checking libraries
2025-05-03 13:34:26,246:INFO:System:
2025-05-03 13:34:26,246:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 13:34:26,246:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 13:34:26,246:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 13:34:26,246:INFO:PyCaret required dependencies:
2025-05-03 13:34:26,248:INFO:                 pip: 25.0
2025-05-03 13:34:26,248:INFO:          setuptools: 75.8.0
2025-05-03 13:34:26,248:INFO:             pycaret: 3.3.2
2025-05-03 13:34:26,248:INFO:             IPython: 8.32.0
2025-05-03 13:34:26,248:INFO:          ipywidgets: 8.1.6
2025-05-03 13:34:26,248:INFO:                tqdm: 4.67.1
2025-05-03 13:34:26,249:INFO:               numpy: 1.26.4
2025-05-03 13:34:26,249:INFO:              pandas: 2.1.4
2025-05-03 13:34:26,249:INFO:              jinja2: 3.1.6
2025-05-03 13:34:26,249:INFO:               scipy: 1.11.4
2025-05-03 13:34:26,249:INFO:              joblib: 1.3.2
2025-05-03 13:34:26,249:INFO:             sklearn: 1.4.2
2025-05-03 13:34:26,249:INFO:                pyod: 2.0.5
2025-05-03 13:34:26,249:INFO:            imblearn: 0.13.0
2025-05-03 13:34:26,249:INFO:   category_encoders: 2.7.0
2025-05-03 13:34:26,249:INFO:            lightgbm: 4.6.0
2025-05-03 13:34:26,249:INFO:               numba: 0.61.0
2025-05-03 13:34:26,249:INFO:            requests: 2.32.3
2025-05-03 13:34:26,249:INFO:          matplotlib: 3.7.5
2025-05-03 13:34:26,249:INFO:          scikitplot: 0.3.7
2025-05-03 13:34:26,249:INFO:         yellowbrick: 1.5
2025-05-03 13:34:26,249:INFO:              plotly: 5.24.1
2025-05-03 13:34:26,249:INFO:    plotly-resampler: Not installed
2025-05-03 13:34:26,249:INFO:             kaleido: 0.2.1
2025-05-03 13:34:26,249:INFO:           schemdraw: 0.15
2025-05-03 13:34:26,249:INFO:         statsmodels: 0.14.4
2025-05-03 13:34:26,249:INFO:              sktime: 0.26.0
2025-05-03 13:34:26,249:INFO:               tbats: 1.1.3
2025-05-03 13:34:26,249:INFO:            pmdarima: 2.0.4
2025-05-03 13:34:26,249:INFO:              psutil: 6.1.1
2025-05-03 13:34:26,249:INFO:          markupsafe: 3.0.2
2025-05-03 13:34:26,249:INFO:             pickle5: Not installed
2025-05-03 13:34:26,249:INFO:         cloudpickle: 3.1.1
2025-05-03 13:34:26,250:INFO:         deprecation: 2.1.0
2025-05-03 13:34:26,250:INFO:              xxhash: 3.5.0
2025-05-03 13:34:26,250:INFO:           wurlitzer: Not installed
2025-05-03 13:34:26,250:INFO:PyCaret optional dependencies:
2025-05-03 13:34:26,531:INFO:                shap: 0.47.2
2025-05-03 13:34:26,531:INFO:           interpret: 0.6.10
2025-05-03 13:34:26,531:INFO:                umap: Not installed
2025-05-03 13:34:26,531:INFO:     ydata_profiling: Not installed
2025-05-03 13:34:26,531:INFO:  explainerdashboard: Not installed
2025-05-03 13:34:26,531:INFO:             autoviz: Not installed
2025-05-03 13:34:26,531:INFO:           fairlearn: Not installed
2025-05-03 13:34:26,531:INFO:          deepchecks: Not installed
2025-05-03 13:34:26,531:INFO:             xgboost: 3.0.0
2025-05-03 13:34:26,532:INFO:            catboost: Not installed
2025-05-03 13:34:26,532:INFO:              kmodes: Not installed
2025-05-03 13:34:26,532:INFO:             mlxtend: Not installed
2025-05-03 13:34:26,532:INFO:       statsforecast: Not installed
2025-05-03 13:34:26,532:INFO:        tune_sklearn: Not installed
2025-05-03 13:34:26,532:INFO:                 ray: Not installed
2025-05-03 13:34:26,532:INFO:            hyperopt: 0.2.7
2025-05-03 13:34:26,532:INFO:              optuna: Not installed
2025-05-03 13:34:26,532:INFO:               skopt: 0.10.2
2025-05-03 13:34:26,532:INFO:              mlflow: 2.22.0
2025-05-03 13:34:26,532:INFO:              gradio: Not installed
2025-05-03 13:34:26,532:INFO:             fastapi: 0.115.12
2025-05-03 13:34:26,532:INFO:             uvicorn: 0.34.2
2025-05-03 13:34:26,532:INFO:              m2cgen: Not installed
2025-05-03 13:34:26,532:INFO:           evidently: Not installed
2025-05-03 13:34:26,532:INFO:               fugue: Not installed
2025-05-03 13:34:26,532:INFO:           streamlit: Not installed
2025-05-03 13:34:26,532:INFO:             prophet: Not installed
2025-05-03 13:34:26,532:INFO:None
2025-05-03 13:34:26,532:INFO:Set up data.
2025-05-03 13:35:30,069:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:35:30,069:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:35:30,069:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:35:30,069:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 13:36:20,359:INFO:PyCaret ClassificationExperiment
2025-05-03 13:36:20,359:INFO:Logging name: clf-default-name
2025-05-03 13:36:20,359:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 13:36:20,359:INFO:version 3.3.2
2025-05-03 13:36:20,359:INFO:Initializing setup()
2025-05-03 13:36:20,359:INFO:self.USI: e6ed
2025-05-03 13:36:20,359:INFO:self._variable_keys: {'y', 'idx', 'log_plots_param', 'fold_groups_param', 'fold_generator', 'gpu_n_jobs_param', 'y_train', 'data', 'gpu_param', 'y_test', 'target_param', 'fix_imbalance', 'X_train', 'exp_id', '_available_plots', 'fold_shuffle_param', 'n_jobs_param', 'logging_param', 'USI', 'exp_name_log', 'is_multiclass', '_ml_usecase', 'memory', 'pipeline', 'html_param', 'X', 'X_test', 'seed'}
2025-05-03 13:36:20,359:INFO:Checking environment
2025-05-03 13:36:20,359:INFO:python_version: 3.11.11
2025-05-03 13:36:20,360:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 13:36:20,360:INFO:machine: AMD64
2025-05-03 13:36:20,360:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 13:36:20,366:INFO:Memory: svmem(total=16965230592, available=3455451136, percent=79.6, used=13509779456, free=3455451136)
2025-05-03 13:36:20,366:INFO:Physical Core: 4
2025-05-03 13:36:20,366:INFO:Logical Core: 8
2025-05-03 13:36:20,366:INFO:Checking libraries
2025-05-03 13:36:20,366:INFO:System:
2025-05-03 13:36:20,366:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 13:36:20,366:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 13:36:20,366:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 13:36:20,366:INFO:PyCaret required dependencies:
2025-05-03 13:36:20,368:INFO:                 pip: 25.0
2025-05-03 13:36:20,368:INFO:          setuptools: 75.8.0
2025-05-03 13:36:20,368:INFO:             pycaret: 3.3.2
2025-05-03 13:36:20,368:INFO:             IPython: 8.32.0
2025-05-03 13:36:20,368:INFO:          ipywidgets: 8.1.6
2025-05-03 13:36:20,368:INFO:                tqdm: 4.67.1
2025-05-03 13:36:20,368:INFO:               numpy: 1.26.4
2025-05-03 13:36:20,368:INFO:              pandas: 2.1.4
2025-05-03 13:36:20,368:INFO:              jinja2: 3.1.6
2025-05-03 13:36:20,368:INFO:               scipy: 1.11.4
2025-05-03 13:36:20,368:INFO:              joblib: 1.3.2
2025-05-03 13:36:20,368:INFO:             sklearn: 1.4.2
2025-05-03 13:36:20,368:INFO:                pyod: 2.0.5
2025-05-03 13:36:20,368:INFO:            imblearn: 0.13.0
2025-05-03 13:36:20,368:INFO:   category_encoders: 2.7.0
2025-05-03 13:36:20,368:INFO:            lightgbm: 4.6.0
2025-05-03 13:36:20,368:INFO:               numba: 0.61.0
2025-05-03 13:36:20,368:INFO:            requests: 2.32.3
2025-05-03 13:36:20,368:INFO:          matplotlib: 3.7.5
2025-05-03 13:36:20,369:INFO:          scikitplot: 0.3.7
2025-05-03 13:36:20,369:INFO:         yellowbrick: 1.5
2025-05-03 13:36:20,369:INFO:              plotly: 5.24.1
2025-05-03 13:36:20,369:INFO:    plotly-resampler: Not installed
2025-05-03 13:36:20,369:INFO:             kaleido: 0.2.1
2025-05-03 13:36:20,369:INFO:           schemdraw: 0.15
2025-05-03 13:36:20,369:INFO:         statsmodels: 0.14.4
2025-05-03 13:36:20,369:INFO:              sktime: 0.26.0
2025-05-03 13:36:20,369:INFO:               tbats: 1.1.3
2025-05-03 13:36:20,369:INFO:            pmdarima: 2.0.4
2025-05-03 13:36:20,369:INFO:              psutil: 6.1.1
2025-05-03 13:36:20,369:INFO:          markupsafe: 3.0.2
2025-05-03 13:36:20,369:INFO:             pickle5: Not installed
2025-05-03 13:36:20,369:INFO:         cloudpickle: 3.1.1
2025-05-03 13:36:20,369:INFO:         deprecation: 2.1.0
2025-05-03 13:36:20,369:INFO:              xxhash: 3.5.0
2025-05-03 13:36:20,369:INFO:           wurlitzer: Not installed
2025-05-03 13:36:20,369:INFO:PyCaret optional dependencies:
2025-05-03 13:36:20,725:INFO:                shap: 0.47.2
2025-05-03 13:36:20,725:INFO:           interpret: 0.6.10
2025-05-03 13:36:20,726:INFO:                umap: Not installed
2025-05-03 13:36:20,726:INFO:     ydata_profiling: Not installed
2025-05-03 13:36:20,726:INFO:  explainerdashboard: Not installed
2025-05-03 13:36:20,726:INFO:             autoviz: Not installed
2025-05-03 13:36:20,726:INFO:           fairlearn: Not installed
2025-05-03 13:36:20,726:INFO:          deepchecks: Not installed
2025-05-03 13:36:20,726:INFO:             xgboost: 3.0.0
2025-05-03 13:36:20,726:INFO:            catboost: Not installed
2025-05-03 13:36:20,726:INFO:              kmodes: Not installed
2025-05-03 13:36:20,726:INFO:             mlxtend: Not installed
2025-05-03 13:36:20,726:INFO:       statsforecast: Not installed
2025-05-03 13:36:20,726:INFO:        tune_sklearn: Not installed
2025-05-03 13:36:20,726:INFO:                 ray: Not installed
2025-05-03 13:36:20,726:INFO:            hyperopt: 0.2.7
2025-05-03 13:36:20,726:INFO:              optuna: Not installed
2025-05-03 13:36:20,726:INFO:               skopt: 0.10.2
2025-05-03 13:36:20,726:INFO:              mlflow: 2.22.0
2025-05-03 13:36:20,726:INFO:              gradio: Not installed
2025-05-03 13:36:20,726:INFO:             fastapi: 0.115.12
2025-05-03 13:36:20,726:INFO:             uvicorn: 0.34.2
2025-05-03 13:36:20,726:INFO:              m2cgen: Not installed
2025-05-03 13:36:20,726:INFO:           evidently: Not installed
2025-05-03 13:36:20,726:INFO:               fugue: Not installed
2025-05-03 13:36:20,726:INFO:           streamlit: Not installed
2025-05-03 13:36:20,726:INFO:             prophet: Not installed
2025-05-03 13:36:20,726:INFO:None
2025-05-03 13:36:20,726:INFO:Set up data.
2025-05-03 13:36:20,737:INFO:Set up folding strategy.
2025-05-03 13:36:20,737:INFO:Set up train/test split.
2025-05-03 13:36:20,755:INFO:Set up index.
2025-05-03 13:36:20,756:INFO:Assigning column types.
2025-05-03 13:36:20,767:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 13:36:20,799:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:36:20,803:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:36:20,825:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:20,827:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:20,862:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 13:36:20,862:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:36:20,883:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:20,886:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:20,886:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 13:36:20,920:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:36:20,941:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:20,943:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:20,979:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 13:36:21,000:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:21,002:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:21,003:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 13:36:21,060:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:21,062:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:21,121:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:21,123:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:21,128:INFO:Set up column name cleaning.
2025-05-03 13:36:21,157:INFO:Finished creating preprocessing pipeline.
2025-05-03 13:36:21,159:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 13:36:21,160:INFO:Creating final display dataframe.
2025-05-03 13:36:21,257:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 13:36:21,314:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:21,316:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:21,372:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 13:36:21,374:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 13:36:21,375:INFO:setup() successfully completed in 1.02s...............
2025-05-03 13:36:34,933:INFO:Initializing compare_models()
2025-05-03 13:36:34,933:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 13:36:34,933:INFO:Checking exceptions
2025-05-03 13:36:34,943:INFO:Preparing display monitor
2025-05-03 13:36:34,968:INFO:Initializing Logistic Regression
2025-05-03 13:36:34,968:INFO:Total runtime is 0.0 minutes
2025-05-03 13:36:34,972:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:34,972:INFO:Initializing create_model()
2025-05-03 13:36:34,974:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:34,974:INFO:Checking exceptions
2025-05-03 13:36:34,974:INFO:Importing libraries
2025-05-03 13:36:34,974:INFO:Copying training dataset
2025-05-03 13:36:34,991:INFO:Defining folds
2025-05-03 13:36:34,991:INFO:Declaring metric variables
2025-05-03 13:36:34,995:INFO:Importing untrained model
2025-05-03 13:36:34,998:INFO:Logistic Regression Imported successfully
2025-05-03 13:36:35,003:INFO:Starting cross validation
2025-05-03 13:36:35,004:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:38,332:INFO:Calculating mean and std
2025-05-03 13:36:38,358:INFO:Creating metrics dataframe
2025-05-03 13:36:38,360:INFO:Uploading results into container
2025-05-03 13:36:38,361:INFO:Uploading model into container now
2025-05-03 13:36:38,361:INFO:_master_model_container: 1
2025-05-03 13:36:38,361:INFO:_display_container: 2
2025-05-03 13:36:38,362:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 13:36:38,362:INFO:create_model() successfully completed......................................
2025-05-03 13:36:38,508:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:38,508:INFO:Creating metrics dataframe
2025-05-03 13:36:38,513:INFO:Initializing Random Forest Classifier
2025-05-03 13:36:38,515:INFO:Total runtime is 0.05910855929056803 minutes
2025-05-03 13:36:38,517:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:38,518:INFO:Initializing create_model()
2025-05-03 13:36:38,518:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:38,518:INFO:Checking exceptions
2025-05-03 13:36:38,518:INFO:Importing libraries
2025-05-03 13:36:38,518:INFO:Copying training dataset
2025-05-03 13:36:38,533:INFO:Defining folds
2025-05-03 13:36:38,533:INFO:Declaring metric variables
2025-05-03 13:36:38,537:INFO:Importing untrained model
2025-05-03 13:36:38,541:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:36:38,547:INFO:Starting cross validation
2025-05-03 13:36:38,548:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:42,394:INFO:Calculating mean and std
2025-05-03 13:36:42,395:INFO:Creating metrics dataframe
2025-05-03 13:36:42,399:INFO:Uploading results into container
2025-05-03 13:36:42,400:INFO:Uploading model into container now
2025-05-03 13:36:42,401:INFO:_master_model_container: 2
2025-05-03 13:36:42,401:INFO:_display_container: 2
2025-05-03 13:36:42,401:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:36:42,401:INFO:create_model() successfully completed......................................
2025-05-03 13:36:42,527:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:42,527:INFO:Creating metrics dataframe
2025-05-03 13:36:42,534:INFO:Initializing Extreme Gradient Boosting
2025-05-03 13:36:42,534:INFO:Total runtime is 0.12609275579452514 minutes
2025-05-03 13:36:42,537:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:42,537:INFO:Initializing create_model()
2025-05-03 13:36:42,538:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:42,538:INFO:Checking exceptions
2025-05-03 13:36:42,538:INFO:Importing libraries
2025-05-03 13:36:42,538:INFO:Copying training dataset
2025-05-03 13:36:42,555:INFO:Defining folds
2025-05-03 13:36:42,555:INFO:Declaring metric variables
2025-05-03 13:36:42,560:INFO:Importing untrained model
2025-05-03 13:36:42,564:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:36:42,572:INFO:Starting cross validation
2025-05-03 13:36:42,573:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:45,538:INFO:Calculating mean and std
2025-05-03 13:36:45,540:INFO:Creating metrics dataframe
2025-05-03 13:36:45,541:INFO:Uploading results into container
2025-05-03 13:36:45,541:INFO:Uploading model into container now
2025-05-03 13:36:45,542:INFO:_master_model_container: 3
2025-05-03 13:36:45,542:INFO:_display_container: 2
2025-05-03 13:36:45,543:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:36:45,543:INFO:create_model() successfully completed......................................
2025-05-03 13:36:45,663:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:45,663:INFO:Creating metrics dataframe
2025-05-03 13:36:45,670:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 13:36:45,670:INFO:Total runtime is 0.17836565176645913 minutes
2025-05-03 13:36:45,673:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:45,673:INFO:Initializing create_model()
2025-05-03 13:36:45,675:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:45,675:INFO:Checking exceptions
2025-05-03 13:36:45,675:INFO:Importing libraries
2025-05-03 13:36:45,675:INFO:Copying training dataset
2025-05-03 13:36:45,691:INFO:Defining folds
2025-05-03 13:36:45,691:INFO:Declaring metric variables
2025-05-03 13:36:45,697:INFO:Importing untrained model
2025-05-03 13:36:45,701:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:36:45,708:INFO:Starting cross validation
2025-05-03 13:36:45,710:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:46,241:INFO:Calculating mean and std
2025-05-03 13:36:46,242:INFO:Creating metrics dataframe
2025-05-03 13:36:46,246:INFO:Uploading results into container
2025-05-03 13:36:46,247:INFO:Uploading model into container now
2025-05-03 13:36:46,247:INFO:_master_model_container: 4
2025-05-03 13:36:46,247:INFO:_display_container: 2
2025-05-03 13:36:46,248:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:36:46,248:INFO:create_model() successfully completed......................................
2025-05-03 13:36:46,378:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:46,378:INFO:Creating metrics dataframe
2025-05-03 13:36:46,384:INFO:Initializing Extra Trees Classifier
2025-05-03 13:36:46,385:INFO:Total runtime is 0.1902647296587626 minutes
2025-05-03 13:36:46,387:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:46,389:INFO:Initializing create_model()
2025-05-03 13:36:46,389:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:46,389:INFO:Checking exceptions
2025-05-03 13:36:46,389:INFO:Importing libraries
2025-05-03 13:36:46,389:INFO:Copying training dataset
2025-05-03 13:36:46,403:INFO:Defining folds
2025-05-03 13:36:46,405:INFO:Declaring metric variables
2025-05-03 13:36:46,408:INFO:Importing untrained model
2025-05-03 13:36:46,412:INFO:Extra Trees Classifier Imported successfully
2025-05-03 13:36:46,421:INFO:Starting cross validation
2025-05-03 13:36:46,421:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:47,686:INFO:Calculating mean and std
2025-05-03 13:36:47,686:INFO:Creating metrics dataframe
2025-05-03 13:36:47,688:INFO:Uploading results into container
2025-05-03 13:36:47,689:INFO:Uploading model into container now
2025-05-03 13:36:47,689:INFO:_master_model_container: 5
2025-05-03 13:36:47,689:INFO:_display_container: 2
2025-05-03 13:36:47,690:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 13:36:47,690:INFO:create_model() successfully completed......................................
2025-05-03 13:36:47,806:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:47,806:INFO:Creating metrics dataframe
2025-05-03 13:36:47,814:INFO:Initializing Ridge Classifier
2025-05-03 13:36:47,814:INFO:Total runtime is 0.21409724553426104 minutes
2025-05-03 13:36:47,818:INFO:SubProcess create_model() called ==================================
2025-05-03 13:36:47,818:INFO:Initializing create_model()
2025-05-03 13:36:47,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C51FBFE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:47,819:INFO:Checking exceptions
2025-05-03 13:36:47,819:INFO:Importing libraries
2025-05-03 13:36:47,819:INFO:Copying training dataset
2025-05-03 13:36:47,834:INFO:Defining folds
2025-05-03 13:36:47,835:INFO:Declaring metric variables
2025-05-03 13:36:47,837:INFO:Importing untrained model
2025-05-03 13:36:47,841:INFO:Ridge Classifier Imported successfully
2025-05-03 13:36:47,849:INFO:Starting cross validation
2025-05-03 13:36:47,850:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:47,928:INFO:Calculating mean and std
2025-05-03 13:36:47,929:INFO:Creating metrics dataframe
2025-05-03 13:36:47,930:INFO:Uploading results into container
2025-05-03 13:36:47,930:INFO:Uploading model into container now
2025-05-03 13:36:47,931:INFO:_master_model_container: 6
2025-05-03 13:36:47,931:INFO:_display_container: 2
2025-05-03 13:36:47,931:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 13:36:47,931:INFO:create_model() successfully completed......................................
2025-05-03 13:36:48,042:INFO:SubProcess create_model() end ==================================
2025-05-03 13:36:48,042:INFO:Creating metrics dataframe
2025-05-03 13:36:48,050:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 13:36:48,058:INFO:Initializing create_model()
2025-05-03 13:36:48,059:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:48,059:INFO:Checking exceptions
2025-05-03 13:36:48,061:INFO:Importing libraries
2025-05-03 13:36:48,061:INFO:Copying training dataset
2025-05-03 13:36:48,076:INFO:Defining folds
2025-05-03 13:36:48,076:INFO:Declaring metric variables
2025-05-03 13:36:48,076:INFO:Importing untrained model
2025-05-03 13:36:48,076:INFO:Declaring custom model
2025-05-03 13:36:48,077:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:36:48,078:INFO:Cross validation set to False
2025-05-03 13:36:48,078:INFO:Fitting Model
2025-05-03 13:36:48,098:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:36:48,101:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.
2025-05-03 13:36:48,101:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:36:48,101:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:36:48,102:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:36:48,102:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:36:48,102:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:36:48,102:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:36:48,264:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:36:48,265:INFO:create_model() successfully completed......................................
2025-05-03 13:36:48,425:INFO:_master_model_container: 6
2025-05-03 13:36:48,425:INFO:_display_container: 2
2025-05-03 13:36:48,426:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:36:48,426:INFO:compare_models() successfully completed......................................
2025-05-03 13:36:48,441:INFO:Initializing create_model()
2025-05-03 13:36:48,441:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:48,441:INFO:Checking exceptions
2025-05-03 13:36:48,458:INFO:Importing libraries
2025-05-03 13:36:48,458:INFO:Copying training dataset
2025-05-03 13:36:48,479:INFO:Defining folds
2025-05-03 13:36:48,479:INFO:Declaring metric variables
2025-05-03 13:36:48,482:INFO:Importing untrained model
2025-05-03 13:36:48,487:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:36:48,496:INFO:Starting cross validation
2025-05-03 13:36:48,496:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:49,102:INFO:Calculating mean and std
2025-05-03 13:36:49,102:INFO:Creating metrics dataframe
2025-05-03 13:36:49,107:INFO:Finalizing model
2025-05-03 13:36:49,135:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:36:49,138:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.
2025-05-03 13:36:49,138:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:36:49,138:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:36:49,138:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:36:49,138:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:36:49,138:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:36:49,138:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:36:49,312:INFO:Uploading results into container
2025-05-03 13:36:49,313:INFO:Uploading model into container now
2025-05-03 13:36:49,323:INFO:_master_model_container: 7
2025-05-03 13:36:49,323:INFO:_display_container: 3
2025-05-03 13:36:49,325:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:36:49,326:INFO:create_model() successfully completed......................................
2025-05-03 13:36:49,482:INFO:Initializing create_model()
2025-05-03 13:36:49,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:49,482:INFO:Checking exceptions
2025-05-03 13:36:49,495:INFO:Importing libraries
2025-05-03 13:36:49,495:INFO:Copying training dataset
2025-05-03 13:36:49,521:INFO:Defining folds
2025-05-03 13:36:49,521:INFO:Declaring metric variables
2025-05-03 13:36:49,527:INFO:Importing untrained model
2025-05-03 13:36:49,532:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:36:49,539:INFO:Starting cross validation
2025-05-03 13:36:49,540:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:49,850:INFO:Calculating mean and std
2025-05-03 13:36:49,850:INFO:Creating metrics dataframe
2025-05-03 13:36:49,854:INFO:Finalizing model
2025-05-03 13:36:50,009:INFO:Uploading results into container
2025-05-03 13:36:50,010:INFO:Uploading model into container now
2025-05-03 13:36:50,022:INFO:_master_model_container: 8
2025-05-03 13:36:50,022:INFO:_display_container: 4
2025-05-03 13:36:50,023:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 13:36:50,023:INFO:create_model() successfully completed......................................
2025-05-03 13:36:50,171:INFO:Initializing create_model()
2025-05-03 13:36:50,172:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 13:36:50,172:INFO:Checking exceptions
2025-05-03 13:36:50,186:INFO:Importing libraries
2025-05-03 13:36:50,186:INFO:Copying training dataset
2025-05-03 13:36:50,208:INFO:Defining folds
2025-05-03 13:36:50,208:INFO:Declaring metric variables
2025-05-03 13:36:50,232:INFO:Importing untrained model
2025-05-03 13:36:50,250:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:36:50,266:INFO:Starting cross validation
2025-05-03 13:36:50,267:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:36:51,869:INFO:Calculating mean and std
2025-05-03 13:36:51,870:INFO:Creating metrics dataframe
2025-05-03 13:36:51,875:INFO:Finalizing model
2025-05-03 13:36:52,550:INFO:Uploading results into container
2025-05-03 13:36:52,551:INFO:Uploading model into container now
2025-05-03 13:36:52,559:INFO:_master_model_container: 9
2025-05-03 13:36:52,560:INFO:_display_container: 5
2025-05-03 13:36:52,560:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 13:36:52,560:INFO:create_model() successfully completed......................................
2025-05-03 13:37:05,508:INFO:Initializing create_model()
2025-05-03 13:37:05,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 13:37:05,509:INFO:Checking exceptions
2025-05-03 13:37:05,520:INFO:Importing libraries
2025-05-03 13:37:05,520:INFO:Copying training dataset
2025-05-03 13:37:05,541:INFO:Defining folds
2025-05-03 13:37:05,541:INFO:Declaring metric variables
2025-05-03 13:37:05,543:INFO:Importing untrained model
2025-05-03 13:37:05,548:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 13:37:05,554:INFO:Starting cross validation
2025-05-03 13:37:05,555:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:37:05,937:INFO:Calculating mean and std
2025-05-03 13:37:05,937:INFO:Creating metrics dataframe
2025-05-03 13:37:05,942:INFO:Finalizing model
2025-05-03 13:37:05,952:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:37:05,953:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:37:05,953:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:37:05,967:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 13:37:05,967:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 13:37:05,967:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 13:37:05,967:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 13:37:05,970:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.
2025-05-03 13:37:05,970:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 13:37:05,970:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 13:37:05,970:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 13:37:05,970:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 13:37:05,970:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 13:37:05,970:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 13:37:05,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:05,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,052:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,052:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,059:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,060:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,061:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,067:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,067:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,071:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,073:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,073:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,074:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,077:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,079:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,080:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,080:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,082:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,082:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,083:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,088:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,088:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,088:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,092:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,092:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,094:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,094:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,095:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,095:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,096:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,100:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,100:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,104:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,104:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,105:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,109:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,109:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,112:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,114:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,115:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,115:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,121:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 13:37:06,121:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 13:37:06,132:INFO:Uploading results into container
2025-05-03 13:37:06,133:INFO:Uploading model into container now
2025-05-03 13:37:06,140:INFO:_master_model_container: 10
2025-05-03 13:37:06,142:INFO:_display_container: 6
2025-05-03 13:37:06,143:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 13:37:06,143:INFO:create_model() successfully completed......................................
2025-05-03 13:37:06,277:INFO:Initializing create_model()
2025-05-03 13:37:06,278:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 13:37:06,278:INFO:Checking exceptions
2025-05-03 13:37:06,288:INFO:Importing libraries
2025-05-03 13:37:06,288:INFO:Copying training dataset
2025-05-03 13:37:06,310:INFO:Defining folds
2025-05-03 13:37:06,310:INFO:Declaring metric variables
2025-05-03 13:37:06,348:INFO:Importing untrained model
2025-05-03 13:37:06,366:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 13:37:06,373:INFO:Starting cross validation
2025-05-03 13:37:06,374:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:37:06,782:INFO:Calculating mean and std
2025-05-03 13:37:06,782:INFO:Creating metrics dataframe
2025-05-03 13:37:06,786:INFO:Finalizing model
2025-05-03 13:37:06,969:INFO:Uploading results into container
2025-05-03 13:37:06,969:INFO:Uploading model into container now
2025-05-03 13:37:06,981:INFO:_master_model_container: 11
2025-05-03 13:37:06,981:INFO:_display_container: 7
2025-05-03 13:37:06,982:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 13:37:06,982:INFO:create_model() successfully completed......................................
2025-05-03 13:37:07,122:INFO:Initializing create_model()
2025-05-03 13:37:07,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 13:37:07,122:INFO:Checking exceptions
2025-05-03 13:37:07,134:INFO:Importing libraries
2025-05-03 13:37:07,134:INFO:Copying training dataset
2025-05-03 13:37:07,154:INFO:Defining folds
2025-05-03 13:37:07,154:INFO:Declaring metric variables
2025-05-03 13:37:07,158:INFO:Importing untrained model
2025-05-03 13:37:07,162:INFO:Random Forest Classifier Imported successfully
2025-05-03 13:37:07,170:INFO:Starting cross validation
2025-05-03 13:37:07,171:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 13:37:11,263:INFO:Calculating mean and std
2025-05-03 13:37:11,265:INFO:Creating metrics dataframe
2025-05-03 13:37:11,268:INFO:Finalizing model
2025-05-03 13:37:13,565:INFO:Uploading results into container
2025-05-03 13:37:13,565:INFO:Uploading model into container now
2025-05-03 13:37:13,575:INFO:_master_model_container: 12
2025-05-03 13:37:13,575:INFO:_display_container: 8
2025-05-03 13:37:13,576:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 13:37:13,576:INFO:create_model() successfully completed......................................
2025-05-03 13:37:51,817:INFO:Initializing interpret_model()
2025-05-03 13:37:51,818:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:37:51,818:INFO:Checking exceptions
2025-05-03 13:37:51,818:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 13:37:51,841:INFO:plot type: msa
2025-05-03 13:37:52,235:INFO:Visual Rendered Successfully
2025-05-03 13:37:52,235:INFO:interpret_model() successfully completed......................................
2025-05-03 13:38:47,927:INFO:Initializing interpret_model()
2025-05-03 13:38:47,927:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=feature, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:38:47,927:INFO:Checking exceptions
2025-05-03 13:39:10,752:INFO:Initializing interpret_model()
2025-05-03 13:39:10,752:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:39:10,752:INFO:Checking exceptions
2025-05-03 13:39:10,752:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 13:39:10,771:INFO:plot type: msa
2025-05-03 13:39:10,996:INFO:Visual Rendered Successfully
2025-05-03 13:39:10,996:INFO:interpret_model() successfully completed......................................
2025-05-03 13:39:18,268:INFO:Initializing interpret_model()
2025-05-03 13:39:18,268:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:39:18,268:INFO:Checking exceptions
2025-05-03 13:39:18,268:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 13:39:18,291:INFO:plot type: msa
2025-05-03 13:39:18,291:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 13:39:18,532:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 13:39:18,624:INFO:Visual Rendered Successfully
2025-05-03 13:39:18,624:INFO:interpret_model() successfully completed......................................
2025-05-03 13:40:52,539:INFO:Initializing interpret_model()
2025-05-03 13:40:52,539:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:40:52,540:INFO:Checking exceptions
2025-05-03 13:40:52,540:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 13:40:52,559:INFO:plot type: summary
2025-05-03 13:40:52,560:INFO:Creating TreeExplainer
2025-05-03 13:40:52,590:INFO:Compiling shap values
2025-05-03 13:48:08,338:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 13:48:08,362:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 13:48:08,469:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 13:48:08,737:INFO:Visual Rendered Successfully
2025-05-03 13:48:08,737:INFO:interpret_model() successfully completed......................................
2025-05-03 13:48:08,936:INFO:Initializing interpret_model()
2025-05-03 13:48:08,936:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:48:08,936:INFO:Checking exceptions
2025-05-03 13:48:08,936:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 13:48:08,956:INFO:plot type: summary
2025-05-03 13:48:08,956:INFO:Creating TreeExplainer
2025-05-03 13:48:08,989:INFO:Compiling shap values
2025-05-03 13:48:09,745:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 13:48:09,746:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 13:48:10,806:INFO:Visual Rendered Successfully
2025-05-03 13:48:10,806:INFO:interpret_model() successfully completed......................................
2025-05-03 13:48:10,958:INFO:Initializing interpret_model()
2025-05-03 13:48:10,958:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 13:48:10,958:INFO:Checking exceptions
2025-05-03 13:48:10,958:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 13:48:10,979:INFO:plot type: summary
2025-05-03 13:48:10,979:INFO:Creating TreeExplainer
2025-05-03 13:48:11,005:INFO:Compiling shap values
2025-05-03 13:48:12,712:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 13:48:13,800:INFO:Visual Rendered Successfully
2025-05-03 13:48:13,800:INFO:interpret_model() successfully completed......................................
2025-05-03 14:02:07,422:INFO:Initializing interpret_model()
2025-05-03 14:02:07,422:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 14:02:07,423:INFO:Checking exceptions
2025-05-03 14:02:07,423:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:02:07,450:INFO:plot type: summary
2025-05-03 14:02:07,450:INFO:Creating TreeExplainer
2025-05-03 14:02:07,474:INFO:Compiling shap values
2025-05-03 14:02:08,889:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:02:09,059:INFO:Visual Rendered Successfully
2025-05-03 14:02:09,059:INFO:interpret_model() successfully completed......................................
2025-05-03 14:02:09,199:INFO:Initializing interpret_model()
2025-05-03 14:02:09,199:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:02:09,199:INFO:Checking exceptions
2025-05-03 14:02:09,199:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:02:09,214:INFO:plot type: summary
2025-05-03 14:02:09,214:INFO:Creating TreeExplainer
2025-05-03 14:02:09,239:INFO:Compiling shap values
2025-05-03 14:02:10,743:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:02:11,835:INFO:Visual Rendered Successfully
2025-05-03 14:02:11,835:INFO:interpret_model() successfully completed......................................
2025-05-03 14:02:39,247:INFO:Initializing interpret_model()
2025-05-03 14:02:39,248:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 14:02:39,248:INFO:Checking exceptions
2025-05-03 14:02:39,248:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:02:39,268:INFO:plot type: summary
2025-05-03 14:02:39,269:INFO:Creating TreeExplainer
2025-05-03 14:02:39,299:INFO:Compiling shap values
2025-05-03 14:02:39,929:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 14:02:39,929:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:02:40,095:INFO:Visual Rendered Successfully
2025-05-03 14:02:40,095:INFO:interpret_model() successfully completed......................................
2025-05-03 14:02:40,238:INFO:Initializing interpret_model()
2025-05-03 14:02:40,238:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:02:40,238:INFO:Checking exceptions
2025-05-03 14:02:40,238:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:02:40,255:INFO:plot type: summary
2025-05-03 14:02:40,255:INFO:Creating TreeExplainer
2025-05-03 14:02:40,286:INFO:Compiling shap values
2025-05-03 14:02:40,978:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 14:02:40,980:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:02:42,089:INFO:Visual Rendered Successfully
2025-05-03 14:02:42,089:INFO:interpret_model() successfully completed......................................
2025-05-03 14:03:18,105:INFO:Initializing interpret_model()
2025-05-03 14:03:18,107:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 14:03:18,107:INFO:Checking exceptions
2025-05-03 14:03:18,107:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:03:18,126:INFO:plot type: summary
2025-05-03 14:03:18,126:INFO:Creating TreeExplainer
2025-05-03 14:03:18,154:INFO:Compiling shap values
2025-05-03 14:10:24,906:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:10:24,923:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:10:25,009:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:10:25,238:INFO:Visual Rendered Successfully
2025-05-03 14:10:25,238:INFO:interpret_model() successfully completed......................................
2025-05-03 14:10:25,374:INFO:Initializing interpret_model()
2025-05-03 14:10:25,374:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:10:25,375:INFO:Checking exceptions
2025-05-03 14:10:25,375:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:10:25,392:INFO:plot type: summary
2025-05-03 14:10:25,392:INFO:Creating TreeExplainer
2025-05-03 14:10:25,421:INFO:Compiling shap values
2025-05-03 14:17:46,243:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:17:46,259:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:17:46,342:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 14:17:46,576:INFO:Visual Rendered Successfully
2025-05-03 14:17:46,576:INFO:interpret_model() successfully completed......................................
2025-05-03 14:27:55,809:INFO:Initializing interpret_model()
2025-05-03 14:27:55,809:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C51DC2E590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
0                          4.0           1.0               0.191912   
1                          4.0           1.0               0.062443   
2                          4.0           1.0               0.160733   
3                          0.0           0.0               0.160733   
4                          0.0           1.0               0.251061   
...                        ...           ...                    ...   
39068                     -1.0           1.0               0.160733   
39069                      4.0           1.0               0.191912   
39070                      2.0           0.0               0.191912   
39071                      1.0           0.0               0.251061   
39072                      4.0           1.0               0.548190   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
0                    0.124794                  0.448484          0.254406   
1                    0.225234                  0.448484          0.254406   
2                    0.225234                  0.448484          0.254406   
3                    0.137670                  0.102740          0.254406   
4                    0.207173                  0.102740          0.254406   
...                       ...                       ...               ...   
39068                0.042973                  0.102740          0.123228   
39069                0.225234                  0.448484          0.254406   
39070                0.476895                  0.102740          0.254406   
39071                0.137670                  0.060844          0.123228   
39072                0.458596                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
0                   0.218236               0.408328                 -0.155278   
1                   0.281607               1.004967                  0.145313   
2                   0.293717              -0.179066                  0.123446   
3                   0.218236              -0.715668                 -0.385638   
4                   0.218236               0.662138                 -2.776625   
...                      ...                    ...                       ...   
39068               0.218236              -0.020498                  1.862292   
39069               0.218236              -0.179066                 -0.636347   
39070               0.218236              -0.715668                  0.719212   
39071               0.269962               0.662138                 -0.809178   
39072               0.218236               1.358948                  0.155282   

       scaler_only__educational-num  scaler_only__hours-per-week  \
0                         -0.029836                    -0.034370   
1                         -1.585248                    -0.435727   
2                         -0.418689                    -0.034370   
3                         -0.418689                    -0.034370   
4                          0.359017                     0.768345   
...                             ...                          ...   
39068                     -0.418689                    -0.034370   
39069                     -0.029836                     0.768345   
39070                     -0.029836                    -0.034370   
39071                      0.359017                    -0.034370   
39072                      1.525576                    -2.683328   

       binary__has_capital_gain  binary__has_capital_loss  \
0                           0.0                       0.0   
1                           0.0                       0.0   
2                           0.0                       0.0   
3                           0.0                       0.0   
4                           0.0                       0.0   
...                         ...                       ...   
39068                       0.0                       0.0   
39069                       0.0                       0.0   
39070                       0.0                       0.0   
39071                       0.0                       0.0   
39072                       0.0                       0.0   

       binary__es_estadounidense  
0                            1.0  
1                            1.0  
2                            1.0  
3                            1.0  
4                            1.0  
...                          ...  
39068                        1.0  
39069                        1.0  
39070                        1.0  
39071                        1.0  
39072                        1.0  

[39073 rows x 14 columns], y_new_sample=       income
0           0
1           0
2           0
3           0
4           0
...       ...
39068       0
39069       0
39070       0
39071       0
39072       1

[39073 rows x 1 columns], save=False, kwargs={'plot_type': 'bar'})
2025-05-03 14:27:55,809:INFO:Checking exceptions
2025-05-03 14:27:55,809:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:27:55,824:INFO:plot type: summary
2025-05-03 14:27:55,824:INFO:Creating TreeExplainer
2025-05-03 14:27:55,856:INFO:Compiling shap values
2025-05-03 14:41:41,888:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:41:41,888:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:41:41,888:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:41:41,888:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:41:45,735:INFO:PyCaret ClassificationExperiment
2025-05-03 14:41:45,735:INFO:Logging name: clf-default-name
2025-05-03 14:41:45,735:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 14:41:45,735:INFO:version 3.3.2
2025-05-03 14:41:45,735:INFO:Initializing setup()
2025-05-03 14:41:45,735:INFO:self.USI: a6c8
2025-05-03 14:41:45,735:INFO:self._variable_keys: {'X', 'fold_groups_param', 'target_param', 'is_multiclass', 'y_test', 'fold_shuffle_param', 'fold_generator', 'seed', 'idx', 'y_train', 'logging_param', 'memory', 'USI', 'html_param', 'gpu_param', '_available_plots', 'gpu_n_jobs_param', 'exp_id', 'pipeline', 'X_test', 'fix_imbalance', 'y', 'log_plots_param', 'data', 'n_jobs_param', '_ml_usecase', 'exp_name_log', 'X_train'}
2025-05-03 14:41:45,735:INFO:Checking environment
2025-05-03 14:41:45,735:INFO:python_version: 3.11.11
2025-05-03 14:41:45,735:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 14:41:45,735:INFO:machine: AMD64
2025-05-03 14:41:45,735:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 14:41:45,735:INFO:Memory: svmem(total=16965230592, available=4377542656, percent=74.2, used=12587687936, free=4377542656)
2025-05-03 14:41:45,735:INFO:Physical Core: 4
2025-05-03 14:41:45,735:INFO:Logical Core: 8
2025-05-03 14:41:45,735:INFO:Checking libraries
2025-05-03 14:41:45,735:INFO:System:
2025-05-03 14:41:45,735:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 14:41:45,735:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 14:41:45,735:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 14:41:45,735:INFO:PyCaret required dependencies:
2025-05-03 14:41:45,735:INFO:                 pip: 25.0
2025-05-03 14:41:45,735:INFO:          setuptools: 75.8.0
2025-05-03 14:41:45,735:INFO:             pycaret: 3.3.2
2025-05-03 14:41:45,735:INFO:             IPython: 8.32.0
2025-05-03 14:41:45,735:INFO:          ipywidgets: 8.1.6
2025-05-03 14:41:45,735:INFO:                tqdm: 4.67.1
2025-05-03 14:41:45,735:INFO:               numpy: 1.26.4
2025-05-03 14:41:45,735:INFO:              pandas: 2.1.4
2025-05-03 14:41:45,735:INFO:              jinja2: 3.1.6
2025-05-03 14:41:45,735:INFO:               scipy: 1.11.4
2025-05-03 14:41:45,735:INFO:              joblib: 1.3.2
2025-05-03 14:41:45,735:INFO:             sklearn: 1.4.2
2025-05-03 14:41:45,735:INFO:                pyod: 2.0.5
2025-05-03 14:41:45,735:INFO:            imblearn: 0.13.0
2025-05-03 14:41:45,735:INFO:   category_encoders: 2.7.0
2025-05-03 14:41:45,735:INFO:            lightgbm: 4.6.0
2025-05-03 14:41:45,735:INFO:               numba: 0.61.0
2025-05-03 14:41:45,735:INFO:            requests: 2.32.3
2025-05-03 14:41:45,735:INFO:          matplotlib: 3.7.5
2025-05-03 14:41:45,735:INFO:          scikitplot: 0.3.7
2025-05-03 14:41:45,735:INFO:         yellowbrick: 1.5
2025-05-03 14:41:45,735:INFO:              plotly: 5.24.1
2025-05-03 14:41:45,735:INFO:    plotly-resampler: Not installed
2025-05-03 14:41:45,735:INFO:             kaleido: 0.2.1
2025-05-03 14:41:45,735:INFO:           schemdraw: 0.15
2025-05-03 14:41:45,735:INFO:         statsmodels: 0.14.4
2025-05-03 14:41:45,735:INFO:              sktime: 0.26.0
2025-05-03 14:41:45,735:INFO:               tbats: 1.1.3
2025-05-03 14:41:45,735:INFO:            pmdarima: 2.0.4
2025-05-03 14:41:45,735:INFO:              psutil: 6.1.1
2025-05-03 14:41:45,735:INFO:          markupsafe: 3.0.2
2025-05-03 14:41:45,735:INFO:             pickle5: Not installed
2025-05-03 14:41:45,735:INFO:         cloudpickle: 3.1.1
2025-05-03 14:41:45,735:INFO:         deprecation: 2.1.0
2025-05-03 14:41:45,735:INFO:              xxhash: 3.5.0
2025-05-03 14:41:45,735:INFO:           wurlitzer: Not installed
2025-05-03 14:41:45,735:INFO:PyCaret optional dependencies:
2025-05-03 14:41:46,210:INFO:                shap: 0.47.2
2025-05-03 14:41:46,210:INFO:           interpret: 0.6.10
2025-05-03 14:41:46,210:INFO:                umap: Not installed
2025-05-03 14:41:46,210:INFO:     ydata_profiling: Not installed
2025-05-03 14:41:46,210:INFO:  explainerdashboard: Not installed
2025-05-03 14:41:46,210:INFO:             autoviz: Not installed
2025-05-03 14:41:46,210:INFO:           fairlearn: Not installed
2025-05-03 14:41:46,210:INFO:          deepchecks: Not installed
2025-05-03 14:41:46,210:INFO:             xgboost: 3.0.0
2025-05-03 14:41:46,210:INFO:            catboost: Not installed
2025-05-03 14:41:46,210:INFO:              kmodes: Not installed
2025-05-03 14:41:46,210:INFO:             mlxtend: Not installed
2025-05-03 14:41:46,210:INFO:       statsforecast: Not installed
2025-05-03 14:41:46,210:INFO:        tune_sklearn: Not installed
2025-05-03 14:41:46,210:INFO:                 ray: Not installed
2025-05-03 14:41:46,210:INFO:            hyperopt: 0.2.7
2025-05-03 14:41:46,210:INFO:              optuna: Not installed
2025-05-03 14:41:46,210:INFO:               skopt: 0.10.2
2025-05-03 14:41:46,210:INFO:              mlflow: 2.22.0
2025-05-03 14:41:46,210:INFO:              gradio: Not installed
2025-05-03 14:41:46,210:INFO:             fastapi: 0.115.12
2025-05-03 14:41:46,210:INFO:             uvicorn: 0.34.2
2025-05-03 14:41:46,210:INFO:              m2cgen: Not installed
2025-05-03 14:41:46,210:INFO:           evidently: Not installed
2025-05-03 14:41:46,210:INFO:               fugue: Not installed
2025-05-03 14:41:46,210:INFO:           streamlit: Not installed
2025-05-03 14:41:46,210:INFO:             prophet: Not installed
2025-05-03 14:41:46,210:INFO:None
2025-05-03 14:41:46,210:INFO:Set up data.
2025-05-03 14:41:46,226:INFO:Set up folding strategy.
2025-05-03 14:41:46,226:INFO:Set up train/test split.
2025-05-03 14:41:46,251:INFO:Set up index.
2025-05-03 14:41:46,251:INFO:Assigning column types.
2025-05-03 14:41:46,267:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 14:41:46,317:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,318:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,351:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,394:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,394:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,427:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,427:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,427:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 14:41:46,475:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,505:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,552:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:41:46,584:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,584:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,584:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 14:41:46,661:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,663:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,751:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,751:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:46,751:INFO:Set up column name cleaning.
2025-05-03 14:41:46,780:INFO:Finished creating preprocessing pipeline.
2025-05-03 14:41:46,784:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 14:41:46,784:INFO:Creating final display dataframe.
2025-05-03 14:41:46,876:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 14:41:46,950:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:46,950:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:47,067:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:41:47,067:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:41:47,067:INFO:setup() successfully completed in 1.35s...............
2025-05-03 14:41:47,113:INFO:Initializing compare_models()
2025-05-03 14:41:47,113:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 14:41:47,113:INFO:Checking exceptions
2025-05-03 14:41:47,134:INFO:Preparing display monitor
2025-05-03 14:41:47,194:INFO:Initializing Logistic Regression
2025-05-03 14:41:47,194:INFO:Total runtime is 3.345807393391927e-05 minutes
2025-05-03 14:41:47,202:INFO:SubProcess create_model() called ==================================
2025-05-03 14:41:47,203:INFO:Initializing create_model()
2025-05-03 14:41:47,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:41:47,203:INFO:Checking exceptions
2025-05-03 14:41:47,203:INFO:Importing libraries
2025-05-03 14:41:47,203:INFO:Copying training dataset
2025-05-03 14:41:47,225:INFO:Defining folds
2025-05-03 14:41:47,225:INFO:Declaring metric variables
2025-05-03 14:41:47,233:INFO:Importing untrained model
2025-05-03 14:41:47,243:INFO:Logistic Regression Imported successfully
2025-05-03 14:41:47,248:INFO:Starting cross validation
2025-05-03 14:41:47,251:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:41:51,193:INFO:Calculating mean and std
2025-05-03 14:41:51,193:INFO:Creating metrics dataframe
2025-05-03 14:41:51,193:INFO:Uploading results into container
2025-05-03 14:41:51,193:INFO:Uploading model into container now
2025-05-03 14:41:51,193:INFO:_master_model_container: 1
2025-05-03 14:41:51,193:INFO:_display_container: 2
2025-05-03 14:41:51,193:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 14:41:51,193:INFO:create_model() successfully completed......................................
2025-05-03 14:41:51,351:INFO:SubProcess create_model() end ==================================
2025-05-03 14:41:51,353:INFO:Creating metrics dataframe
2025-05-03 14:41:51,357:INFO:Initializing Random Forest Classifier
2025-05-03 14:41:51,357:INFO:Total runtime is 0.06941274801890054 minutes
2025-05-03 14:41:51,357:INFO:SubProcess create_model() called ==================================
2025-05-03 14:41:51,362:INFO:Initializing create_model()
2025-05-03 14:41:51,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:41:51,362:INFO:Checking exceptions
2025-05-03 14:41:51,362:INFO:Importing libraries
2025-05-03 14:41:51,362:INFO:Copying training dataset
2025-05-03 14:41:51,378:INFO:Defining folds
2025-05-03 14:41:51,378:INFO:Declaring metric variables
2025-05-03 14:41:51,387:INFO:Importing untrained model
2025-05-03 14:41:51,387:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:41:51,395:INFO:Starting cross validation
2025-05-03 14:41:51,395:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:41:55,812:INFO:Calculating mean and std
2025-05-03 14:41:55,813:INFO:Creating metrics dataframe
2025-05-03 14:41:55,813:INFO:Uploading results into container
2025-05-03 14:41:55,827:INFO:Uploading model into container now
2025-05-03 14:41:55,827:INFO:_master_model_container: 2
2025-05-03 14:41:55,827:INFO:_display_container: 2
2025-05-03 14:41:55,827:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 14:41:55,827:INFO:create_model() successfully completed......................................
2025-05-03 14:41:55,990:INFO:SubProcess create_model() end ==================================
2025-05-03 14:41:55,990:INFO:Creating metrics dataframe
2025-05-03 14:41:55,995:INFO:Initializing Extreme Gradient Boosting
2025-05-03 14:41:55,995:INFO:Total runtime is 0.14670999844868976 minutes
2025-05-03 14:41:56,003:INFO:SubProcess create_model() called ==================================
2025-05-03 14:41:56,003:INFO:Initializing create_model()
2025-05-03 14:41:56,003:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:41:56,003:INFO:Checking exceptions
2025-05-03 14:41:56,003:INFO:Importing libraries
2025-05-03 14:41:56,003:INFO:Copying training dataset
2025-05-03 14:41:56,027:INFO:Defining folds
2025-05-03 14:41:56,027:INFO:Declaring metric variables
2025-05-03 14:41:56,027:INFO:Importing untrained model
2025-05-03 14:41:56,027:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:41:56,050:INFO:Starting cross validation
2025-05-03 14:41:56,050:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:41:59,413:INFO:Calculating mean and std
2025-05-03 14:41:59,413:INFO:Creating metrics dataframe
2025-05-03 14:41:59,416:INFO:Uploading results into container
2025-05-03 14:41:59,418:INFO:Uploading model into container now
2025-05-03 14:41:59,418:INFO:_master_model_container: 3
2025-05-03 14:41:59,418:INFO:_display_container: 2
2025-05-03 14:41:59,418:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:41:59,418:INFO:create_model() successfully completed......................................
2025-05-03 14:41:59,545:INFO:SubProcess create_model() end ==================================
2025-05-03 14:41:59,545:INFO:Creating metrics dataframe
2025-05-03 14:41:59,561:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 14:41:59,561:INFO:Total runtime is 0.20614898204803464 minutes
2025-05-03 14:41:59,561:INFO:SubProcess create_model() called ==================================
2025-05-03 14:41:59,561:INFO:Initializing create_model()
2025-05-03 14:41:59,561:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:41:59,561:INFO:Checking exceptions
2025-05-03 14:41:59,561:INFO:Importing libraries
2025-05-03 14:41:59,561:INFO:Copying training dataset
2025-05-03 14:41:59,592:INFO:Defining folds
2025-05-03 14:41:59,592:INFO:Declaring metric variables
2025-05-03 14:41:59,592:INFO:Importing untrained model
2025-05-03 14:41:59,592:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:41:59,606:INFO:Starting cross validation
2025-05-03 14:41:59,606:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:00,092:INFO:Calculating mean and std
2025-05-03 14:42:00,094:INFO:Creating metrics dataframe
2025-05-03 14:42:00,096:INFO:Uploading results into container
2025-05-03 14:42:00,098:INFO:Uploading model into container now
2025-05-03 14:42:00,098:INFO:_master_model_container: 4
2025-05-03 14:42:00,098:INFO:_display_container: 2
2025-05-03 14:42:00,098:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:42:00,098:INFO:create_model() successfully completed......................................
2025-05-03 14:42:00,241:INFO:SubProcess create_model() end ==================================
2025-05-03 14:42:00,241:INFO:Creating metrics dataframe
2025-05-03 14:42:00,249:INFO:Initializing Extra Trees Classifier
2025-05-03 14:42:00,249:INFO:Total runtime is 0.21761477390925088 minutes
2025-05-03 14:42:00,257:INFO:SubProcess create_model() called ==================================
2025-05-03 14:42:00,257:INFO:Initializing create_model()
2025-05-03 14:42:00,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:00,257:INFO:Checking exceptions
2025-05-03 14:42:00,257:INFO:Importing libraries
2025-05-03 14:42:00,257:INFO:Copying training dataset
2025-05-03 14:42:00,290:INFO:Defining folds
2025-05-03 14:42:00,290:INFO:Declaring metric variables
2025-05-03 14:42:00,305:INFO:Importing untrained model
2025-05-03 14:42:00,306:INFO:Extra Trees Classifier Imported successfully
2025-05-03 14:42:00,321:INFO:Starting cross validation
2025-05-03 14:42:00,321:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:01,441:INFO:Calculating mean and std
2025-05-03 14:42:01,442:INFO:Creating metrics dataframe
2025-05-03 14:42:01,444:INFO:Uploading results into container
2025-05-03 14:42:01,444:INFO:Uploading model into container now
2025-05-03 14:42:01,446:INFO:_master_model_container: 5
2025-05-03 14:42:01,446:INFO:_display_container: 2
2025-05-03 14:42:01,446:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 14:42:01,446:INFO:create_model() successfully completed......................................
2025-05-03 14:42:01,655:INFO:SubProcess create_model() end ==================================
2025-05-03 14:42:01,655:INFO:Creating metrics dataframe
2025-05-03 14:42:01,671:INFO:Initializing Ridge Classifier
2025-05-03 14:42:01,675:INFO:Total runtime is 0.24138945738474527 minutes
2025-05-03 14:42:01,676:INFO:SubProcess create_model() called ==================================
2025-05-03 14:42:01,676:INFO:Initializing create_model()
2025-05-03 14:42:01,676:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA75C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:01,676:INFO:Checking exceptions
2025-05-03 14:42:01,676:INFO:Importing libraries
2025-05-03 14:42:01,676:INFO:Copying training dataset
2025-05-03 14:42:01,691:INFO:Defining folds
2025-05-03 14:42:01,691:INFO:Declaring metric variables
2025-05-03 14:42:01,702:INFO:Importing untrained model
2025-05-03 14:42:01,705:INFO:Ridge Classifier Imported successfully
2025-05-03 14:42:01,707:INFO:Starting cross validation
2025-05-03 14:42:01,707:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:01,793:INFO:Calculating mean and std
2025-05-03 14:42:01,793:INFO:Creating metrics dataframe
2025-05-03 14:42:01,796:INFO:Uploading results into container
2025-05-03 14:42:01,796:INFO:Uploading model into container now
2025-05-03 14:42:01,796:INFO:_master_model_container: 6
2025-05-03 14:42:01,796:INFO:_display_container: 2
2025-05-03 14:42:01,796:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 14:42:01,796:INFO:create_model() successfully completed......................................
2025-05-03 14:42:01,930:INFO:SubProcess create_model() end ==================================
2025-05-03 14:42:01,930:INFO:Creating metrics dataframe
2025-05-03 14:42:01,930:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 14:42:01,996:INFO:Initializing create_model()
2025-05-03 14:42:01,996:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:01,996:INFO:Checking exceptions
2025-05-03 14:42:02,000:INFO:Importing libraries
2025-05-03 14:42:02,000:INFO:Copying training dataset
2025-05-03 14:42:02,025:INFO:Defining folds
2025-05-03 14:42:02,025:INFO:Declaring metric variables
2025-05-03 14:42:02,025:INFO:Importing untrained model
2025-05-03 14:42:02,025:INFO:Declaring custom model
2025-05-03 14:42:02,027:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:42:02,029:INFO:Cross validation set to False
2025-05-03 14:42:02,029:INFO:Fitting Model
2025-05-03 14:42:02,070:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:42:02,075:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.
2025-05-03 14:42:02,075:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:42:02,075:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:42:02,075:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:42:02,075:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:42:02,075:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:42:02,075:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:42:02,293:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:42:02,293:INFO:create_model() successfully completed......................................
2025-05-03 14:42:02,512:INFO:_master_model_container: 6
2025-05-03 14:42:02,512:INFO:_display_container: 2
2025-05-03 14:42:02,512:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:42:02,512:INFO:compare_models() successfully completed......................................
2025-05-03 14:42:02,531:INFO:Initializing create_model()
2025-05-03 14:42:02,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:02,531:INFO:Checking exceptions
2025-05-03 14:42:02,554:INFO:Importing libraries
2025-05-03 14:42:02,554:INFO:Copying training dataset
2025-05-03 14:42:02,576:INFO:Defining folds
2025-05-03 14:42:02,576:INFO:Declaring metric variables
2025-05-03 14:42:02,576:INFO:Importing untrained model
2025-05-03 14:42:02,585:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:42:02,593:INFO:Starting cross validation
2025-05-03 14:42:02,593:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:03,107:INFO:Calculating mean and std
2025-05-03 14:42:03,107:INFO:Creating metrics dataframe
2025-05-03 14:42:03,113:INFO:Finalizing model
2025-05-03 14:42:03,143:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:42:03,147:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.
2025-05-03 14:42:03,147:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:42:03,147:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:42:03,147:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:42:03,147:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:42:03,147:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:42:03,147:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:42:03,331:INFO:Uploading results into container
2025-05-03 14:42:03,332:INFO:Uploading model into container now
2025-05-03 14:42:03,342:INFO:_master_model_container: 7
2025-05-03 14:42:03,342:INFO:_display_container: 3
2025-05-03 14:42:03,344:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:42:03,344:INFO:create_model() successfully completed......................................
2025-05-03 14:42:03,577:INFO:Initializing create_model()
2025-05-03 14:42:03,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:03,577:INFO:Checking exceptions
2025-05-03 14:42:03,603:INFO:Importing libraries
2025-05-03 14:42:03,603:INFO:Copying training dataset
2025-05-03 14:42:03,624:INFO:Defining folds
2025-05-03 14:42:03,624:INFO:Declaring metric variables
2025-05-03 14:42:03,632:INFO:Importing untrained model
2025-05-03 14:42:03,634:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:42:03,640:INFO:Starting cross validation
2025-05-03 14:42:03,640:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:04,010:INFO:Calculating mean and std
2025-05-03 14:42:04,010:INFO:Creating metrics dataframe
2025-05-03 14:42:04,010:INFO:Finalizing model
2025-05-03 14:42:04,147:INFO:Uploading results into container
2025-05-03 14:42:04,147:INFO:Uploading model into container now
2025-05-03 14:42:04,155:INFO:_master_model_container: 8
2025-05-03 14:42:04,155:INFO:_display_container: 4
2025-05-03 14:42:04,159:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:42:04,159:INFO:create_model() successfully completed......................................
2025-05-03 14:42:04,340:INFO:Initializing create_model()
2025-05-03 14:42:04,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:42:04,340:INFO:Checking exceptions
2025-05-03 14:42:04,360:INFO:Importing libraries
2025-05-03 14:42:04,360:INFO:Copying training dataset
2025-05-03 14:42:04,393:INFO:Defining folds
2025-05-03 14:42:04,393:INFO:Declaring metric variables
2025-05-03 14:42:04,397:INFO:Importing untrained model
2025-05-03 14:42:04,397:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:42:04,421:INFO:Starting cross validation
2025-05-03 14:42:04,421:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:42:05,663:INFO:Calculating mean and std
2025-05-03 14:42:05,663:INFO:Creating metrics dataframe
2025-05-03 14:42:05,670:INFO:Finalizing model
2025-05-03 14:42:06,324:INFO:Uploading results into container
2025-05-03 14:42:06,324:INFO:Uploading model into container now
2025-05-03 14:42:06,324:INFO:_master_model_container: 9
2025-05-03 14:42:06,324:INFO:_display_container: 5
2025-05-03 14:42:06,324:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 14:42:06,324:INFO:create_model() successfully completed......................................
2025-05-03 14:42:06,480:INFO:Initializing tune_model()
2025-05-03 14:42:06,480:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:42:06,480:INFO:Checking exceptions
2025-05-03 14:42:06,480:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:42:06,506:INFO:Copying training dataset
2025-05-03 14:42:06,514:INFO:Checking base model
2025-05-03 14:42:06,514:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 14:42:06,524:INFO:Declaring metric variables
2025-05-03 14:42:06,524:INFO:Defining Hyperparameters
2025-05-03 14:42:06,658:INFO:Tuning with n_jobs=-1
2025-05-03 14:42:06,666:INFO:Initializing skopt.BayesSearchCV
2025-05-03 14:44:04,729:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 14:44:04,729:INFO:Hyperparameter search completed
2025-05-03 14:44:04,729:INFO:SubProcess create_model() called ==================================
2025-05-03 14:44:04,729:INFO:Initializing create_model()
2025-05-03 14:44:04,729:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA50C750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 14:44:04,729:INFO:Checking exceptions
2025-05-03 14:44:04,729:INFO:Importing libraries
2025-05-03 14:44:04,729:INFO:Copying training dataset
2025-05-03 14:44:04,751:INFO:Defining folds
2025-05-03 14:44:04,751:INFO:Declaring metric variables
2025-05-03 14:44:04,754:INFO:Importing untrained model
2025-05-03 14:44:04,754:INFO:Declaring custom model
2025-05-03 14:44:04,759:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:44:04,766:INFO:Starting cross validation
2025-05-03 14:44:04,766:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:44:05,147:INFO:Calculating mean and std
2025-05-03 14:44:05,149:INFO:Creating metrics dataframe
2025-05-03 14:44:05,155:INFO:Finalizing model
2025-05-03 14:44:05,168:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:44:05,168:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:44:05,168:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:44:05,183:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:44:05,185:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:44:05,185:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:44:05,185:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:44:05,187:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.
2025-05-03 14:44:05,187:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:44:05,187:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:44:05,187:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:44:05,189:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:44:05,189:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:44:05,189:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:44:05,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,284:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,286:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,288:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,301:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,303:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,317:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,322:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,324:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,334:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,334:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:44:05,334:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:44:05,344:INFO:Uploading results into container
2025-05-03 14:44:05,344:INFO:Uploading model into container now
2025-05-03 14:44:05,344:INFO:_master_model_container: 10
2025-05-03 14:44:05,344:INFO:_display_container: 6
2025-05-03 14:44:05,346:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:44:05,346:INFO:create_model() successfully completed......................................
2025-05-03 14:44:05,495:INFO:SubProcess create_model() end ==================================
2025-05-03 14:44:05,495:INFO:choose_better activated
2025-05-03 14:44:05,497:INFO:SubProcess create_model() called ==================================
2025-05-03 14:44:05,503:INFO:Initializing create_model()
2025-05-03 14:44:05,503:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:44:05,503:INFO:Checking exceptions
2025-05-03 14:44:05,503:INFO:Importing libraries
2025-05-03 14:44:05,503:INFO:Copying training dataset
2025-05-03 14:44:05,537:INFO:Defining folds
2025-05-03 14:44:05,537:INFO:Declaring metric variables
2025-05-03 14:44:05,537:INFO:Importing untrained model
2025-05-03 14:44:05,537:INFO:Declaring custom model
2025-05-03 14:44:05,537:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:44:05,537:INFO:Starting cross validation
2025-05-03 14:44:05,537:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:44:06,022:INFO:Calculating mean and std
2025-05-03 14:44:06,022:INFO:Creating metrics dataframe
2025-05-03 14:44:06,024:INFO:Finalizing model
2025-05-03 14:44:06,052:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:44:06,054:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.
2025-05-03 14:44:06,054:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:44:06,054:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:44:06,054:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:44:06,054:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:44:06,054:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:44:06,054:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:44:06,169:INFO:Uploading results into container
2025-05-03 14:44:06,171:INFO:Uploading model into container now
2025-05-03 14:44:06,171:INFO:_master_model_container: 11
2025-05-03 14:44:06,171:INFO:_display_container: 7
2025-05-03 14:44:06,171:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:44:06,171:INFO:create_model() successfully completed......................................
2025-05-03 14:44:06,319:INFO:SubProcess create_model() end ==================================
2025-05-03 14:44:06,319:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 14:44:06,322:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 14:44:06,322:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 14:44:06,322:INFO:choose_better completed
2025-05-03 14:44:06,336:INFO:_master_model_container: 11
2025-05-03 14:44:06,336:INFO:_display_container: 6
2025-05-03 14:44:06,336:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:44:06,336:INFO:tune_model() successfully completed......................................
2025-05-03 14:44:06,494:INFO:Initializing tune_model()
2025-05-03 14:44:06,494:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:44:06,494:INFO:Checking exceptions
2025-05-03 14:44:06,494:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:44:06,521:INFO:Copying training dataset
2025-05-03 14:44:06,531:INFO:Checking base model
2025-05-03 14:44:06,531:INFO:Base model : Extreme Gradient Boosting
2025-05-03 14:44:06,539:INFO:Declaring metric variables
2025-05-03 14:44:06,547:INFO:Defining Hyperparameters
2025-05-03 14:44:06,671:INFO:Tuning with n_jobs=-1
2025-05-03 14:44:06,671:INFO:Initializing skopt.BayesSearchCV
2025-05-03 14:46:00,167:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 14:46:00,167:INFO:Hyperparameter search completed
2025-05-03 14:46:00,167:INFO:SubProcess create_model() called ==================================
2025-05-03 14:46:00,167:INFO:Initializing create_model()
2025-05-03 14:46:00,167:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FBA1ECC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 14:46:00,167:INFO:Checking exceptions
2025-05-03 14:46:00,167:INFO:Importing libraries
2025-05-03 14:46:00,167:INFO:Copying training dataset
2025-05-03 14:46:00,204:INFO:Defining folds
2025-05-03 14:46:00,206:INFO:Declaring metric variables
2025-05-03 14:46:00,213:INFO:Importing untrained model
2025-05-03 14:46:00,213:INFO:Declaring custom model
2025-05-03 14:46:00,219:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:46:00,242:INFO:Starting cross validation
2025-05-03 14:46:00,242:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:46:00,704:INFO:Calculating mean and std
2025-05-03 14:46:00,704:INFO:Creating metrics dataframe
2025-05-03 14:46:00,709:INFO:Finalizing model
2025-05-03 14:46:00,874:INFO:Uploading results into container
2025-05-03 14:46:00,875:INFO:Uploading model into container now
2025-05-03 14:46:00,877:INFO:_master_model_container: 12
2025-05-03 14:46:00,877:INFO:_display_container: 7
2025-05-03 14:46:00,877:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 14:46:00,877:INFO:create_model() successfully completed......................................
2025-05-03 14:46:01,028:INFO:SubProcess create_model() end ==================================
2025-05-03 14:46:01,028:INFO:choose_better activated
2025-05-03 14:46:01,033:INFO:SubProcess create_model() called ==================================
2025-05-03 14:46:01,033:INFO:Initializing create_model()
2025-05-03 14:46:01,033:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:46:01,033:INFO:Checking exceptions
2025-05-03 14:46:01,039:INFO:Importing libraries
2025-05-03 14:46:01,041:INFO:Copying training dataset
2025-05-03 14:46:01,058:INFO:Defining folds
2025-05-03 14:46:01,064:INFO:Declaring metric variables
2025-05-03 14:46:01,064:INFO:Importing untrained model
2025-05-03 14:46:01,064:INFO:Declaring custom model
2025-05-03 14:46:01,064:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:46:01,064:INFO:Starting cross validation
2025-05-03 14:46:01,066:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:46:01,398:INFO:Calculating mean and std
2025-05-03 14:46:01,398:INFO:Creating metrics dataframe
2025-05-03 14:46:01,400:INFO:Finalizing model
2025-05-03 14:46:01,528:INFO:Uploading results into container
2025-05-03 14:46:01,528:INFO:Uploading model into container now
2025-05-03 14:46:01,530:INFO:_master_model_container: 13
2025-05-03 14:46:01,530:INFO:_display_container: 8
2025-05-03 14:46:01,530:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:46:01,530:INFO:create_model() successfully completed......................................
2025-05-03 14:46:01,674:INFO:SubProcess create_model() end ==================================
2025-05-03 14:46:01,674:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 14:46:01,674:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 14:46:01,674:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 14:46:01,674:INFO:choose_better completed
2025-05-03 14:46:01,691:INFO:_master_model_container: 13
2025-05-03 14:46:01,691:INFO:_display_container: 7
2025-05-03 14:46:01,695:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 14:46:01,695:INFO:tune_model() successfully completed......................................
2025-05-03 14:46:01,853:INFO:Initializing tune_model()
2025-05-03 14:46:01,853:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:46:01,853:INFO:Checking exceptions
2025-05-03 14:46:01,853:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:46:01,883:INFO:Copying training dataset
2025-05-03 14:46:01,899:INFO:Checking base model
2025-05-03 14:46:01,899:INFO:Base model : Random Forest Classifier
2025-05-03 14:46:01,899:INFO:Declaring metric variables
2025-05-03 14:46:01,908:INFO:Defining Hyperparameters
2025-05-03 14:46:02,037:INFO:Tuning with n_jobs=-1
2025-05-03 14:46:02,037:INFO:Initializing skopt.BayesSearchCV
2025-05-03 14:49:53,500:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 14:49:53,500:INFO:Hyperparameter search completed
2025-05-03 14:49:53,500:INFO:SubProcess create_model() called ==================================
2025-05-03 14:49:53,500:INFO:Initializing create_model()
2025-05-03 14:49:53,500:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026FB5A62090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 14:49:53,500:INFO:Checking exceptions
2025-05-03 14:49:53,500:INFO:Importing libraries
2025-05-03 14:49:53,500:INFO:Copying training dataset
2025-05-03 14:49:53,529:INFO:Defining folds
2025-05-03 14:49:53,529:INFO:Declaring metric variables
2025-05-03 14:49:53,533:INFO:Importing untrained model
2025-05-03 14:49:53,533:INFO:Declaring custom model
2025-05-03 14:49:53,537:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:49:53,546:INFO:Starting cross validation
2025-05-03 14:49:53,546:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:49:57,582:INFO:Calculating mean and std
2025-05-03 14:49:57,582:INFO:Creating metrics dataframe
2025-05-03 14:49:57,590:INFO:Finalizing model
2025-05-03 14:50:00,425:INFO:Uploading results into container
2025-05-03 14:50:00,426:INFO:Uploading model into container now
2025-05-03 14:50:00,428:INFO:_master_model_container: 14
2025-05-03 14:50:00,428:INFO:_display_container: 8
2025-05-03 14:50:00,430:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 14:50:00,430:INFO:create_model() successfully completed......................................
2025-05-03 14:50:00,594:INFO:SubProcess create_model() end ==================================
2025-05-03 14:50:00,594:INFO:choose_better activated
2025-05-03 14:50:00,594:INFO:SubProcess create_model() called ==================================
2025-05-03 14:50:00,594:INFO:Initializing create_model()
2025-05-03 14:50:00,594:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:50:00,594:INFO:Checking exceptions
2025-05-03 14:50:00,602:INFO:Importing libraries
2025-05-03 14:50:00,602:INFO:Copying training dataset
2025-05-03 14:50:00,627:INFO:Defining folds
2025-05-03 14:50:00,627:INFO:Declaring metric variables
2025-05-03 14:50:00,627:INFO:Importing untrained model
2025-05-03 14:50:00,627:INFO:Declaring custom model
2025-05-03 14:50:00,627:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:50:00,627:INFO:Starting cross validation
2025-05-03 14:50:00,627:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:50:02,057:INFO:Calculating mean and std
2025-05-03 14:50:02,057:INFO:Creating metrics dataframe
2025-05-03 14:50:02,060:INFO:Finalizing model
2025-05-03 14:50:02,709:INFO:Uploading results into container
2025-05-03 14:50:02,709:INFO:Uploading model into container now
2025-05-03 14:50:02,709:INFO:_master_model_container: 15
2025-05-03 14:50:02,709:INFO:_display_container: 9
2025-05-03 14:50:02,709:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 14:50:02,709:INFO:create_model() successfully completed......................................
2025-05-03 14:50:02,842:INFO:SubProcess create_model() end ==================================
2025-05-03 14:50:02,842:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 14:50:02,842:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 14:50:02,842:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 14:50:02,842:INFO:choose_better completed
2025-05-03 14:50:02,854:INFO:_master_model_container: 15
2025-05-03 14:50:02,858:INFO:_display_container: 8
2025-05-03 14:50:02,859:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 14:50:02,859:INFO:tune_model() successfully completed......................................
2025-05-03 14:50:03,027:INFO:Initializing create_model()
2025-05-03 14:50:03,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 14:50:03,027:INFO:Checking exceptions
2025-05-03 14:50:03,045:INFO:Importing libraries
2025-05-03 14:50:03,045:INFO:Copying training dataset
2025-05-03 14:50:03,069:INFO:Defining folds
2025-05-03 14:50:03,069:INFO:Declaring metric variables
2025-05-03 14:50:03,078:INFO:Importing untrained model
2025-05-03 14:50:03,078:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:50:03,086:INFO:Starting cross validation
2025-05-03 14:50:03,086:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:50:03,514:INFO:Calculating mean and std
2025-05-03 14:50:03,514:INFO:Creating metrics dataframe
2025-05-03 14:50:03,520:INFO:Finalizing model
2025-05-03 14:50:03,534:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:50:03,534:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:50:03,534:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:50:03,551:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:50:03,553:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:50:03,553:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:50:03,553:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:50:03,556:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.
2025-05-03 14:50:03,556:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:50:03,556:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:50:03,556:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:50:03,556:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:50:03,556:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:50:03,556:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:50:03,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,649:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,662:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,668:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,668:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,670:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,673:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,677:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,677:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,677:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,679:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,679:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,680:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,681:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,681:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,683:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,685:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,693:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,693:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,695:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,695:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,699:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,701:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,701:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,708:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,711:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,711:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,727:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,731:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:50:03,733:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:50:03,743:INFO:Uploading results into container
2025-05-03 14:50:03,745:INFO:Uploading model into container now
2025-05-03 14:50:03,754:INFO:_master_model_container: 16
2025-05-03 14:50:03,754:INFO:_display_container: 9
2025-05-03 14:50:03,754:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:50:03,758:INFO:create_model() successfully completed......................................
2025-05-03 14:50:03,899:INFO:Initializing create_model()
2025-05-03 14:50:03,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 14:50:03,901:INFO:Checking exceptions
2025-05-03 14:50:03,925:INFO:Importing libraries
2025-05-03 14:50:03,925:INFO:Copying training dataset
2025-05-03 14:50:03,954:INFO:Defining folds
2025-05-03 14:50:03,954:INFO:Declaring metric variables
2025-05-03 14:50:03,957:INFO:Importing untrained model
2025-05-03 14:50:03,959:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:50:03,965:INFO:Starting cross validation
2025-05-03 14:50:03,969:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:50:04,439:INFO:Calculating mean and std
2025-05-03 14:50:04,439:INFO:Creating metrics dataframe
2025-05-03 14:50:04,445:INFO:Finalizing model
2025-05-03 14:50:04,609:INFO:Uploading results into container
2025-05-03 14:50:04,611:INFO:Uploading model into container now
2025-05-03 14:50:04,619:INFO:_master_model_container: 17
2025-05-03 14:50:04,619:INFO:_display_container: 10
2025-05-03 14:50:04,621:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 14:50:04,621:INFO:create_model() successfully completed......................................
2025-05-03 14:50:04,762:INFO:Initializing create_model()
2025-05-03 14:50:04,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 14:50:04,762:INFO:Checking exceptions
2025-05-03 14:50:04,805:INFO:Importing libraries
2025-05-03 14:50:04,805:INFO:Copying training dataset
2025-05-03 14:50:04,822:INFO:Defining folds
2025-05-03 14:50:04,822:INFO:Declaring metric variables
2025-05-03 14:50:04,829:INFO:Importing untrained model
2025-05-03 14:50:04,834:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:50:04,838:INFO:Starting cross validation
2025-05-03 14:50:04,843:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:50:09,057:INFO:Calculating mean and std
2025-05-03 14:50:09,057:INFO:Creating metrics dataframe
2025-05-03 14:50:09,059:INFO:Finalizing model
2025-05-03 14:50:11,627:INFO:Uploading results into container
2025-05-03 14:50:11,627:INFO:Uploading model into container now
2025-05-03 14:50:11,634:INFO:_master_model_container: 18
2025-05-03 14:50:11,634:INFO:_display_container: 11
2025-05-03 14:50:11,634:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 14:50:11,634:INFO:create_model() successfully completed......................................
2025-05-03 14:50:11,792:INFO:Initializing interpret_model()
2025-05-03 14:50:11,793:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:50:11,793:INFO:Checking exceptions
2025-05-03 14:50:11,793:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 14:50:11,820:INFO:plot type: msa
2025-05-03 14:50:12,360:INFO:Visual Rendered Successfully
2025-05-03 14:50:12,360:INFO:interpret_model() successfully completed......................................
2025-05-03 14:50:12,713:INFO:Initializing interpret_model()
2025-05-03 14:50:12,713:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:50:12,713:INFO:Checking exceptions
2025-05-03 14:50:12,713:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 14:50:12,738:INFO:plot type: msa
2025-05-03 14:50:13,000:INFO:Visual Rendered Successfully
2025-05-03 14:50:13,000:INFO:interpret_model() successfully completed......................................
2025-05-03 14:50:13,259:INFO:Initializing interpret_model()
2025-05-03 14:50:13,259:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 14:50:13,259:INFO:Checking exceptions
2025-05-03 14:50:13,259:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 14:50:13,292:INFO:plot type: msa
2025-05-03 14:50:13,292:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 14:50:13,672:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 14:50:13,791:INFO:Visual Rendered Successfully
2025-05-03 14:50:13,791:INFO:interpret_model() successfully completed......................................
2025-05-03 14:50:13,960:INFO:Initializing save_model()
2025-05-03 14:50:13,960:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 14:50:13,960:INFO:Adding model into prep_pipe
2025-05-03 14:50:13,968:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 14:50:13,974:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split',
                                learning_rate=0.49999999999999994, max_depth=-1,
                                min_child_samples=100, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=197, n_jobs=-1,
                                num_leaves=256, objective=None, random_state=42,
                                reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 14:50:13,974:INFO:save_model() successfully completed......................................
2025-05-03 14:50:14,127:INFO:Initializing save_model()
2025-05-03 14:50:14,127:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 14:50:14,127:INFO:Adding model into prep_pipe
2025-05-03 14:50:14,144:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 14:50:14,156:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 14:50:14,156:INFO:save_model() successfully completed......................................
2025-05-03 14:50:14,410:INFO:Initializing save_model()
2025-05-03 14:50:14,410:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 14:50:14,410:INFO:Adding model into prep_pipe
2025-05-03 14:50:14,554:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 14:50:14,559:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 14:50:14,559:INFO:save_model() successfully completed......................................
2025-05-03 14:50:14,727:INFO:Initializing load_model()
2025-05-03 14:50:14,727:INFO:load_model(model_name=../models/LightGBM_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 14:50:14,746:INFO:Initializing load_model()
2025-05-03 14:50:14,746:INFO:load_model(model_name=../models/XGBoost_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 14:50:14,793:INFO:Initializing load_model()
2025-05-03 14:50:14,793:INFO:load_model(model_name=../models/RandomForest_bayes_opt, platform=None, authentication=None, verbose=True)
2025-05-03 14:50:14,950:INFO:Initializing interpret_model()
2025-05-03 14:50:14,950:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026FB9B2BE10>, estimator=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split',
                                learning_rate=0.49999999999999994, max_depth=-1,
                                min_child_samples=100, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=197, n_jobs=-1,
                                num_leaves=256, objective=None, random_state=42,
                                reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 14:50:14,950:INFO:Checking exceptions
2025-05-03 14:50:14,950:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 14:53:39,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:53:39,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:53:39,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:53:39,394:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 14:53:42,961:INFO:PyCaret ClassificationExperiment
2025-05-03 14:53:42,961:INFO:Logging name: clf-default-name
2025-05-03 14:53:42,961:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 14:53:42,961:INFO:version 3.3.2
2025-05-03 14:53:42,961:INFO:Initializing setup()
2025-05-03 14:53:42,961:INFO:self.USI: 59b8
2025-05-03 14:53:42,961:INFO:self._variable_keys: {'_ml_usecase', 'gpu_n_jobs_param', 'pipeline', 'y', 'USI', 'memory', 'idx', 'gpu_param', 'y_test', 'exp_id', 'log_plots_param', 'data', 'is_multiclass', 'seed', 'X_train', 'fold_groups_param', 'html_param', 'X', '_available_plots', 'target_param', 'fold_shuffle_param', 'y_train', 'fix_imbalance', 'logging_param', 'X_test', 'n_jobs_param', 'fold_generator', 'exp_name_log'}
2025-05-03 14:53:42,961:INFO:Checking environment
2025-05-03 14:53:42,961:INFO:python_version: 3.11.11
2025-05-03 14:53:42,961:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 14:53:42,961:INFO:machine: AMD64
2025-05-03 14:53:42,961:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 14:53:42,961:INFO:Memory: svmem(total=16965230592, available=4491161600, percent=73.5, used=12474068992, free=4491161600)
2025-05-03 14:53:42,961:INFO:Physical Core: 4
2025-05-03 14:53:42,961:INFO:Logical Core: 8
2025-05-03 14:53:42,961:INFO:Checking libraries
2025-05-03 14:53:42,961:INFO:System:
2025-05-03 14:53:42,961:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 14:53:42,961:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 14:53:42,961:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 14:53:42,961:INFO:PyCaret required dependencies:
2025-05-03 14:53:42,961:INFO:                 pip: 25.0
2025-05-03 14:53:42,961:INFO:          setuptools: 75.8.0
2025-05-03 14:53:42,961:INFO:             pycaret: 3.3.2
2025-05-03 14:53:42,961:INFO:             IPython: 8.32.0
2025-05-03 14:53:42,961:INFO:          ipywidgets: 8.1.6
2025-05-03 14:53:42,961:INFO:                tqdm: 4.67.1
2025-05-03 14:53:42,961:INFO:               numpy: 1.26.4
2025-05-03 14:53:42,961:INFO:              pandas: 2.1.4
2025-05-03 14:53:42,961:INFO:              jinja2: 3.1.6
2025-05-03 14:53:42,961:INFO:               scipy: 1.11.4
2025-05-03 14:53:42,961:INFO:              joblib: 1.3.2
2025-05-03 14:53:42,961:INFO:             sklearn: 1.4.2
2025-05-03 14:53:42,961:INFO:                pyod: 2.0.5
2025-05-03 14:53:42,961:INFO:            imblearn: 0.13.0
2025-05-03 14:53:42,961:INFO:   category_encoders: 2.7.0
2025-05-03 14:53:42,961:INFO:            lightgbm: 4.6.0
2025-05-03 14:53:42,961:INFO:               numba: 0.61.0
2025-05-03 14:53:42,961:INFO:            requests: 2.32.3
2025-05-03 14:53:42,961:INFO:          matplotlib: 3.7.5
2025-05-03 14:53:42,961:INFO:          scikitplot: 0.3.7
2025-05-03 14:53:42,961:INFO:         yellowbrick: 1.5
2025-05-03 14:53:42,961:INFO:              plotly: 5.24.1
2025-05-03 14:53:42,961:INFO:    plotly-resampler: Not installed
2025-05-03 14:53:42,961:INFO:             kaleido: 0.2.1
2025-05-03 14:53:42,961:INFO:           schemdraw: 0.15
2025-05-03 14:53:42,961:INFO:         statsmodels: 0.14.4
2025-05-03 14:53:42,961:INFO:              sktime: 0.26.0
2025-05-03 14:53:42,961:INFO:               tbats: 1.1.3
2025-05-03 14:53:42,961:INFO:            pmdarima: 2.0.4
2025-05-03 14:53:42,961:INFO:              psutil: 6.1.1
2025-05-03 14:53:42,961:INFO:          markupsafe: 3.0.2
2025-05-03 14:53:42,961:INFO:             pickle5: Not installed
2025-05-03 14:53:42,961:INFO:         cloudpickle: 3.1.1
2025-05-03 14:53:42,961:INFO:         deprecation: 2.1.0
2025-05-03 14:53:42,961:INFO:              xxhash: 3.5.0
2025-05-03 14:53:42,961:INFO:           wurlitzer: Not installed
2025-05-03 14:53:42,961:INFO:PyCaret optional dependencies:
2025-05-03 14:53:43,411:INFO:                shap: 0.47.2
2025-05-03 14:53:43,411:INFO:           interpret: 0.6.10
2025-05-03 14:53:43,411:INFO:                umap: Not installed
2025-05-03 14:53:43,411:INFO:     ydata_profiling: Not installed
2025-05-03 14:53:43,411:INFO:  explainerdashboard: Not installed
2025-05-03 14:53:43,411:INFO:             autoviz: Not installed
2025-05-03 14:53:43,411:INFO:           fairlearn: Not installed
2025-05-03 14:53:43,411:INFO:          deepchecks: Not installed
2025-05-03 14:53:43,411:INFO:             xgboost: 3.0.0
2025-05-03 14:53:43,411:INFO:            catboost: Not installed
2025-05-03 14:53:43,411:INFO:              kmodes: Not installed
2025-05-03 14:53:43,411:INFO:             mlxtend: Not installed
2025-05-03 14:53:43,411:INFO:       statsforecast: Not installed
2025-05-03 14:53:43,411:INFO:        tune_sklearn: Not installed
2025-05-03 14:53:43,411:INFO:                 ray: Not installed
2025-05-03 14:53:43,411:INFO:            hyperopt: 0.2.7
2025-05-03 14:53:43,411:INFO:              optuna: Not installed
2025-05-03 14:53:43,411:INFO:               skopt: 0.10.2
2025-05-03 14:53:43,411:INFO:              mlflow: 2.22.0
2025-05-03 14:53:43,411:INFO:              gradio: Not installed
2025-05-03 14:53:43,411:INFO:             fastapi: 0.115.12
2025-05-03 14:53:43,411:INFO:             uvicorn: 0.34.2
2025-05-03 14:53:43,411:INFO:              m2cgen: Not installed
2025-05-03 14:53:43,411:INFO:           evidently: Not installed
2025-05-03 14:53:43,411:INFO:               fugue: Not installed
2025-05-03 14:53:43,411:INFO:           streamlit: Not installed
2025-05-03 14:53:43,411:INFO:             prophet: Not installed
2025-05-03 14:53:43,411:INFO:None
2025-05-03 14:53:43,411:INFO:Set up data.
2025-05-03 14:53:43,428:INFO:Set up folding strategy.
2025-05-03 14:53:43,428:INFO:Set up train/test split.
2025-05-03 14:53:43,452:INFO:Set up index.
2025-05-03 14:53:43,452:INFO:Assigning column types.
2025-05-03 14:53:43,461:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 14:53:43,515:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,515:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,552:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,554:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,594:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,594:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,627:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,627:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 14:53:43,677:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,711:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,711:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,755:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 14:53:43,777:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,777:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,777:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 14:53:43,860:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,860:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,943:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:43,960:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:43,965:INFO:Set up column name cleaning.
2025-05-03 14:53:43,987:INFO:Finished creating preprocessing pipeline.
2025-05-03 14:53:43,990:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 14:53:43,990:INFO:Creating final display dataframe.
2025-05-03 14:53:44,095:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 14:53:44,177:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:44,177:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:44,257:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 14:53:44,260:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 14:53:44,260:INFO:setup() successfully completed in 1.31s...............
2025-05-03 14:53:44,273:INFO:Initializing compare_models()
2025-05-03 14:53:44,276:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 14:53:44,276:INFO:Checking exceptions
2025-05-03 14:53:44,293:INFO:Preparing display monitor
2025-05-03 14:53:44,369:INFO:Initializing Logistic Regression
2025-05-03 14:53:44,370:INFO:Total runtime is 2.9226144154866535e-05 minutes
2025-05-03 14:53:44,373:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:44,373:INFO:Initializing create_model()
2025-05-03 14:53:44,373:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:44,373:INFO:Checking exceptions
2025-05-03 14:53:44,373:INFO:Importing libraries
2025-05-03 14:53:44,373:INFO:Copying training dataset
2025-05-03 14:53:44,391:INFO:Defining folds
2025-05-03 14:53:44,391:INFO:Declaring metric variables
2025-05-03 14:53:44,391:INFO:Importing untrained model
2025-05-03 14:53:44,401:INFO:Logistic Regression Imported successfully
2025-05-03 14:53:44,405:INFO:Starting cross validation
2025-05-03 14:53:44,409:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:48,317:INFO:Calculating mean and std
2025-05-03 14:53:48,317:INFO:Creating metrics dataframe
2025-05-03 14:53:48,322:INFO:Uploading results into container
2025-05-03 14:53:48,322:INFO:Uploading model into container now
2025-05-03 14:53:48,322:INFO:_master_model_container: 1
2025-05-03 14:53:48,322:INFO:_display_container: 2
2025-05-03 14:53:48,322:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 14:53:48,322:INFO:create_model() successfully completed......................................
2025-05-03 14:53:48,459:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:48,459:INFO:Creating metrics dataframe
2025-05-03 14:53:48,463:INFO:Initializing Random Forest Classifier
2025-05-03 14:53:48,463:INFO:Total runtime is 0.06824287176132202 minutes
2025-05-03 14:53:48,463:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:48,463:INFO:Initializing create_model()
2025-05-03 14:53:48,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:48,469:INFO:Checking exceptions
2025-05-03 14:53:48,469:INFO:Importing libraries
2025-05-03 14:53:48,469:INFO:Copying training dataset
2025-05-03 14:53:48,487:INFO:Defining folds
2025-05-03 14:53:48,487:INFO:Declaring metric variables
2025-05-03 14:53:48,491:INFO:Importing untrained model
2025-05-03 14:53:48,491:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:53:48,501:INFO:Starting cross validation
2025-05-03 14:53:48,501:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:52,776:INFO:Calculating mean and std
2025-05-03 14:53:52,776:INFO:Creating metrics dataframe
2025-05-03 14:53:52,780:INFO:Uploading results into container
2025-05-03 14:53:52,780:INFO:Uploading model into container now
2025-05-03 14:53:52,780:INFO:_master_model_container: 2
2025-05-03 14:53:52,780:INFO:_display_container: 2
2025-05-03 14:53:52,780:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 14:53:52,780:INFO:create_model() successfully completed......................................
2025-05-03 14:53:52,947:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:52,947:INFO:Creating metrics dataframe
2025-05-03 14:53:52,958:INFO:Initializing Extreme Gradient Boosting
2025-05-03 14:53:52,958:INFO:Total runtime is 0.14315194288889566 minutes
2025-05-03 14:53:52,965:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:52,965:INFO:Initializing create_model()
2025-05-03 14:53:52,965:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:52,965:INFO:Checking exceptions
2025-05-03 14:53:52,965:INFO:Importing libraries
2025-05-03 14:53:52,965:INFO:Copying training dataset
2025-05-03 14:53:52,986:INFO:Defining folds
2025-05-03 14:53:52,986:INFO:Declaring metric variables
2025-05-03 14:53:52,986:INFO:Importing untrained model
2025-05-03 14:53:52,996:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:53:53,003:INFO:Starting cross validation
2025-05-03 14:53:53,003:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:56,338:INFO:Calculating mean and std
2025-05-03 14:53:56,338:INFO:Creating metrics dataframe
2025-05-03 14:53:56,338:INFO:Uploading results into container
2025-05-03 14:53:56,338:INFO:Uploading model into container now
2025-05-03 14:53:56,338:INFO:_master_model_container: 3
2025-05-03 14:53:56,338:INFO:_display_container: 2
2025-05-03 14:53:56,338:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:53:56,338:INFO:create_model() successfully completed......................................
2025-05-03 14:53:56,483:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:56,483:INFO:Creating metrics dataframe
2025-05-03 14:53:56,483:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 14:53:56,483:INFO:Total runtime is 0.20190894603729248 minutes
2025-05-03 14:53:56,483:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:56,483:INFO:Initializing create_model()
2025-05-03 14:53:56,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:56,483:INFO:Checking exceptions
2025-05-03 14:53:56,483:INFO:Importing libraries
2025-05-03 14:53:56,483:INFO:Copying training dataset
2025-05-03 14:53:56,516:INFO:Defining folds
2025-05-03 14:53:56,516:INFO:Declaring metric variables
2025-05-03 14:53:56,518:INFO:Importing untrained model
2025-05-03 14:53:56,518:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:53:56,533:INFO:Starting cross validation
2025-05-03 14:53:56,533:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:57,012:INFO:Calculating mean and std
2025-05-03 14:53:57,014:INFO:Creating metrics dataframe
2025-05-03 14:53:57,016:INFO:Uploading results into container
2025-05-03 14:53:57,016:INFO:Uploading model into container now
2025-05-03 14:53:57,018:INFO:_master_model_container: 4
2025-05-03 14:53:57,018:INFO:_display_container: 2
2025-05-03 14:53:57,018:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:53:57,018:INFO:create_model() successfully completed......................................
2025-05-03 14:53:57,150:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:57,150:INFO:Creating metrics dataframe
2025-05-03 14:53:57,166:INFO:Initializing Extra Trees Classifier
2025-05-03 14:53:57,166:INFO:Total runtime is 0.21329177618026735 minutes
2025-05-03 14:53:57,179:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:57,179:INFO:Initializing create_model()
2025-05-03 14:53:57,179:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:57,179:INFO:Checking exceptions
2025-05-03 14:53:57,182:INFO:Importing libraries
2025-05-03 14:53:57,182:INFO:Copying training dataset
2025-05-03 14:53:57,206:INFO:Defining folds
2025-05-03 14:53:57,206:INFO:Declaring metric variables
2025-05-03 14:53:57,211:INFO:Importing untrained model
2025-05-03 14:53:57,216:INFO:Extra Trees Classifier Imported successfully
2025-05-03 14:53:57,216:INFO:Starting cross validation
2025-05-03 14:53:57,216:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:58,274:INFO:Calculating mean and std
2025-05-03 14:53:58,274:INFO:Creating metrics dataframe
2025-05-03 14:53:58,278:INFO:Uploading results into container
2025-05-03 14:53:58,279:INFO:Uploading model into container now
2025-05-03 14:53:58,279:INFO:_master_model_container: 5
2025-05-03 14:53:58,279:INFO:_display_container: 2
2025-05-03 14:53:58,279:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 14:53:58,281:INFO:create_model() successfully completed......................................
2025-05-03 14:53:58,414:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:58,414:INFO:Creating metrics dataframe
2025-05-03 14:53:58,422:INFO:Initializing Ridge Classifier
2025-05-03 14:53:58,422:INFO:Total runtime is 0.23421863714853924 minutes
2025-05-03 14:53:58,431:INFO:SubProcess create_model() called ==================================
2025-05-03 14:53:58,431:INFO:Initializing create_model()
2025-05-03 14:53:58,431:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0EE4750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:58,431:INFO:Checking exceptions
2025-05-03 14:53:58,431:INFO:Importing libraries
2025-05-03 14:53:58,431:INFO:Copying training dataset
2025-05-03 14:53:58,465:INFO:Defining folds
2025-05-03 14:53:58,465:INFO:Declaring metric variables
2025-05-03 14:53:58,475:INFO:Importing untrained model
2025-05-03 14:53:58,483:INFO:Ridge Classifier Imported successfully
2025-05-03 14:53:58,498:INFO:Starting cross validation
2025-05-03 14:53:58,498:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:58,586:INFO:Calculating mean and std
2025-05-03 14:53:58,586:INFO:Creating metrics dataframe
2025-05-03 14:53:58,586:INFO:Uploading results into container
2025-05-03 14:53:58,586:INFO:Uploading model into container now
2025-05-03 14:53:58,586:INFO:_master_model_container: 6
2025-05-03 14:53:58,586:INFO:_display_container: 2
2025-05-03 14:53:58,586:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 14:53:58,586:INFO:create_model() successfully completed......................................
2025-05-03 14:53:58,726:INFO:SubProcess create_model() end ==================================
2025-05-03 14:53:58,726:INFO:Creating metrics dataframe
2025-05-03 14:53:58,743:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 14:53:58,758:INFO:Initializing create_model()
2025-05-03 14:53:58,758:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:58,758:INFO:Checking exceptions
2025-05-03 14:53:58,760:INFO:Importing libraries
2025-05-03 14:53:58,760:INFO:Copying training dataset
2025-05-03 14:53:58,790:INFO:Defining folds
2025-05-03 14:53:58,790:INFO:Declaring metric variables
2025-05-03 14:53:58,790:INFO:Importing untrained model
2025-05-03 14:53:58,790:INFO:Declaring custom model
2025-05-03 14:53:58,790:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:53:58,790:INFO:Cross validation set to False
2025-05-03 14:53:58,790:INFO:Fitting Model
2025-05-03 14:53:58,837:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:53:58,840:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001242 seconds.
2025-05-03 14:53:58,840:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:53:58,840:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:53:58,840:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:53:58,840:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:53:58,840:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:53:58,840:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:53:59,014:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:53:59,014:INFO:create_model() successfully completed......................................
2025-05-03 14:53:59,183:INFO:_master_model_container: 6
2025-05-03 14:53:59,183:INFO:_display_container: 2
2025-05-03 14:53:59,183:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:53:59,183:INFO:compare_models() successfully completed......................................
2025-05-03 14:53:59,210:INFO:Initializing create_model()
2025-05-03 14:53:59,210:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:53:59,210:INFO:Checking exceptions
2025-05-03 14:53:59,226:INFO:Importing libraries
2025-05-03 14:53:59,226:INFO:Copying training dataset
2025-05-03 14:53:59,250:INFO:Defining folds
2025-05-03 14:53:59,251:INFO:Declaring metric variables
2025-05-03 14:53:59,251:INFO:Importing untrained model
2025-05-03 14:53:59,256:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:53:59,264:INFO:Starting cross validation
2025-05-03 14:53:59,265:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:53:59,819:INFO:Calculating mean and std
2025-05-03 14:53:59,819:INFO:Creating metrics dataframe
2025-05-03 14:53:59,825:INFO:Finalizing model
2025-05-03 14:53:59,853:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:53:59,856:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001238 seconds.
2025-05-03 14:53:59,856:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:53:59,856:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:53:59,856:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:53:59,857:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:53:59,857:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:53:59,857:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:54:00,043:INFO:Uploading results into container
2025-05-03 14:54:00,045:INFO:Uploading model into container now
2025-05-03 14:54:00,055:INFO:_master_model_container: 7
2025-05-03 14:54:00,057:INFO:_display_container: 3
2025-05-03 14:54:00,057:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:54:00,057:INFO:create_model() successfully completed......................................
2025-05-03 14:54:00,214:INFO:Initializing create_model()
2025-05-03 14:54:00,214:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:54:00,214:INFO:Checking exceptions
2025-05-03 14:54:00,230:INFO:Importing libraries
2025-05-03 14:54:00,230:INFO:Copying training dataset
2025-05-03 14:54:00,256:INFO:Defining folds
2025-05-03 14:54:00,256:INFO:Declaring metric variables
2025-05-03 14:54:00,264:INFO:Importing untrained model
2025-05-03 14:54:00,267:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:54:00,276:INFO:Starting cross validation
2025-05-03 14:54:00,276:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:54:00,644:INFO:Calculating mean and std
2025-05-03 14:54:00,644:INFO:Creating metrics dataframe
2025-05-03 14:54:00,647:INFO:Finalizing model
2025-05-03 14:54:00,796:INFO:Uploading results into container
2025-05-03 14:54:00,798:INFO:Uploading model into container now
2025-05-03 14:54:00,805:INFO:_master_model_container: 8
2025-05-03 14:54:00,808:INFO:_display_container: 4
2025-05-03 14:54:00,808:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:54:00,808:INFO:create_model() successfully completed......................................
2025-05-03 14:54:00,980:INFO:Initializing create_model()
2025-05-03 14:54:00,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:54:00,980:INFO:Checking exceptions
2025-05-03 14:54:01,000:INFO:Importing libraries
2025-05-03 14:54:01,001:INFO:Copying training dataset
2025-05-03 14:54:01,022:INFO:Defining folds
2025-05-03 14:54:01,022:INFO:Declaring metric variables
2025-05-03 14:54:01,023:INFO:Importing untrained model
2025-05-03 14:54:01,023:INFO:Random Forest Classifier Imported successfully
2025-05-03 14:54:01,038:INFO:Starting cross validation
2025-05-03 14:54:01,038:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:54:02,221:INFO:Calculating mean and std
2025-05-03 14:54:02,221:INFO:Creating metrics dataframe
2025-05-03 14:54:02,224:INFO:Finalizing model
2025-05-03 14:54:02,797:INFO:Uploading results into container
2025-05-03 14:54:02,797:INFO:Uploading model into container now
2025-05-03 14:54:02,847:INFO:_master_model_container: 9
2025-05-03 14:54:02,849:INFO:_display_container: 5
2025-05-03 14:54:02,849:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 14:54:02,849:INFO:create_model() successfully completed......................................
2025-05-03 14:54:03,005:INFO:Initializing tune_model()
2025-05-03 14:54:03,005:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:54:03,005:INFO:Checking exceptions
2025-05-03 14:54:03,006:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:54:03,027:INFO:Copying training dataset
2025-05-03 14:54:03,042:INFO:Checking base model
2025-05-03 14:54:03,043:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 14:54:03,043:INFO:Declaring metric variables
2025-05-03 14:54:03,043:INFO:Defining Hyperparameters
2025-05-03 14:54:03,180:INFO:Tuning with n_jobs=-1
2025-05-03 14:54:03,186:INFO:Initializing skopt.BayesSearchCV
2025-05-03 14:56:01,696:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 14:56:01,697:INFO:Hyperparameter search completed
2025-05-03 14:56:01,697:INFO:SubProcess create_model() called ==================================
2025-05-03 14:56:01,697:INFO:Initializing create_model()
2025-05-03 14:56:01,697:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CCC714A10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 14:56:01,697:INFO:Checking exceptions
2025-05-03 14:56:01,697:INFO:Importing libraries
2025-05-03 14:56:01,697:INFO:Copying training dataset
2025-05-03 14:56:01,724:INFO:Defining folds
2025-05-03 14:56:01,724:INFO:Declaring metric variables
2025-05-03 14:56:01,724:INFO:Importing untrained model
2025-05-03 14:56:01,724:INFO:Declaring custom model
2025-05-03 14:56:01,734:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:56:01,740:INFO:Starting cross validation
2025-05-03 14:56:01,740:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:56:02,119:INFO:Calculating mean and std
2025-05-03 14:56:02,119:INFO:Creating metrics dataframe
2025-05-03 14:56:02,127:INFO:Finalizing model
2025-05-03 14:56:02,140:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:56:02,140:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:56:02,140:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:56:02,156:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 14:56:02,156:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 14:56:02,156:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 14:56:02,156:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:56:02,159:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002214 seconds.
2025-05-03 14:56:02,161:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 14:56:02,161:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:56:02,161:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:56:02,161:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:56:02,161:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:56:02,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,248:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,252:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,256:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,259:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,260:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,262:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,265:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,277:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,285:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,289:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,291:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,306:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 14:56:02,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 14:56:02,321:INFO:Uploading results into container
2025-05-03 14:56:02,321:INFO:Uploading model into container now
2025-05-03 14:56:02,322:INFO:_master_model_container: 10
2025-05-03 14:56:02,322:INFO:_display_container: 6
2025-05-03 14:56:02,322:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:56:02,324:INFO:create_model() successfully completed......................................
2025-05-03 14:56:02,473:INFO:SubProcess create_model() end ==================================
2025-05-03 14:56:02,473:INFO:choose_better activated
2025-05-03 14:56:02,480:INFO:SubProcess create_model() called ==================================
2025-05-03 14:56:02,483:INFO:Initializing create_model()
2025-05-03 14:56:02,483:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:56:02,483:INFO:Checking exceptions
2025-05-03 14:56:02,488:INFO:Importing libraries
2025-05-03 14:56:02,488:INFO:Copying training dataset
2025-05-03 14:56:02,529:INFO:Defining folds
2025-05-03 14:56:02,530:INFO:Declaring metric variables
2025-05-03 14:56:02,530:INFO:Importing untrained model
2025-05-03 14:56:02,530:INFO:Declaring custom model
2025-05-03 14:56:02,530:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 14:56:02,530:INFO:Starting cross validation
2025-05-03 14:56:02,530:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:56:03,044:INFO:Calculating mean and std
2025-05-03 14:56:03,044:INFO:Creating metrics dataframe
2025-05-03 14:56:03,045:INFO:Finalizing model
2025-05-03 14:56:03,073:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 14:56:03,076:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.
2025-05-03 14:56:03,076:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 14:56:03,076:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 14:56:03,076:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 14:56:03,076:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 14:56:03,076:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 14:56:03,076:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 14:56:03,193:INFO:Uploading results into container
2025-05-03 14:56:03,195:INFO:Uploading model into container now
2025-05-03 14:56:03,195:INFO:_master_model_container: 11
2025-05-03 14:56:03,195:INFO:_display_container: 7
2025-05-03 14:56:03,195:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:56:03,195:INFO:create_model() successfully completed......................................
2025-05-03 14:56:03,342:INFO:SubProcess create_model() end ==================================
2025-05-03 14:56:03,342:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 14:56:03,342:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 14:56:03,344:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 14:56:03,344:INFO:choose_better completed
2025-05-03 14:56:03,345:INFO:_master_model_container: 11
2025-05-03 14:56:03,345:INFO:_display_container: 6
2025-05-03 14:56:03,345:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 14:56:03,345:INFO:tune_model() successfully completed......................................
2025-05-03 14:56:03,516:INFO:Initializing tune_model()
2025-05-03 14:56:03,524:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:56:03,524:INFO:Checking exceptions
2025-05-03 14:56:03,524:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:56:03,550:INFO:Copying training dataset
2025-05-03 14:56:03,561:INFO:Checking base model
2025-05-03 14:56:03,561:INFO:Base model : Extreme Gradient Boosting
2025-05-03 14:56:03,568:INFO:Declaring metric variables
2025-05-03 14:56:03,570:INFO:Defining Hyperparameters
2025-05-03 14:56:03,697:INFO:Tuning with n_jobs=-1
2025-05-03 14:56:03,713:INFO:Initializing skopt.BayesSearchCV
2025-05-03 14:57:56,952:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 14:57:56,952:INFO:Hyperparameter search completed
2025-05-03 14:57:56,952:INFO:SubProcess create_model() called ==================================
2025-05-03 14:57:56,952:INFO:Initializing create_model()
2025-05-03 14:57:56,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CD0D81190>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 14:57:56,952:INFO:Checking exceptions
2025-05-03 14:57:56,952:INFO:Importing libraries
2025-05-03 14:57:56,952:INFO:Copying training dataset
2025-05-03 14:57:56,969:INFO:Defining folds
2025-05-03 14:57:56,969:INFO:Declaring metric variables
2025-05-03 14:57:56,982:INFO:Importing untrained model
2025-05-03 14:57:56,982:INFO:Declaring custom model
2025-05-03 14:57:56,985:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:57:56,995:INFO:Starting cross validation
2025-05-03 14:57:56,995:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:57:57,455:INFO:Calculating mean and std
2025-05-03 14:57:57,455:INFO:Creating metrics dataframe
2025-05-03 14:57:57,455:INFO:Finalizing model
2025-05-03 14:57:57,624:INFO:Uploading results into container
2025-05-03 14:57:57,626:INFO:Uploading model into container now
2025-05-03 14:57:57,626:INFO:_master_model_container: 12
2025-05-03 14:57:57,626:INFO:_display_container: 7
2025-05-03 14:57:57,628:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 14:57:57,628:INFO:create_model() successfully completed......................................
2025-05-03 14:57:57,768:INFO:SubProcess create_model() end ==================================
2025-05-03 14:57:57,768:INFO:choose_better activated
2025-05-03 14:57:57,768:INFO:SubProcess create_model() called ==================================
2025-05-03 14:57:57,768:INFO:Initializing create_model()
2025-05-03 14:57:57,768:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 14:57:57,768:INFO:Checking exceptions
2025-05-03 14:57:57,785:INFO:Importing libraries
2025-05-03 14:57:57,785:INFO:Copying training dataset
2025-05-03 14:57:57,819:INFO:Defining folds
2025-05-03 14:57:57,819:INFO:Declaring metric variables
2025-05-03 14:57:57,819:INFO:Importing untrained model
2025-05-03 14:57:57,819:INFO:Declaring custom model
2025-05-03 14:57:57,827:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 14:57:57,827:INFO:Starting cross validation
2025-05-03 14:57:57,827:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 14:57:58,182:INFO:Calculating mean and std
2025-05-03 14:57:58,182:INFO:Creating metrics dataframe
2025-05-03 14:57:58,184:INFO:Finalizing model
2025-05-03 14:57:58,317:INFO:Uploading results into container
2025-05-03 14:57:58,317:INFO:Uploading model into container now
2025-05-03 14:57:58,319:INFO:_master_model_container: 13
2025-05-03 14:57:58,319:INFO:_display_container: 8
2025-05-03 14:57:58,320:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 14:57:58,320:INFO:create_model() successfully completed......................................
2025-05-03 14:57:58,465:INFO:SubProcess create_model() end ==================================
2025-05-03 14:57:58,467:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 14:57:58,467:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 14:57:58,467:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 14:57:58,467:INFO:choose_better completed
2025-05-03 14:57:58,479:INFO:_master_model_container: 13
2025-05-03 14:57:58,479:INFO:_display_container: 7
2025-05-03 14:57:58,487:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 14:57:58,487:INFO:tune_model() successfully completed......................................
2025-05-03 14:57:58,646:INFO:Initializing tune_model()
2025-05-03 14:57:58,656:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 14:57:58,656:INFO:Checking exceptions
2025-05-03 14:57:58,656:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 14:57:58,680:INFO:Copying training dataset
2025-05-03 14:57:58,691:INFO:Checking base model
2025-05-03 14:57:58,691:INFO:Base model : Random Forest Classifier
2025-05-03 14:57:58,691:INFO:Declaring metric variables
2025-05-03 14:57:58,699:INFO:Defining Hyperparameters
2025-05-03 14:57:58,829:INFO:Tuning with n_jobs=-1
2025-05-03 14:57:58,836:INFO:Initializing skopt.BayesSearchCV
2025-05-03 15:01:49,573:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 15:01:49,573:INFO:Hyperparameter search completed
2025-05-03 15:01:49,577:INFO:SubProcess create_model() called ==================================
2025-05-03 15:01:49,577:INFO:Initializing create_model()
2025-05-03 15:01:49,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000021CA4DEEFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 15:01:49,577:INFO:Checking exceptions
2025-05-03 15:01:49,577:INFO:Importing libraries
2025-05-03 15:01:49,577:INFO:Copying training dataset
2025-05-03 15:01:49,594:INFO:Defining folds
2025-05-03 15:01:49,596:INFO:Declaring metric variables
2025-05-03 15:01:49,596:INFO:Importing untrained model
2025-05-03 15:01:49,596:INFO:Declaring custom model
2025-05-03 15:01:49,596:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:01:49,610:INFO:Starting cross validation
2025-05-03 15:01:49,610:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:01:53,876:INFO:Calculating mean and std
2025-05-03 15:01:53,876:INFO:Creating metrics dataframe
2025-05-03 15:01:53,876:INFO:Finalizing model
2025-05-03 15:01:56,406:INFO:Uploading results into container
2025-05-03 15:01:56,406:INFO:Uploading model into container now
2025-05-03 15:01:56,406:INFO:_master_model_container: 14
2025-05-03 15:01:56,406:INFO:_display_container: 8
2025-05-03 15:01:56,406:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 15:01:56,406:INFO:create_model() successfully completed......................................
2025-05-03 15:01:56,538:INFO:SubProcess create_model() end ==================================
2025-05-03 15:01:56,538:INFO:choose_better activated
2025-05-03 15:01:56,551:INFO:SubProcess create_model() called ==================================
2025-05-03 15:01:56,551:INFO:Initializing create_model()
2025-05-03 15:01:56,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:01:56,551:INFO:Checking exceptions
2025-05-03 15:01:56,555:INFO:Importing libraries
2025-05-03 15:01:56,555:INFO:Copying training dataset
2025-05-03 15:01:56,571:INFO:Defining folds
2025-05-03 15:01:56,571:INFO:Declaring metric variables
2025-05-03 15:01:56,571:INFO:Importing untrained model
2025-05-03 15:01:56,571:INFO:Declaring custom model
2025-05-03 15:01:56,571:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:01:56,571:INFO:Starting cross validation
2025-05-03 15:01:56,571:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:01:58,059:INFO:Calculating mean and std
2025-05-03 15:01:58,059:INFO:Creating metrics dataframe
2025-05-03 15:01:58,061:INFO:Finalizing model
2025-05-03 15:01:58,670:INFO:Uploading results into container
2025-05-03 15:01:58,670:INFO:Uploading model into container now
2025-05-03 15:01:58,670:INFO:_master_model_container: 15
2025-05-03 15:01:58,670:INFO:_display_container: 9
2025-05-03 15:01:58,670:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 15:01:58,670:INFO:create_model() successfully completed......................................
2025-05-03 15:01:58,812:INFO:SubProcess create_model() end ==================================
2025-05-03 15:01:58,812:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 15:01:58,812:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 15:01:58,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 15:01:58,814:INFO:choose_better completed
2025-05-03 15:01:58,819:INFO:_master_model_container: 15
2025-05-03 15:01:58,819:INFO:_display_container: 8
2025-05-03 15:01:58,819:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 15:01:58,819:INFO:tune_model() successfully completed......................................
2025-05-03 15:01:58,990:INFO:Initializing create_model()
2025-05-03 15:01:58,990:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 15:01:58,990:INFO:Checking exceptions
2025-05-03 15:01:59,016:INFO:Importing libraries
2025-05-03 15:01:59,017:INFO:Copying training dataset
2025-05-03 15:01:59,036:INFO:Defining folds
2025-05-03 15:01:59,036:INFO:Declaring metric variables
2025-05-03 15:01:59,036:INFO:Importing untrained model
2025-05-03 15:01:59,044:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 15:01:59,049:INFO:Starting cross validation
2025-05-03 15:01:59,052:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:01:59,456:INFO:Calculating mean and std
2025-05-03 15:01:59,456:INFO:Creating metrics dataframe
2025-05-03 15:01:59,463:INFO:Finalizing model
2025-05-03 15:01:59,475:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 15:01:59,475:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 15:01:59,475:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 15:01:59,502:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 15:01:59,502:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 15:01:59,502:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 15:01:59,502:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 15:01:59,506:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002337 seconds.
2025-05-03 15:01:59,506:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 15:01:59,506:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 15:01:59,506:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 15:01:59,506:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 15:01:59,506:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 15:01:59,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,585:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,585:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,587:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,594:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,601:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,602:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,604:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,606:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,608:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,610:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,620:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,622:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,624:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,626:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,628:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,630:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,632:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,634:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,636:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,638:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,640:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,642:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,644:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,646:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,648:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,650:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,652:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,658:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,660:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:01:59,662:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:01:59,673:INFO:Uploading results into container
2025-05-03 15:01:59,673:INFO:Uploading model into container now
2025-05-03 15:01:59,685:INFO:_master_model_container: 16
2025-05-03 15:01:59,686:INFO:_display_container: 9
2025-05-03 15:01:59,686:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:01:59,686:INFO:create_model() successfully completed......................................
2025-05-03 15:01:59,819:INFO:Initializing create_model()
2025-05-03 15:01:59,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 15:01:59,819:INFO:Checking exceptions
2025-05-03 15:01:59,868:INFO:Importing libraries
2025-05-03 15:01:59,868:INFO:Copying training dataset
2025-05-03 15:01:59,900:INFO:Defining folds
2025-05-03 15:01:59,900:INFO:Declaring metric variables
2025-05-03 15:01:59,908:INFO:Importing untrained model
2025-05-03 15:01:59,917:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 15:01:59,960:INFO:Starting cross validation
2025-05-03 15:01:59,960:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:02:00,438:INFO:Calculating mean and std
2025-05-03 15:02:00,438:INFO:Creating metrics dataframe
2025-05-03 15:02:00,438:INFO:Finalizing model
2025-05-03 15:02:00,613:INFO:Uploading results into container
2025-05-03 15:02:00,615:INFO:Uploading model into container now
2025-05-03 15:02:00,625:INFO:_master_model_container: 17
2025-05-03 15:02:00,625:INFO:_display_container: 10
2025-05-03 15:02:00,627:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 15:02:00,627:INFO:create_model() successfully completed......................................
2025-05-03 15:02:00,768:INFO:Initializing create_model()
2025-05-03 15:02:00,768:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 15:02:00,768:INFO:Checking exceptions
2025-05-03 15:02:00,792:INFO:Importing libraries
2025-05-03 15:02:00,792:INFO:Copying training dataset
2025-05-03 15:02:00,835:INFO:Defining folds
2025-05-03 15:02:00,835:INFO:Declaring metric variables
2025-05-03 15:02:00,841:INFO:Importing untrained model
2025-05-03 15:02:00,850:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:02:00,863:INFO:Starting cross validation
2025-05-03 15:02:00,864:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:02:05,045:INFO:Calculating mean and std
2025-05-03 15:02:05,045:INFO:Creating metrics dataframe
2025-05-03 15:02:05,048:INFO:Finalizing model
2025-05-03 15:02:07,408:INFO:Uploading results into container
2025-05-03 15:02:07,408:INFO:Uploading model into container now
2025-05-03 15:02:07,433:INFO:_master_model_container: 18
2025-05-03 15:02:07,433:INFO:_display_container: 11
2025-05-03 15:02:07,437:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 15:02:07,437:INFO:create_model() successfully completed......................................
2025-05-03 15:02:07,601:INFO:Initializing interpret_model()
2025-05-03 15:02:07,601:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:02:07,601:INFO:Checking exceptions
2025-05-03 15:02:07,601:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:02:07,627:INFO:plot type: msa
2025-05-03 15:02:08,024:INFO:Visual Rendered Successfully
2025-05-03 15:02:08,024:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:08,365:INFO:Initializing interpret_model()
2025-05-03 15:02:08,365:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:02:08,371:INFO:Checking exceptions
2025-05-03 15:02:08,371:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:02:08,394:INFO:plot type: msa
2025-05-03 15:02:08,679:INFO:Visual Rendered Successfully
2025-05-03 15:02:08,679:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:08,836:INFO:Initializing interpret_model()
2025-05-03 15:02:08,836:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:02:08,836:INFO:Checking exceptions
2025-05-03 15:02:08,836:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:02:08,863:INFO:plot type: msa
2025-05-03 15:02:08,863:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 15:02:09,150:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 15:02:09,262:INFO:Visual Rendered Successfully
2025-05-03 15:02:09,262:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:09,421:INFO:Initializing save_model()
2025-05-03 15:02:09,421:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:02:09,421:INFO:Adding model into prep_pipe
2025-05-03 15:02:09,432:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 15:02:09,436:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 15:02:09,436:INFO:save_model() successfully completed......................................
2025-05-03 15:02:09,587:INFO:Initializing save_model()
2025-05-03 15:02:09,587:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:02:09,587:INFO:Adding model into prep_pipe
2025-05-03 15:02:09,599:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 15:02:09,606:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 15:02:09,607:INFO:save_model() successfully completed......................................
2025-05-03 15:02:09,761:INFO:Initializing save_model()
2025-05-03 15:02:09,761:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:02:09,761:INFO:Adding model into prep_pipe
2025-05-03 15:02:09,920:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 15:02:09,920:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 15:02:09,920:INFO:save_model() successfully completed......................................
2025-05-03 15:02:10,101:INFO:Initializing interpret_model()
2025-05-03 15:02:10,101:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:02:10,101:INFO:Checking exceptions
2025-05-03 15:02:10,101:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:10,126:INFO:plot type: summary
2025-05-03 15:02:10,126:INFO:Creating TreeExplainer
2025-05-03 15:02:10,167:INFO:Compiling shap values
2025-05-03 15:02:10,916:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 15:02:10,916:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:11,177:INFO:Visual Rendered Successfully
2025-05-03 15:02:11,177:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:11,321:INFO:Initializing interpret_model()
2025-05-03 15:02:11,321:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:02:11,321:INFO:Checking exceptions
2025-05-03 15:02:11,321:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:11,356:INFO:plot type: summary
2025-05-03 15:02:11,356:INFO:Creating TreeExplainer
2025-05-03 15:02:11,435:INFO:Compiling shap values
2025-05-03 15:02:12,184:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 15:02:12,184:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:13,782:INFO:Visual Rendered Successfully
2025-05-03 15:02:13,782:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:13,956:INFO:Initializing interpret_model()
2025-05-03 15:02:13,956:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:02:13,956:INFO:Checking exceptions
2025-05-03 15:02:13,956:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:13,976:INFO:plot type: summary
2025-05-03 15:02:13,976:INFO:Creating TreeExplainer
2025-05-03 15:02:14,014:INFO:Compiling shap values
2025-05-03 15:02:15,776:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:16,007:INFO:Visual Rendered Successfully
2025-05-03 15:02:16,007:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:16,189:INFO:Initializing interpret_model()
2025-05-03 15:02:16,191:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:02:16,191:INFO:Checking exceptions
2025-05-03 15:02:16,191:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:16,217:INFO:plot type: summary
2025-05-03 15:02:16,217:INFO:Creating TreeExplainer
2025-05-03 15:02:16,247:INFO:Compiling shap values
2025-05-03 15:02:18,129:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:19,680:INFO:Visual Rendered Successfully
2025-05-03 15:02:19,680:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:19,957:INFO:Initializing interpret_model()
2025-05-03 15:02:19,957:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:02:19,957:INFO:Checking exceptions
2025-05-03 15:02:19,957:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:19,964:INFO:plot type: summary
2025-05-03 15:02:19,964:INFO:Creating TreeExplainer
2025-05-03 15:02:20,002:INFO:Compiling shap values
2025-05-03 15:02:44,416:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:44,429:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:44,499:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:02:44,664:INFO:Visual Rendered Successfully
2025-05-03 15:02:44,664:INFO:interpret_model() successfully completed......................................
2025-05-03 15:02:44,817:INFO:Initializing interpret_model()
2025-05-03 15:02:44,817:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:02:44,817:INFO:Checking exceptions
2025-05-03 15:02:44,817:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:02:44,829:INFO:plot type: summary
2025-05-03 15:02:44,829:INFO:Creating TreeExplainer
2025-05-03 15:02:44,861:INFO:Compiling shap values
2025-05-03 15:03:09,911:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:03:09,928:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:03:09,999:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:03:10,179:INFO:Visual Rendered Successfully
2025-05-03 15:03:10,179:INFO:interpret_model() successfully completed......................................
2025-05-03 15:27:42,476:INFO:Initializing interpret_model()
2025-05-03 15:27:42,476:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:27:42,476:INFO:Checking exceptions
2025-05-03 15:28:11,323:INFO:Initializing interpret_model()
2025-05-03 15:28:11,323:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:28:11,323:INFO:Checking exceptions
2025-05-03 15:28:49,211:INFO:Initializing interpret_model()
2025-05-03 15:28:49,211:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:28:49,211:INFO:Checking exceptions
2025-05-03 15:28:49,211:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:28:49,243:INFO:plot type: summary
2025-05-03 15:28:49,243:INFO:Creating TreeExplainer
2025-05-03 15:28:49,276:INFO:Compiling shap values
2025-05-03 15:41:00,512:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:41:00,526:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:726: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:41:00,661:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\plots\_beeswarm.py:746: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:41:00,971:INFO:Visual Rendered Successfully
2025-05-03 15:41:00,971:INFO:interpret_model() successfully completed......................................
2025-05-03 15:41:01,154:INFO:Initializing interpret_model()
2025-05-03 15:41:01,154:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021CA4F44B10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:41:01,154:INFO:Checking exceptions
2025-05-03 15:41:01,154:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:41:01,179:INFO:plot type: summary
2025-05-03 15:41:01,179:INFO:Creating TreeExplainer
2025-05-03 15:41:01,216:INFO:Compiling shap values
2025-05-03 15:47:11,249:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 15:47:11,249:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 15:47:11,249:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 15:47:11,249:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 15:47:15,348:INFO:PyCaret ClassificationExperiment
2025-05-03 15:47:15,348:INFO:Logging name: clf-default-name
2025-05-03 15:47:15,348:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 15:47:15,348:INFO:version 3.3.2
2025-05-03 15:47:15,348:INFO:Initializing setup()
2025-05-03 15:47:15,348:INFO:self.USI: 8f06
2025-05-03 15:47:15,348:INFO:self._variable_keys: {'idx', 'X', 'target_param', 'exp_name_log', 'gpu_n_jobs_param', 'y', 'fold_groups_param', 'logging_param', 'pipeline', 'n_jobs_param', 'fold_generator', 'fold_shuffle_param', 'html_param', 'X_test', 'is_multiclass', 'gpu_param', 'y_test', 'log_plots_param', 'X_train', 'data', 'USI', '_available_plots', 'y_train', 'fix_imbalance', 'memory', 'seed', 'exp_id', '_ml_usecase'}
2025-05-03 15:47:15,348:INFO:Checking environment
2025-05-03 15:47:15,348:INFO:python_version: 3.11.11
2025-05-03 15:47:15,348:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 15:47:15,348:INFO:machine: AMD64
2025-05-03 15:47:15,348:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 15:47:15,348:INFO:Memory: svmem(total=16965230592, available=4042047488, percent=76.2, used=12923183104, free=4042047488)
2025-05-03 15:47:15,348:INFO:Physical Core: 4
2025-05-03 15:47:15,348:INFO:Logical Core: 8
2025-05-03 15:47:15,348:INFO:Checking libraries
2025-05-03 15:47:15,357:INFO:System:
2025-05-03 15:47:15,357:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 15:47:15,357:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 15:47:15,357:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 15:47:15,357:INFO:PyCaret required dependencies:
2025-05-03 15:47:15,357:INFO:                 pip: 25.0
2025-05-03 15:47:15,357:INFO:          setuptools: 75.8.0
2025-05-03 15:47:15,357:INFO:             pycaret: 3.3.2
2025-05-03 15:47:15,357:INFO:             IPython: 8.32.0
2025-05-03 15:47:15,357:INFO:          ipywidgets: 8.1.6
2025-05-03 15:47:15,357:INFO:                tqdm: 4.67.1
2025-05-03 15:47:15,357:INFO:               numpy: 1.26.4
2025-05-03 15:47:15,357:INFO:              pandas: 2.1.4
2025-05-03 15:47:15,357:INFO:              jinja2: 3.1.6
2025-05-03 15:47:15,357:INFO:               scipy: 1.11.4
2025-05-03 15:47:15,357:INFO:              joblib: 1.3.2
2025-05-03 15:47:15,357:INFO:             sklearn: 1.4.2
2025-05-03 15:47:15,357:INFO:                pyod: 2.0.5
2025-05-03 15:47:15,357:INFO:            imblearn: 0.13.0
2025-05-03 15:47:15,357:INFO:   category_encoders: 2.7.0
2025-05-03 15:47:15,357:INFO:            lightgbm: 4.6.0
2025-05-03 15:47:15,357:INFO:               numba: 0.61.0
2025-05-03 15:47:15,357:INFO:            requests: 2.32.3
2025-05-03 15:47:15,357:INFO:          matplotlib: 3.7.5
2025-05-03 15:47:15,357:INFO:          scikitplot: 0.3.7
2025-05-03 15:47:15,357:INFO:         yellowbrick: 1.5
2025-05-03 15:47:15,357:INFO:              plotly: 5.24.1
2025-05-03 15:47:15,357:INFO:    plotly-resampler: Not installed
2025-05-03 15:47:15,357:INFO:             kaleido: 0.2.1
2025-05-03 15:47:15,357:INFO:           schemdraw: 0.15
2025-05-03 15:47:15,357:INFO:         statsmodels: 0.14.4
2025-05-03 15:47:15,357:INFO:              sktime: 0.26.0
2025-05-03 15:47:15,357:INFO:               tbats: 1.1.3
2025-05-03 15:47:15,357:INFO:            pmdarima: 2.0.4
2025-05-03 15:47:15,357:INFO:              psutil: 6.1.1
2025-05-03 15:47:15,357:INFO:          markupsafe: 3.0.2
2025-05-03 15:47:15,357:INFO:             pickle5: Not installed
2025-05-03 15:47:15,357:INFO:         cloudpickle: 3.1.1
2025-05-03 15:47:15,357:INFO:         deprecation: 2.1.0
2025-05-03 15:47:15,357:INFO:              xxhash: 3.5.0
2025-05-03 15:47:15,357:INFO:           wurlitzer: Not installed
2025-05-03 15:47:15,357:INFO:PyCaret optional dependencies:
2025-05-03 15:47:15,809:INFO:                shap: 0.47.2
2025-05-03 15:47:15,809:INFO:           interpret: 0.6.10
2025-05-03 15:47:15,809:INFO:                umap: Not installed
2025-05-03 15:47:15,809:INFO:     ydata_profiling: Not installed
2025-05-03 15:47:15,809:INFO:  explainerdashboard: Not installed
2025-05-03 15:47:15,809:INFO:             autoviz: Not installed
2025-05-03 15:47:15,809:INFO:           fairlearn: Not installed
2025-05-03 15:47:15,809:INFO:          deepchecks: Not installed
2025-05-03 15:47:15,809:INFO:             xgboost: 3.0.0
2025-05-03 15:47:15,809:INFO:            catboost: Not installed
2025-05-03 15:47:15,809:INFO:              kmodes: Not installed
2025-05-03 15:47:15,809:INFO:             mlxtend: Not installed
2025-05-03 15:47:15,809:INFO:       statsforecast: Not installed
2025-05-03 15:47:15,809:INFO:        tune_sklearn: Not installed
2025-05-03 15:47:15,809:INFO:                 ray: Not installed
2025-05-03 15:47:15,809:INFO:            hyperopt: 0.2.7
2025-05-03 15:47:15,809:INFO:              optuna: Not installed
2025-05-03 15:47:15,809:INFO:               skopt: 0.10.2
2025-05-03 15:47:15,809:INFO:              mlflow: 2.22.0
2025-05-03 15:47:15,809:INFO:              gradio: Not installed
2025-05-03 15:47:15,809:INFO:             fastapi: 0.115.12
2025-05-03 15:47:15,809:INFO:             uvicorn: 0.34.2
2025-05-03 15:47:15,809:INFO:              m2cgen: Not installed
2025-05-03 15:47:15,809:INFO:           evidently: Not installed
2025-05-03 15:47:15,809:INFO:               fugue: Not installed
2025-05-03 15:47:15,809:INFO:           streamlit: Not installed
2025-05-03 15:47:15,809:INFO:             prophet: Not installed
2025-05-03 15:47:15,816:INFO:None
2025-05-03 15:47:15,816:INFO:Set up data.
2025-05-03 15:47:15,832:INFO:Set up folding strategy.
2025-05-03 15:47:15,832:INFO:Set up train/test split.
2025-05-03 15:47:15,856:INFO:Set up index.
2025-05-03 15:47:15,856:INFO:Assigning column types.
2025-05-03 15:47:15,873:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 15:47:15,924:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 15:47:15,924:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 15:47:15,962:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:15,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,020:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 15:47:16,020:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 15:47:16,080:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,086:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 15:47:16,145:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 15:47:16,173:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,230:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 15:47:16,262:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,262:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 15:47:16,331:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,331:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,414:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,423:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,431:INFO:Set up column name cleaning.
2025-05-03 15:47:16,449:INFO:Finished creating preprocessing pipeline.
2025-05-03 15:47:16,456:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 15:47:16,456:INFO:Creating final display dataframe.
2025-05-03 15:47:16,567:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 15:47:16,646:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,646:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,723:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 15:47:16,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 15:47:16,732:INFO:setup() successfully completed in 1.39s...............
2025-05-03 15:47:16,748:INFO:Initializing compare_models()
2025-05-03 15:47:16,748:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 15:47:16,748:INFO:Checking exceptions
2025-05-03 15:47:16,756:INFO:Preparing display monitor
2025-05-03 15:47:16,794:INFO:Initializing Logistic Regression
2025-05-03 15:47:16,794:INFO:Total runtime is 0.0 minutes
2025-05-03 15:47:16,799:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:16,799:INFO:Initializing create_model()
2025-05-03 15:47:16,799:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:16,799:INFO:Checking exceptions
2025-05-03 15:47:16,799:INFO:Importing libraries
2025-05-03 15:47:16,799:INFO:Copying training dataset
2025-05-03 15:47:16,816:INFO:Defining folds
2025-05-03 15:47:16,816:INFO:Declaring metric variables
2025-05-03 15:47:16,816:INFO:Importing untrained model
2025-05-03 15:47:16,830:INFO:Logistic Regression Imported successfully
2025-05-03 15:47:16,838:INFO:Starting cross validation
2025-05-03 15:47:16,838:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:21,717:INFO:Calculating mean and std
2025-05-03 15:47:21,718:INFO:Creating metrics dataframe
2025-05-03 15:47:21,721:INFO:Uploading results into container
2025-05-03 15:47:21,724:INFO:Uploading model into container now
2025-05-03 15:47:21,724:INFO:_master_model_container: 1
2025-05-03 15:47:21,724:INFO:_display_container: 2
2025-05-03 15:47:21,724:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 15:47:21,724:INFO:create_model() successfully completed......................................
2025-05-03 15:47:21,869:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:21,869:INFO:Creating metrics dataframe
2025-05-03 15:47:21,879:INFO:Initializing Random Forest Classifier
2025-05-03 15:47:21,888:INFO:Total runtime is 0.08490749994913736 minutes
2025-05-03 15:47:21,893:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:21,893:INFO:Initializing create_model()
2025-05-03 15:47:21,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:21,893:INFO:Checking exceptions
2025-05-03 15:47:21,893:INFO:Importing libraries
2025-05-03 15:47:21,893:INFO:Copying training dataset
2025-05-03 15:47:21,909:INFO:Defining folds
2025-05-03 15:47:21,909:INFO:Declaring metric variables
2025-05-03 15:47:21,919:INFO:Importing untrained model
2025-05-03 15:47:21,923:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:47:21,931:INFO:Starting cross validation
2025-05-03 15:47:21,931:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:26,996:INFO:Calculating mean and std
2025-05-03 15:47:26,996:INFO:Creating metrics dataframe
2025-05-03 15:47:26,996:INFO:Uploading results into container
2025-05-03 15:47:26,996:INFO:Uploading model into container now
2025-05-03 15:47:26,996:INFO:_master_model_container: 2
2025-05-03 15:47:26,996:INFO:_display_container: 2
2025-05-03 15:47:26,996:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 15:47:26,996:INFO:create_model() successfully completed......................................
2025-05-03 15:47:27,140:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:27,140:INFO:Creating metrics dataframe
2025-05-03 15:47:27,157:INFO:Initializing Extreme Gradient Boosting
2025-05-03 15:47:27,157:INFO:Total runtime is 0.17272348801294962 minutes
2025-05-03 15:47:27,159:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:27,159:INFO:Initializing create_model()
2025-05-03 15:47:27,159:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:27,159:INFO:Checking exceptions
2025-05-03 15:47:27,159:INFO:Importing libraries
2025-05-03 15:47:27,159:INFO:Copying training dataset
2025-05-03 15:47:27,181:INFO:Defining folds
2025-05-03 15:47:27,181:INFO:Declaring metric variables
2025-05-03 15:47:27,181:INFO:Importing untrained model
2025-05-03 15:47:27,194:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 15:47:27,201:INFO:Starting cross validation
2025-05-03 15:47:27,201:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:30,957:INFO:Calculating mean and std
2025-05-03 15:47:30,957:INFO:Creating metrics dataframe
2025-05-03 15:47:30,962:INFO:Uploading results into container
2025-05-03 15:47:30,962:INFO:Uploading model into container now
2025-05-03 15:47:30,964:INFO:_master_model_container: 3
2025-05-03 15:47:30,964:INFO:_display_container: 2
2025-05-03 15:47:30,966:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 15:47:30,966:INFO:create_model() successfully completed......................................
2025-05-03 15:47:31,101:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:31,101:INFO:Creating metrics dataframe
2025-05-03 15:47:31,101:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 15:47:31,101:INFO:Total runtime is 0.23845053513844808 minutes
2025-05-03 15:47:31,115:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:31,117:INFO:Initializing create_model()
2025-05-03 15:47:31,117:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:31,117:INFO:Checking exceptions
2025-05-03 15:47:31,117:INFO:Importing libraries
2025-05-03 15:47:31,117:INFO:Copying training dataset
2025-05-03 15:47:31,133:INFO:Defining folds
2025-05-03 15:47:31,133:INFO:Declaring metric variables
2025-05-03 15:47:31,139:INFO:Importing untrained model
2025-05-03 15:47:31,139:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 15:47:31,148:INFO:Starting cross validation
2025-05-03 15:47:31,153:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:31,623:INFO:Calculating mean and std
2025-05-03 15:47:31,623:INFO:Creating metrics dataframe
2025-05-03 15:47:31,627:INFO:Uploading results into container
2025-05-03 15:47:31,627:INFO:Uploading model into container now
2025-05-03 15:47:31,627:INFO:_master_model_container: 4
2025-05-03 15:47:31,629:INFO:_display_container: 2
2025-05-03 15:47:31,630:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:47:31,630:INFO:create_model() successfully completed......................................
2025-05-03 15:47:31,774:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:31,774:INFO:Creating metrics dataframe
2025-05-03 15:47:31,786:INFO:Initializing Extra Trees Classifier
2025-05-03 15:47:31,786:INFO:Total runtime is 0.24986775318781534 minutes
2025-05-03 15:47:31,792:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:31,792:INFO:Initializing create_model()
2025-05-03 15:47:31,792:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:31,792:INFO:Checking exceptions
2025-05-03 15:47:31,792:INFO:Importing libraries
2025-05-03 15:47:31,792:INFO:Copying training dataset
2025-05-03 15:47:31,821:INFO:Defining folds
2025-05-03 15:47:31,821:INFO:Declaring metric variables
2025-05-03 15:47:31,830:INFO:Importing untrained model
2025-05-03 15:47:31,830:INFO:Extra Trees Classifier Imported successfully
2025-05-03 15:47:31,838:INFO:Starting cross validation
2025-05-03 15:47:31,838:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:33,039:INFO:Calculating mean and std
2025-05-03 15:47:33,039:INFO:Creating metrics dataframe
2025-05-03 15:47:33,042:INFO:Uploading results into container
2025-05-03 15:47:33,042:INFO:Uploading model into container now
2025-05-03 15:47:33,044:INFO:_master_model_container: 5
2025-05-03 15:47:33,044:INFO:_display_container: 2
2025-05-03 15:47:33,044:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 15:47:33,045:INFO:create_model() successfully completed......................................
2025-05-03 15:47:33,172:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:33,172:INFO:Creating metrics dataframe
2025-05-03 15:47:33,186:INFO:Initializing Ridge Classifier
2025-05-03 15:47:33,186:INFO:Total runtime is 0.27320804198582965 minutes
2025-05-03 15:47:33,200:INFO:SubProcess create_model() called ==================================
2025-05-03 15:47:33,200:INFO:Initializing create_model()
2025-05-03 15:47:33,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E303BF6FD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:33,200:INFO:Checking exceptions
2025-05-03 15:47:33,200:INFO:Importing libraries
2025-05-03 15:47:33,200:INFO:Copying training dataset
2025-05-03 15:47:33,222:INFO:Defining folds
2025-05-03 15:47:33,222:INFO:Declaring metric variables
2025-05-03 15:47:33,230:INFO:Importing untrained model
2025-05-03 15:47:33,237:INFO:Ridge Classifier Imported successfully
2025-05-03 15:47:33,237:INFO:Starting cross validation
2025-05-03 15:47:33,237:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:33,346:INFO:Calculating mean and std
2025-05-03 15:47:33,347:INFO:Creating metrics dataframe
2025-05-03 15:47:33,350:INFO:Uploading results into container
2025-05-03 15:47:33,351:INFO:Uploading model into container now
2025-05-03 15:47:33,351:INFO:_master_model_container: 6
2025-05-03 15:47:33,351:INFO:_display_container: 2
2025-05-03 15:47:33,351:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 15:47:33,351:INFO:create_model() successfully completed......................................
2025-05-03 15:47:33,492:INFO:SubProcess create_model() end ==================================
2025-05-03 15:47:33,492:INFO:Creating metrics dataframe
2025-05-03 15:47:33,505:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 15:47:33,520:INFO:Initializing create_model()
2025-05-03 15:47:33,520:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:33,520:INFO:Checking exceptions
2025-05-03 15:47:33,527:INFO:Importing libraries
2025-05-03 15:47:33,527:INFO:Copying training dataset
2025-05-03 15:47:33,546:INFO:Defining folds
2025-05-03 15:47:33,546:INFO:Declaring metric variables
2025-05-03 15:47:33,548:INFO:Importing untrained model
2025-05-03 15:47:33,548:INFO:Declaring custom model
2025-05-03 15:47:33,549:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 15:47:33,549:INFO:Cross validation set to False
2025-05-03 15:47:33,549:INFO:Fitting Model
2025-05-03 15:47:33,590:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 15:47:33,595:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001247 seconds.
2025-05-03 15:47:33,595:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 15:47:33,595:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 15:47:33,595:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 15:47:33,595:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 15:47:33,595:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 15:47:33,595:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 15:47:33,817:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:47:33,817:INFO:create_model() successfully completed......................................
2025-05-03 15:47:33,999:INFO:_master_model_container: 6
2025-05-03 15:47:34,001:INFO:_display_container: 2
2025-05-03 15:47:34,001:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:47:34,001:INFO:compare_models() successfully completed......................................
2025-05-03 15:47:34,019:INFO:Initializing create_model()
2025-05-03 15:47:34,019:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:34,019:INFO:Checking exceptions
2025-05-03 15:47:34,038:INFO:Importing libraries
2025-05-03 15:47:34,038:INFO:Copying training dataset
2025-05-03 15:47:34,063:INFO:Defining folds
2025-05-03 15:47:34,063:INFO:Declaring metric variables
2025-05-03 15:47:34,067:INFO:Importing untrained model
2025-05-03 15:47:34,067:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 15:47:34,096:INFO:Starting cross validation
2025-05-03 15:47:34,098:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:34,703:INFO:Calculating mean and std
2025-05-03 15:47:34,703:INFO:Creating metrics dataframe
2025-05-03 15:47:34,709:INFO:Finalizing model
2025-05-03 15:47:34,739:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 15:47:34,741:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.
2025-05-03 15:47:34,741:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 15:47:34,741:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 15:47:34,741:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 15:47:34,743:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 15:47:34,743:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 15:47:34,743:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 15:47:34,943:INFO:Uploading results into container
2025-05-03 15:47:34,944:INFO:Uploading model into container now
2025-05-03 15:47:34,960:INFO:_master_model_container: 7
2025-05-03 15:47:34,960:INFO:_display_container: 3
2025-05-03 15:47:34,960:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:47:34,960:INFO:create_model() successfully completed......................................
2025-05-03 15:47:35,142:INFO:Initializing create_model()
2025-05-03 15:47:35,143:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:35,143:INFO:Checking exceptions
2025-05-03 15:47:35,167:INFO:Importing libraries
2025-05-03 15:47:35,171:INFO:Copying training dataset
2025-05-03 15:47:35,200:INFO:Defining folds
2025-05-03 15:47:35,200:INFO:Declaring metric variables
2025-05-03 15:47:35,202:INFO:Importing untrained model
2025-05-03 15:47:35,208:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 15:47:35,216:INFO:Starting cross validation
2025-05-03 15:47:35,216:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:35,623:INFO:Calculating mean and std
2025-05-03 15:47:35,623:INFO:Creating metrics dataframe
2025-05-03 15:47:35,626:INFO:Finalizing model
2025-05-03 15:47:35,787:INFO:Uploading results into container
2025-05-03 15:47:35,787:INFO:Uploading model into container now
2025-05-03 15:47:35,797:INFO:_master_model_container: 8
2025-05-03 15:47:35,797:INFO:_display_container: 4
2025-05-03 15:47:35,799:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 15:47:35,799:INFO:create_model() successfully completed......................................
2025-05-03 15:47:35,967:INFO:Initializing create_model()
2025-05-03 15:47:35,967:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 15:47:35,967:INFO:Checking exceptions
2025-05-03 15:47:35,986:INFO:Importing libraries
2025-05-03 15:47:35,986:INFO:Copying training dataset
2025-05-03 15:47:36,013:INFO:Defining folds
2025-05-03 15:47:36,015:INFO:Declaring metric variables
2025-05-03 15:47:36,017:INFO:Importing untrained model
2025-05-03 15:47:36,023:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:47:36,023:INFO:Starting cross validation
2025-05-03 15:47:36,023:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:37,503:INFO:Calculating mean and std
2025-05-03 15:47:37,504:INFO:Creating metrics dataframe
2025-05-03 15:47:37,510:INFO:Finalizing model
2025-05-03 15:47:38,108:INFO:Uploading results into container
2025-05-03 15:47:38,108:INFO:Uploading model into container now
2025-05-03 15:47:38,114:INFO:_master_model_container: 9
2025-05-03 15:47:38,114:INFO:_display_container: 5
2025-05-03 15:47:38,114:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 15:47:38,114:INFO:create_model() successfully completed......................................
2025-05-03 15:47:38,280:INFO:Initializing create_model()
2025-05-03 15:47:38,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 15:47:38,280:INFO:Checking exceptions
2025-05-03 15:47:38,303:INFO:Importing libraries
2025-05-03 15:47:38,303:INFO:Copying training dataset
2025-05-03 15:47:38,334:INFO:Defining folds
2025-05-03 15:47:38,334:INFO:Declaring metric variables
2025-05-03 15:47:38,341:INFO:Importing untrained model
2025-05-03 15:47:38,345:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 15:47:38,352:INFO:Starting cross validation
2025-05-03 15:47:38,353:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:38,898:INFO:Calculating mean and std
2025-05-03 15:47:38,899:INFO:Creating metrics dataframe
2025-05-03 15:47:38,905:INFO:Finalizing model
2025-05-03 15:47:38,918:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 15:47:38,918:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 15:47:38,918:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 15:47:38,937:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 15:47:38,937:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 15:47:38,937:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 15:47:38,938:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 15:47:38,942:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000948 seconds.
2025-05-03 15:47:38,942:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 15:47:38,942:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 15:47:38,942:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 15:47:38,942:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 15:47:38,942:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 15:47:38,942:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 15:47:38,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:38,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,073:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,077:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,108:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,112:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,112:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,114:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,114:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,114:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,142:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,142:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,144:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,146:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,156:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,156:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,158:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,158:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,159:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,164:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,171:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,171:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 15:47:39,173:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 15:47:39,185:INFO:Uploading results into container
2025-05-03 15:47:39,185:INFO:Uploading model into container now
2025-05-03 15:47:39,205:INFO:_master_model_container: 10
2025-05-03 15:47:39,205:INFO:_display_container: 6
2025-05-03 15:47:39,205:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 15:47:39,206:INFO:create_model() successfully completed......................................
2025-05-03 15:47:39,352:INFO:Initializing create_model()
2025-05-03 15:47:39,352:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 15:47:39,352:INFO:Checking exceptions
2025-05-03 15:47:39,369:INFO:Importing libraries
2025-05-03 15:47:39,369:INFO:Copying training dataset
2025-05-03 15:47:39,398:INFO:Defining folds
2025-05-03 15:47:39,398:INFO:Declaring metric variables
2025-05-03 15:47:39,403:INFO:Importing untrained model
2025-05-03 15:47:39,408:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 15:47:39,419:INFO:Starting cross validation
2025-05-03 15:47:39,419:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:39,911:INFO:Calculating mean and std
2025-05-03 15:47:39,911:INFO:Creating metrics dataframe
2025-05-03 15:47:39,913:INFO:Finalizing model
2025-05-03 15:47:40,093:INFO:Uploading results into container
2025-05-03 15:47:40,095:INFO:Uploading model into container now
2025-05-03 15:47:40,107:INFO:_master_model_container: 11
2025-05-03 15:47:40,107:INFO:_display_container: 7
2025-05-03 15:47:40,109:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 15:47:40,109:INFO:create_model() successfully completed......................................
2025-05-03 15:47:40,252:INFO:Initializing create_model()
2025-05-03 15:47:40,252:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 15:47:40,252:INFO:Checking exceptions
2025-05-03 15:47:40,276:INFO:Importing libraries
2025-05-03 15:47:40,276:INFO:Copying training dataset
2025-05-03 15:47:40,309:INFO:Defining folds
2025-05-03 15:47:40,309:INFO:Declaring metric variables
2025-05-03 15:47:40,317:INFO:Importing untrained model
2025-05-03 15:47:40,321:INFO:Random Forest Classifier Imported successfully
2025-05-03 15:47:40,333:INFO:Starting cross validation
2025-05-03 15:47:40,333:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 15:47:44,352:INFO:Calculating mean and std
2025-05-03 15:47:44,352:INFO:Creating metrics dataframe
2025-05-03 15:47:44,352:INFO:Finalizing model
2025-05-03 15:47:46,730:INFO:Uploading results into container
2025-05-03 15:47:46,730:INFO:Uploading model into container now
2025-05-03 15:47:46,735:INFO:_master_model_container: 12
2025-05-03 15:47:46,735:INFO:_display_container: 8
2025-05-03 15:47:46,735:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 15:47:46,735:INFO:create_model() successfully completed......................................
2025-05-03 15:47:46,925:INFO:Initializing interpret_model()
2025-05-03 15:47:46,925:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:47:46,925:INFO:Checking exceptions
2025-05-03 15:47:46,925:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:47:46,982:INFO:plot type: msa
2025-05-03 15:47:47,455:INFO:Visual Rendered Successfully
2025-05-03 15:47:47,455:INFO:interpret_model() successfully completed......................................
2025-05-03 15:47:47,829:INFO:Initializing interpret_model()
2025-05-03 15:47:47,829:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:47:47,829:INFO:Checking exceptions
2025-05-03 15:47:47,829:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:47:47,857:INFO:plot type: msa
2025-05-03 15:47:48,141:INFO:Visual Rendered Successfully
2025-05-03 15:47:48,141:INFO:interpret_model() successfully completed......................................
2025-05-03 15:47:51,522:INFO:Initializing interpret_model()
2025-05-03 15:47:51,522:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:47:51,522:INFO:Checking exceptions
2025-05-03 15:47:51,522:INFO:Soft dependency imported: interpret: 0.6.10
2025-05-03 15:47:51,545:INFO:plot type: msa
2025-05-03 15:47:51,545:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 15:47:51,825:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 15:47:51,946:INFO:Visual Rendered Successfully
2025-05-03 15:47:51,946:INFO:interpret_model() successfully completed......................................
2025-05-03 15:47:55,515:INFO:Initializing save_model()
2025-05-03 15:47:55,515:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:47:55,515:INFO:Adding model into prep_pipe
2025-05-03 15:47:55,524:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 15:47:55,528:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 15:47:55,528:INFO:save_model() successfully completed......................................
2025-05-03 15:47:55,683:INFO:Initializing save_model()
2025-05-03 15:47:55,683:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:47:55,683:INFO:Adding model into prep_pipe
2025-05-03 15:47:55,689:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 15:47:55,695:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 15:47:55,695:INFO:save_model() successfully completed......................................
2025-05-03 15:47:55,838:INFO:Initializing save_model()
2025-05-03 15:47:55,838:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:47:55,838:INFO:Adding model into prep_pipe
2025-05-03 15:47:55,987:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 15:47:55,989:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 15:47:55,989:INFO:save_model() successfully completed......................................
2025-05-03 15:48:04,539:INFO:Initializing interpret_model()
2025-05-03 15:48:04,539:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:48:04,539:INFO:Checking exceptions
2025-05-03 15:48:04,539:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:48:04,578:INFO:plot type: summary
2025-05-03 15:48:04,578:INFO:Creating TreeExplainer
2025-05-03 15:48:04,625:INFO:Compiling shap values
2025-05-03 15:48:05,471:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 15:48:05,471:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:48:05,808:INFO:Visual Rendered Successfully
2025-05-03 15:48:05,808:INFO:interpret_model() successfully completed......................................
2025-05-03 15:48:05,961:INFO:Initializing interpret_model()
2025-05-03 15:48:05,961:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:48:05,961:INFO:Checking exceptions
2025-05-03 15:48:05,961:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:48:06,027:INFO:plot type: summary
2025-05-03 15:48:06,027:INFO:Creating TreeExplainer
2025-05-03 15:48:06,099:INFO:Compiling shap values
2025-05-03 15:48:06,919:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:583: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 15:48:06,924:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:48:08,739:INFO:Visual Rendered Successfully
2025-05-03 15:48:08,739:INFO:interpret_model() successfully completed......................................
2025-05-03 15:48:15,353:INFO:Initializing interpret_model()
2025-05-03 15:48:15,353:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 15:48:15,353:INFO:Checking exceptions
2025-05-03 15:48:15,353:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:48:15,388:INFO:plot type: summary
2025-05-03 15:48:15,388:INFO:Creating TreeExplainer
2025-05-03 15:48:15,418:INFO:Compiling shap values
2025-05-03 15:48:17,128:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:48:17,352:INFO:Visual Rendered Successfully
2025-05-03 15:48:17,364:INFO:interpret_model() successfully completed......................................
2025-05-03 15:48:17,496:INFO:Initializing interpret_model()
2025-05-03 15:48:17,496:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:48:17,496:INFO:Checking exceptions
2025-05-03 15:48:17,496:INFO:Soft dependency imported: shap: 0.47.2
2025-05-03 15:48:17,523:INFO:plot type: summary
2025-05-03 15:48:17,523:INFO:Creating TreeExplainer
2025-05-03 15:48:17,558:INFO:Compiling shap values
2025-05-03 15:48:19,221:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:4145: FutureWarning:

The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.


2025-05-03 15:48:21,088:INFO:Visual Rendered Successfully
2025-05-03 15:48:21,088:INFO:interpret_model() successfully completed......................................
2025-05-03 15:48:22,627:INFO:Initializing interpret_model()
2025-05-03 15:48:22,627:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:48:22,627:INFO:Checking exceptions
2025-05-03 15:50:31,811:INFO:Initializing interpret_model()
2025-05-03 15:50:31,813:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:50:31,813:INFO:Checking exceptions
2025-05-03 15:50:31,813:ERROR:
'interpret_community' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret-community` to install.
NoneType: None
2025-05-03 15:52:17,645:INFO:Initializing interpret_model()
2025-05-03 15:52:17,645:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:52:17,645:INFO:Checking exceptions
2025-05-03 15:52:17,645:ERROR:
'interpret_community' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret-community` to install.
NoneType: None
2025-05-03 15:53:07,239:INFO:Initializing interpret_model()
2025-05-03 15:53:07,239:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
37272                      2.0           1.0               0.160733   
1912                       0.0           1.0               0.416784   
27220                      4.0           1.0               0.416784   
33245                      4.0           1.0               0.160733   
27732                      2.0           1.0               0.160733   
...                        ...           ...                    ...   
37257                      4.0           1.0               0.251061   
9162                       4.0           1.0               0.160733   
28320                      2.0           1.0               0.416784   
5343                       2.0           1.0               0.160733   
34219                      4.0           1.0               0.058923   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
37272                0.313750                  0.014036          0.254406   
1912                 0.225234                  0.102740          0.254406   
27220                0.069795                  0.448484          0.254406   
33245                0.476895                  0.448484          0.254406   
27732                0.069795                  0.014036          0.123228   
...                       ...                       ...               ...   
37257                0.225234                  0.448484          0.254406   
9162                 0.225234                  0.448484          0.254406   
28320                0.225234                  0.102740          0.123228   
5343                 0.137670                  0.014036          0.254406   
34219                0.042973                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
37272               0.218236               0.341185                 -0.521963   
1912                0.218236               1.358948                  1.160003   
27220               0.218236               0.722214                  0.690614   
33245               0.559187               1.543108                  0.529510   
27732               0.218236               0.201963                  0.901736   
...                      ...                    ...                       ...   
37257               0.218236               0.600771                 -0.713829   
9162                0.218236               0.055599                 -0.663776   
28320               0.281607               0.201963                  1.171790   
5343                0.218236              -0.261781                 -0.552830   
34219               0.218236               0.055599                 -1.134792   

       scaler_only__educational-num  scaler_only__hours-per-week  \
37272                     -0.418689                    -0.034370   
1912                       1.136723                     0.768345   
27220                      1.136723                    -0.034370   
33245                     -0.418689                    -0.034370   
27732                     -0.418689                    -0.034370   
...                             ...                          ...   
37257                      0.359017                    -0.034370   
9162                      -0.418689                     0.366987   
28320                      1.136723                     1.571059   
5343                      -0.418689                    -0.034370   
34219                     -1.974101                    -0.034370   

       binary__has_capital_gain  binary__has_capital_loss  \
37272                       0.0                       0.0   
1912                        0.0                       0.0   
27220                       0.0                       0.0   
33245                       0.0                       0.0   
27732                       0.0                       0.0   
...                         ...                       ...   
37257                       0.0                       0.0   
9162                        1.0                       0.0   
28320                       0.0                       1.0   
5343                        0.0                       0.0   
34219                       1.0                       0.0   

       binary__es_estadounidense  
37272                        1.0  
1912                         1.0  
27220                        1.0  
33245                        1.0  
27732                        1.0  
...                          ...  
37257                        0.0  
9162                         1.0  
28320                        0.0  
5343                         1.0  
34219                        1.0  

[500 rows x 14 columns], y_new_sample=       income
37272       0
1912        0
27220       0
33245       1
27732       0
...       ...
37257       1
9162        1
28320       0
5343        0
34219       0

[500 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:53:07,239:INFO:Checking exceptions
2025-05-03 15:53:12,746:INFO:Initializing interpret_model()
2025-05-03 15:53:12,746:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:53:12,746:INFO:Checking exceptions
2025-05-03 15:53:24,611:INFO:Initializing interpret_model()
2025-05-03 15:53:24,611:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=bar, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 15:53:24,612:INFO:Checking exceptions
2025-05-03 15:54:32,207:INFO:Initializing save_model()
2025-05-03 15:54:32,207:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 15:54:32,207:INFO:Adding model into prep_pipe
2025-05-03 15:54:32,373:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 15:54:32,402:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 15:54:32,402:INFO:save_model() successfully completed......................................
2025-05-03 15:54:32,987:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:97: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 15:54:32,987:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:132: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 15:59:36,972:INFO:Initializing interpret_model()
2025-05-03 15:59:36,972:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E37F73BDD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
0                          4.0           1.0               0.191912   
1                          4.0           1.0               0.062443   
2                          4.0           1.0               0.160733   
3                          0.0           0.0               0.160733   
4                          0.0           1.0               0.251061   
...                        ...           ...                    ...   
39068                     -1.0           1.0               0.160733   
39069                      4.0           1.0               0.191912   
39070                      2.0           0.0               0.191912   
39071                      1.0           0.0               0.251061   
39072                      4.0           1.0               0.548190   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
0                    0.124794                  0.448484          0.254406   
1                    0.225234                  0.448484          0.254406   
2                    0.225234                  0.448484          0.254406   
3                    0.137670                  0.102740          0.254406   
4                    0.207173                  0.102740          0.254406   
...                       ...                       ...               ...   
39068                0.042973                  0.102740          0.123228   
39069                0.225234                  0.448484          0.254406   
39070                0.476895                  0.102740          0.254406   
39071                0.137670                  0.060844          0.123228   
39072                0.458596                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
0                   0.218236               0.408328                 -0.155278   
1                   0.281607               1.004967                  0.145313   
2                   0.293717              -0.179066                  0.123446   
3                   0.218236              -0.715668                 -0.385638   
4                   0.218236               0.662138                 -2.776625   
...                      ...                    ...                       ...   
39068               0.218236              -0.020498                  1.862292   
39069               0.218236              -0.179066                 -0.636347   
39070               0.218236              -0.715668                  0.719212   
39071               0.269962               0.662138                 -0.809178   
39072               0.218236               1.358948                  0.155282   

       scaler_only__educational-num  scaler_only__hours-per-week  \
0                         -0.029836                    -0.034370   
1                         -1.585248                    -0.435727   
2                         -0.418689                    -0.034370   
3                         -0.418689                    -0.034370   
4                          0.359017                     0.768345   
...                             ...                          ...   
39068                     -0.418689                    -0.034370   
39069                     -0.029836                     0.768345   
39070                     -0.029836                    -0.034370   
39071                      0.359017                    -0.034370   
39072                      1.525576                    -2.683328   

       binary__has_capital_gain  binary__has_capital_loss  \
0                           0.0                       0.0   
1                           0.0                       0.0   
2                           0.0                       0.0   
3                           0.0                       0.0   
4                           0.0                       0.0   
...                         ...                       ...   
39068                       0.0                       0.0   
39069                       0.0                       0.0   
39070                       0.0                       0.0   
39071                       0.0                       0.0   
39072                       0.0                       0.0   

       binary__es_estadounidense  
0                            1.0  
1                            1.0  
2                            1.0  
3                            1.0  
4                            1.0  
...                          ...  
39068                        1.0  
39069                        1.0  
39070                        1.0  
39071                        1.0  
39072                        1.0  

[39073 rows x 14 columns], y_new_sample=       income
0           0
1           0
2           0
3           0
4           0
...       ...
39068       0
39069       0
39070       0
39071       0
39072       1

[39073 rows x 1 columns], save=False, kwargs={})
2025-05-03 15:59:36,972:INFO:Checking exceptions
2025-05-03 15:59:36,972:ERROR:
'interpret_community' is a soft dependency and not included in the pycaret installation. Please run: `pip install interpret-community` to install.
NoneType: None
2025-05-03 16:03:13,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:03:13,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:03:13,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:03:13,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:03:17,929:INFO:PyCaret ClassificationExperiment
2025-05-03 16:03:17,931:INFO:Logging name: clf-default-name
2025-05-03 16:03:17,931:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 16:03:17,931:INFO:version 3.3.2
2025-05-03 16:03:17,931:INFO:Initializing setup()
2025-05-03 16:03:17,931:INFO:self.USI: 546f
2025-05-03 16:03:17,931:INFO:self._variable_keys: {'y', 'USI', 'X_train', 'log_plots_param', 'X', 'logging_param', 'X_test', 'is_multiclass', 'seed', 'gpu_param', 'exp_name_log', 'exp_id', '_ml_usecase', 'gpu_n_jobs_param', 'target_param', 'n_jobs_param', 'y_test', 'y_train', 'idx', 'fold_shuffle_param', 'pipeline', 'fold_generator', '_available_plots', 'fold_groups_param', 'fix_imbalance', 'data', 'memory', 'html_param'}
2025-05-03 16:03:17,931:INFO:Checking environment
2025-05-03 16:03:17,931:INFO:python_version: 3.11.11
2025-05-03 16:03:17,931:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 16:03:17,931:INFO:machine: AMD64
2025-05-03 16:03:17,931:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 16:03:17,938:INFO:Memory: svmem(total=16965230592, available=3957084160, percent=76.7, used=13008146432, free=3957084160)
2025-05-03 16:03:17,938:INFO:Physical Core: 4
2025-05-03 16:03:17,938:INFO:Logical Core: 8
2025-05-03 16:03:17,938:INFO:Checking libraries
2025-05-03 16:03:17,938:INFO:System:
2025-05-03 16:03:17,938:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 16:03:17,938:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 16:03:17,938:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 16:03:17,938:INFO:PyCaret required dependencies:
2025-05-03 16:03:17,940:INFO:                 pip: 25.0
2025-05-03 16:03:17,940:INFO:          setuptools: 75.8.0
2025-05-03 16:03:17,940:INFO:             pycaret: 3.3.2
2025-05-03 16:03:17,940:INFO:             IPython: 8.32.0
2025-05-03 16:03:17,940:INFO:          ipywidgets: 8.1.6
2025-05-03 16:03:17,940:INFO:                tqdm: 4.67.1
2025-05-03 16:03:17,940:INFO:               numpy: 1.26.4
2025-05-03 16:03:17,941:INFO:              pandas: 2.1.4
2025-05-03 16:03:17,941:INFO:              jinja2: 3.1.6
2025-05-03 16:03:17,941:INFO:               scipy: 1.11.4
2025-05-03 16:03:17,941:INFO:              joblib: 1.3.2
2025-05-03 16:03:17,941:INFO:             sklearn: 1.4.2
2025-05-03 16:03:17,941:INFO:                pyod: 2.0.5
2025-05-03 16:03:17,941:INFO:            imblearn: 0.13.0
2025-05-03 16:03:17,941:INFO:   category_encoders: 2.7.0
2025-05-03 16:03:17,941:INFO:            lightgbm: 4.6.0
2025-05-03 16:03:17,941:INFO:               numba: 0.61.0
2025-05-03 16:03:17,941:INFO:            requests: 2.32.3
2025-05-03 16:03:17,941:INFO:          matplotlib: 3.7.5
2025-05-03 16:03:17,941:INFO:          scikitplot: 0.3.7
2025-05-03 16:03:17,941:INFO:         yellowbrick: 1.5
2025-05-03 16:03:17,941:INFO:              plotly: 5.24.1
2025-05-03 16:03:17,941:INFO:    plotly-resampler: Not installed
2025-05-03 16:03:17,941:INFO:             kaleido: 0.2.1
2025-05-03 16:03:17,941:INFO:           schemdraw: 0.15
2025-05-03 16:03:17,941:INFO:         statsmodels: 0.14.4
2025-05-03 16:03:17,941:INFO:              sktime: 0.26.0
2025-05-03 16:03:17,941:INFO:               tbats: 1.1.3
2025-05-03 16:03:17,941:INFO:            pmdarima: 2.0.4
2025-05-03 16:03:17,941:INFO:              psutil: 6.1.1
2025-05-03 16:03:17,941:INFO:          markupsafe: 3.0.2
2025-05-03 16:03:17,941:INFO:             pickle5: Not installed
2025-05-03 16:03:17,941:INFO:         cloudpickle: 3.1.1
2025-05-03 16:03:17,941:INFO:         deprecation: 2.1.0
2025-05-03 16:03:17,941:INFO:              xxhash: 3.5.0
2025-05-03 16:03:17,941:INFO:           wurlitzer: Not installed
2025-05-03 16:03:17,941:INFO:PyCaret optional dependencies:
2025-05-03 16:03:18,441:INFO:                shap: 0.46.0
2025-05-03 16:03:18,441:INFO:           interpret: 0.6.9
2025-05-03 16:03:18,441:INFO:                umap: Not installed
2025-05-03 16:03:18,441:INFO:     ydata_profiling: Not installed
2025-05-03 16:03:18,441:INFO:  explainerdashboard: Not installed
2025-05-03 16:03:18,441:INFO:             autoviz: Not installed
2025-05-03 16:03:18,441:INFO:           fairlearn: Not installed
2025-05-03 16:03:18,441:INFO:          deepchecks: Not installed
2025-05-03 16:03:18,441:INFO:             xgboost: 3.0.0
2025-05-03 16:03:18,441:INFO:            catboost: Not installed
2025-05-03 16:03:18,441:INFO:              kmodes: Not installed
2025-05-03 16:03:18,441:INFO:             mlxtend: Not installed
2025-05-03 16:03:18,441:INFO:       statsforecast: Not installed
2025-05-03 16:03:18,441:INFO:        tune_sklearn: Not installed
2025-05-03 16:03:18,441:INFO:                 ray: Not installed
2025-05-03 16:03:18,441:INFO:            hyperopt: 0.2.7
2025-05-03 16:03:18,441:INFO:              optuna: Not installed
2025-05-03 16:03:18,441:INFO:               skopt: 0.10.2
2025-05-03 16:03:18,441:INFO:              mlflow: 2.22.0
2025-05-03 16:03:18,441:INFO:              gradio: Not installed
2025-05-03 16:03:18,441:INFO:             fastapi: 0.115.12
2025-05-03 16:03:18,441:INFO:             uvicorn: 0.34.2
2025-05-03 16:03:18,441:INFO:              m2cgen: Not installed
2025-05-03 16:03:18,441:INFO:           evidently: Not installed
2025-05-03 16:03:18,441:INFO:               fugue: Not installed
2025-05-03 16:03:18,441:INFO:           streamlit: Not installed
2025-05-03 16:03:18,441:INFO:             prophet: Not installed
2025-05-03 16:03:18,441:INFO:None
2025-05-03 16:03:18,441:INFO:Set up data.
2025-05-03 16:03:18,459:INFO:Set up folding strategy.
2025-05-03 16:03:18,459:INFO:Set up train/test split.
2025-05-03 16:03:18,486:INFO:Set up index.
2025-05-03 16:03:18,491:INFO:Assigning column types.
2025-05-03 16:03:18,502:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 16:03:18,541:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,550:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,575:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:18,575:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:18,642:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,642:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,675:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:18,685:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:18,688:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 16:03:18,757:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,823:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:18,825:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:18,906:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:03:18,952:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:18,957:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:18,957:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 16:03:19,050:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:19,050:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:19,140:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:19,156:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:19,158:INFO:Set up column name cleaning.
2025-05-03 16:03:19,185:INFO:Finished creating preprocessing pipeline.
2025-05-03 16:03:19,190:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 16:03:19,190:INFO:Creating final display dataframe.
2025-05-03 16:03:19,345:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 16:03:19,450:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:19,458:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:19,573:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:03:19,584:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:03:19,586:INFO:setup() successfully completed in 1.66s...............
2025-05-03 16:03:19,603:INFO:Initializing compare_models()
2025-05-03 16:03:19,603:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 16:03:19,604:INFO:Checking exceptions
2025-05-03 16:03:19,620:INFO:Preparing display monitor
2025-05-03 16:03:19,648:INFO:Initializing Logistic Regression
2025-05-03 16:03:19,648:INFO:Total runtime is 0.0 minutes
2025-05-03 16:03:19,653:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:19,653:INFO:Initializing create_model()
2025-05-03 16:03:19,653:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:19,653:INFO:Checking exceptions
2025-05-03 16:03:19,653:INFO:Importing libraries
2025-05-03 16:03:19,653:INFO:Copying training dataset
2025-05-03 16:03:19,675:INFO:Defining folds
2025-05-03 16:03:19,675:INFO:Declaring metric variables
2025-05-03 16:03:19,681:INFO:Importing untrained model
2025-05-03 16:03:19,681:INFO:Logistic Regression Imported successfully
2025-05-03 16:03:19,694:INFO:Starting cross validation
2025-05-03 16:03:19,694:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:24,486:INFO:Calculating mean and std
2025-05-03 16:03:24,486:INFO:Creating metrics dataframe
2025-05-03 16:03:24,489:INFO:Uploading results into container
2025-05-03 16:03:24,489:INFO:Uploading model into container now
2025-05-03 16:03:24,489:INFO:_master_model_container: 1
2025-05-03 16:03:24,489:INFO:_display_container: 2
2025-05-03 16:03:24,492:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 16:03:24,492:INFO:create_model() successfully completed......................................
2025-05-03 16:03:24,634:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:24,634:INFO:Creating metrics dataframe
2025-05-03 16:03:24,644:INFO:Initializing Random Forest Classifier
2025-05-03 16:03:24,644:INFO:Total runtime is 0.08328081766764323 minutes
2025-05-03 16:03:24,644:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:24,644:INFO:Initializing create_model()
2025-05-03 16:03:24,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:24,644:INFO:Checking exceptions
2025-05-03 16:03:24,644:INFO:Importing libraries
2025-05-03 16:03:24,644:INFO:Copying training dataset
2025-05-03 16:03:24,684:INFO:Defining folds
2025-05-03 16:03:24,684:INFO:Declaring metric variables
2025-05-03 16:03:24,696:INFO:Importing untrained model
2025-05-03 16:03:24,705:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:03:24,705:INFO:Starting cross validation
2025-05-03 16:03:24,718:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:30,121:INFO:Calculating mean and std
2025-05-03 16:03:30,121:INFO:Creating metrics dataframe
2025-05-03 16:03:30,121:INFO:Uploading results into container
2025-05-03 16:03:30,125:INFO:Uploading model into container now
2025-05-03 16:03:30,125:INFO:_master_model_container: 2
2025-05-03 16:03:30,125:INFO:_display_container: 2
2025-05-03 16:03:30,125:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:03:30,125:INFO:create_model() successfully completed......................................
2025-05-03 16:03:30,262:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:30,262:INFO:Creating metrics dataframe
2025-05-03 16:03:30,274:INFO:Initializing Extreme Gradient Boosting
2025-05-03 16:03:30,274:INFO:Total runtime is 0.17710086504618328 minutes
2025-05-03 16:03:30,280:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:30,280:INFO:Initializing create_model()
2025-05-03 16:03:30,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:30,280:INFO:Checking exceptions
2025-05-03 16:03:30,280:INFO:Importing libraries
2025-05-03 16:03:30,280:INFO:Copying training dataset
2025-05-03 16:03:30,309:INFO:Defining folds
2025-05-03 16:03:30,309:INFO:Declaring metric variables
2025-05-03 16:03:30,313:INFO:Importing untrained model
2025-05-03 16:03:30,319:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:03:30,333:INFO:Starting cross validation
2025-05-03 16:03:30,335:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:34,018:INFO:Calculating mean and std
2025-05-03 16:03:34,018:INFO:Creating metrics dataframe
2025-05-03 16:03:34,021:INFO:Uploading results into container
2025-05-03 16:03:34,021:INFO:Uploading model into container now
2025-05-03 16:03:34,023:INFO:_master_model_container: 3
2025-05-03 16:03:34,023:INFO:_display_container: 2
2025-05-03 16:03:34,023:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:03:34,023:INFO:create_model() successfully completed......................................
2025-05-03 16:03:34,157:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:34,157:INFO:Creating metrics dataframe
2025-05-03 16:03:34,161:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 16:03:34,161:INFO:Total runtime is 0.24188446203867595 minutes
2025-05-03 16:03:34,161:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:34,170:INFO:Initializing create_model()
2025-05-03 16:03:34,170:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:34,170:INFO:Checking exceptions
2025-05-03 16:03:34,170:INFO:Importing libraries
2025-05-03 16:03:34,170:INFO:Copying training dataset
2025-05-03 16:03:34,186:INFO:Defining folds
2025-05-03 16:03:34,186:INFO:Declaring metric variables
2025-05-03 16:03:34,186:INFO:Importing untrained model
2025-05-03 16:03:34,199:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:03:34,199:INFO:Starting cross validation
2025-05-03 16:03:34,199:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:34,778:INFO:Calculating mean and std
2025-05-03 16:03:34,780:INFO:Creating metrics dataframe
2025-05-03 16:03:34,784:INFO:Uploading results into container
2025-05-03 16:03:34,786:INFO:Uploading model into container now
2025-05-03 16:03:34,786:INFO:_master_model_container: 4
2025-05-03 16:03:34,786:INFO:_display_container: 2
2025-05-03 16:03:34,788:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:03:34,788:INFO:create_model() successfully completed......................................
2025-05-03 16:03:34,930:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:34,930:INFO:Creating metrics dataframe
2025-05-03 16:03:34,942:INFO:Initializing Extra Trees Classifier
2025-05-03 16:03:34,942:INFO:Total runtime is 0.25491241216659544 minutes
2025-05-03 16:03:34,946:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:34,946:INFO:Initializing create_model()
2025-05-03 16:03:34,946:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:34,946:INFO:Checking exceptions
2025-05-03 16:03:34,946:INFO:Importing libraries
2025-05-03 16:03:34,946:INFO:Copying training dataset
2025-05-03 16:03:34,965:INFO:Defining folds
2025-05-03 16:03:34,965:INFO:Declaring metric variables
2025-05-03 16:03:34,975:INFO:Importing untrained model
2025-05-03 16:03:34,980:INFO:Extra Trees Classifier Imported successfully
2025-05-03 16:03:34,988:INFO:Starting cross validation
2025-05-03 16:03:34,988:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:36,093:INFO:Calculating mean and std
2025-05-03 16:03:36,093:INFO:Creating metrics dataframe
2025-05-03 16:03:36,098:INFO:Uploading results into container
2025-05-03 16:03:36,098:INFO:Uploading model into container now
2025-05-03 16:03:36,098:INFO:_master_model_container: 5
2025-05-03 16:03:36,098:INFO:_display_container: 2
2025-05-03 16:03:36,098:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 16:03:36,098:INFO:create_model() successfully completed......................................
2025-05-03 16:03:36,244:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:36,244:INFO:Creating metrics dataframe
2025-05-03 16:03:36,260:INFO:Initializing Ridge Classifier
2025-05-03 16:03:36,260:INFO:Total runtime is 0.27687221368153886 minutes
2025-05-03 16:03:36,260:INFO:SubProcess create_model() called ==================================
2025-05-03 16:03:36,260:INFO:Initializing create_model()
2025-05-03 16:03:36,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001AD9921EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:36,266:INFO:Checking exceptions
2025-05-03 16:03:36,266:INFO:Importing libraries
2025-05-03 16:03:36,266:INFO:Copying training dataset
2025-05-03 16:03:36,283:INFO:Defining folds
2025-05-03 16:03:36,283:INFO:Declaring metric variables
2025-05-03 16:03:36,298:INFO:Importing untrained model
2025-05-03 16:03:36,298:INFO:Ridge Classifier Imported successfully
2025-05-03 16:03:36,311:INFO:Starting cross validation
2025-05-03 16:03:36,313:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:36,395:INFO:Calculating mean and std
2025-05-03 16:03:36,395:INFO:Creating metrics dataframe
2025-05-03 16:03:36,397:INFO:Uploading results into container
2025-05-03 16:03:36,397:INFO:Uploading model into container now
2025-05-03 16:03:36,397:INFO:_master_model_container: 6
2025-05-03 16:03:36,397:INFO:_display_container: 2
2025-05-03 16:03:36,397:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 16:03:36,397:INFO:create_model() successfully completed......................................
2025-05-03 16:03:36,532:INFO:SubProcess create_model() end ==================================
2025-05-03 16:03:36,532:INFO:Creating metrics dataframe
2025-05-03 16:03:36,563:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 16:03:36,578:INFO:Initializing create_model()
2025-05-03 16:03:36,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:36,578:INFO:Checking exceptions
2025-05-03 16:03:36,578:INFO:Importing libraries
2025-05-03 16:03:36,578:INFO:Copying training dataset
2025-05-03 16:03:36,599:INFO:Defining folds
2025-05-03 16:03:36,599:INFO:Declaring metric variables
2025-05-03 16:03:36,599:INFO:Importing untrained model
2025-05-03 16:03:36,599:INFO:Declaring custom model
2025-05-03 16:03:36,599:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:03:36,599:INFO:Cross validation set to False
2025-05-03 16:03:36,599:INFO:Fitting Model
2025-05-03 16:03:36,631:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:03:36,635:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000945 seconds.
2025-05-03 16:03:36,635:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:03:36,635:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:03:36,635:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:03:36,635:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:03:36,637:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:03:36,637:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:03:36,832:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:03:36,832:INFO:create_model() successfully completed......................................
2025-05-03 16:03:37,043:INFO:_master_model_container: 6
2025-05-03 16:03:37,043:INFO:_display_container: 2
2025-05-03 16:03:37,043:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:03:37,045:INFO:compare_models() successfully completed......................................
2025-05-03 16:03:37,076:INFO:Initializing create_model()
2025-05-03 16:03:37,076:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:37,076:INFO:Checking exceptions
2025-05-03 16:03:37,098:INFO:Importing libraries
2025-05-03 16:03:37,098:INFO:Copying training dataset
2025-05-03 16:03:37,128:INFO:Defining folds
2025-05-03 16:03:37,129:INFO:Declaring metric variables
2025-05-03 16:03:37,130:INFO:Importing untrained model
2025-05-03 16:03:37,138:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:03:37,150:INFO:Starting cross validation
2025-05-03 16:03:37,150:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:37,723:INFO:Calculating mean and std
2025-05-03 16:03:37,724:INFO:Creating metrics dataframe
2025-05-03 16:03:37,732:INFO:Finalizing model
2025-05-03 16:03:37,765:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:03:37,769:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.
2025-05-03 16:03:37,769:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:03:37,769:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:03:37,769:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:03:37,769:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:03:37,769:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:03:37,769:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:03:38,004:INFO:Uploading results into container
2025-05-03 16:03:38,006:INFO:Uploading model into container now
2025-05-03 16:03:38,017:INFO:_master_model_container: 7
2025-05-03 16:03:38,017:INFO:_display_container: 3
2025-05-03 16:03:38,019:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:03:38,019:INFO:create_model() successfully completed......................................
2025-05-03 16:03:38,177:INFO:Initializing create_model()
2025-05-03 16:03:38,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:38,179:INFO:Checking exceptions
2025-05-03 16:03:38,199:INFO:Importing libraries
2025-05-03 16:03:38,199:INFO:Copying training dataset
2025-05-03 16:03:38,216:INFO:Defining folds
2025-05-03 16:03:38,216:INFO:Declaring metric variables
2025-05-03 16:03:38,225:INFO:Importing untrained model
2025-05-03 16:03:38,225:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:03:38,232:INFO:Starting cross validation
2025-05-03 16:03:38,232:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:38,655:INFO:Calculating mean and std
2025-05-03 16:03:38,655:INFO:Creating metrics dataframe
2025-05-03 16:03:38,663:INFO:Finalizing model
2025-05-03 16:03:38,857:INFO:Uploading results into container
2025-05-03 16:03:38,859:INFO:Uploading model into container now
2025-05-03 16:03:38,871:INFO:_master_model_container: 8
2025-05-03 16:03:38,871:INFO:_display_container: 4
2025-05-03 16:03:38,873:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:03:38,873:INFO:create_model() successfully completed......................................
2025-05-03 16:03:39,038:INFO:Initializing create_model()
2025-05-03 16:03:39,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:03:39,038:INFO:Checking exceptions
2025-05-03 16:03:39,057:INFO:Importing libraries
2025-05-03 16:03:39,057:INFO:Copying training dataset
2025-05-03 16:03:39,082:INFO:Defining folds
2025-05-03 16:03:39,082:INFO:Declaring metric variables
2025-05-03 16:03:39,086:INFO:Importing untrained model
2025-05-03 16:03:39,091:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:03:39,092:INFO:Starting cross validation
2025-05-03 16:03:39,098:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:40,459:INFO:Calculating mean and std
2025-05-03 16:03:40,459:INFO:Creating metrics dataframe
2025-05-03 16:03:40,462:INFO:Finalizing model
2025-05-03 16:03:41,040:INFO:Uploading results into container
2025-05-03 16:03:41,040:INFO:Uploading model into container now
2025-05-03 16:03:41,055:INFO:_master_model_container: 9
2025-05-03 16:03:41,055:INFO:_display_container: 5
2025-05-03 16:03:41,055:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:03:41,055:INFO:create_model() successfully completed......................................
2025-05-03 16:03:41,204:INFO:Initializing create_model()
2025-05-03 16:03:41,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 16:03:41,204:INFO:Checking exceptions
2025-05-03 16:03:41,226:INFO:Importing libraries
2025-05-03 16:03:41,226:INFO:Copying training dataset
2025-05-03 16:03:41,249:INFO:Defining folds
2025-05-03 16:03:41,249:INFO:Declaring metric variables
2025-05-03 16:03:41,256:INFO:Importing untrained model
2025-05-03 16:03:41,256:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:03:41,271:INFO:Starting cross validation
2025-05-03 16:03:41,271:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:41,752:INFO:Calculating mean and std
2025-05-03 16:03:41,752:INFO:Creating metrics dataframe
2025-05-03 16:03:41,760:INFO:Finalizing model
2025-05-03 16:03:41,772:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:03:41,772:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:03:41,772:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:03:41,786:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:03:41,786:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:03:41,786:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:03:41,786:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:03:41,789:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.
2025-05-03 16:03:41,790:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:03:41,790:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:03:41,790:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:03:41,790:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:03:41,790:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:03:41,790:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:03:41,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,874:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,880:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,884:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,886:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,909:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,932:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:03:41,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:03:41,944:INFO:Uploading results into container
2025-05-03 16:03:41,946:INFO:Uploading model into container now
2025-05-03 16:03:41,991:INFO:_master_model_container: 10
2025-05-03 16:03:41,991:INFO:_display_container: 6
2025-05-03 16:03:41,991:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:03:41,991:INFO:create_model() successfully completed......................................
2025-05-03 16:03:42,125:INFO:Initializing create_model()
2025-05-03 16:03:42,125:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 16:03:42,125:INFO:Checking exceptions
2025-05-03 16:03:42,145:INFO:Importing libraries
2025-05-03 16:03:42,145:INFO:Copying training dataset
2025-05-03 16:03:42,170:INFO:Defining folds
2025-05-03 16:03:42,170:INFO:Declaring metric variables
2025-05-03 16:03:42,170:INFO:Importing untrained model
2025-05-03 16:03:42,179:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:03:42,193:INFO:Starting cross validation
2025-05-03 16:03:42,194:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:42,656:INFO:Calculating mean and std
2025-05-03 16:03:42,656:INFO:Creating metrics dataframe
2025-05-03 16:03:42,656:INFO:Finalizing model
2025-05-03 16:03:42,833:INFO:Uploading results into container
2025-05-03 16:03:42,835:INFO:Uploading model into container now
2025-05-03 16:03:42,844:INFO:_master_model_container: 11
2025-05-03 16:03:42,844:INFO:_display_container: 7
2025-05-03 16:03:42,846:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:03:42,846:INFO:create_model() successfully completed......................................
2025-05-03 16:03:42,990:INFO:Initializing create_model()
2025-05-03 16:03:42,990:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 16:03:42,990:INFO:Checking exceptions
2025-05-03 16:03:43,023:INFO:Importing libraries
2025-05-03 16:03:43,024:INFO:Copying training dataset
2025-05-03 16:03:43,055:INFO:Defining folds
2025-05-03 16:03:43,061:INFO:Declaring metric variables
2025-05-03 16:03:43,062:INFO:Importing untrained model
2025-05-03 16:03:43,071:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:03:43,083:INFO:Starting cross validation
2025-05-03 16:03:43,084:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:03:47,264:INFO:Calculating mean and std
2025-05-03 16:03:47,264:INFO:Creating metrics dataframe
2025-05-03 16:03:47,268:INFO:Finalizing model
2025-05-03 16:03:49,890:INFO:Uploading results into container
2025-05-03 16:03:49,890:INFO:Uploading model into container now
2025-05-03 16:03:49,897:INFO:_master_model_container: 12
2025-05-03 16:03:49,897:INFO:_display_container: 8
2025-05-03 16:03:49,897:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:03:49,897:INFO:create_model() successfully completed......................................
2025-05-03 16:03:50,060:INFO:Initializing interpret_model()
2025-05-03 16:03:50,060:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:03:50,060:INFO:Checking exceptions
2025-05-03 16:03:50,060:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:03:50,099:INFO:plot type: msa
2025-05-03 16:03:50,602:INFO:Visual Rendered Successfully
2025-05-03 16:03:50,602:INFO:interpret_model() successfully completed......................................
2025-05-03 16:03:50,999:INFO:Initializing interpret_model()
2025-05-03 16:03:50,999:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:03:50,999:INFO:Checking exceptions
2025-05-03 16:03:50,999:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:03:51,040:INFO:plot type: msa
2025-05-03 16:03:51,335:INFO:Visual Rendered Successfully
2025-05-03 16:03:51,337:INFO:interpret_model() successfully completed......................................
2025-05-03 16:03:51,499:INFO:Initializing interpret_model()
2025-05-03 16:03:51,499:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:03:51,499:INFO:Checking exceptions
2025-05-03 16:03:51,499:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:03:51,544:INFO:plot type: msa
2025-05-03 16:03:51,544:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:03:51,867:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:03:51,964:INFO:Visual Rendered Successfully
2025-05-03 16:03:51,964:INFO:interpret_model() successfully completed......................................
2025-05-03 16:03:52,176:INFO:Initializing save_model()
2025-05-03 16:03:52,178:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:03:52,178:INFO:Adding model into prep_pipe
2025-05-03 16:03:52,187:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 16:03:52,193:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 16:03:52,195:INFO:save_model() successfully completed......................................
2025-05-03 16:03:52,354:INFO:Initializing save_model()
2025-05-03 16:03:52,354:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:03:52,354:INFO:Adding model into prep_pipe
2025-05-03 16:03:52,368:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 16:03:52,382:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 16:03:52,382:INFO:save_model() successfully completed......................................
2025-05-03 16:03:52,531:INFO:Initializing save_model()
2025-05-03 16:03:52,531:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:03:52,531:INFO:Adding model into prep_pipe
2025-05-03 16:03:52,646:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 16:03:52,662:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 16:03:52,662:INFO:save_model() successfully completed......................................
2025-05-03 16:04:00,730:INFO:Initializing interpret_model()
2025-05-03 16:04:00,730:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:04:00,736:INFO:Checking exceptions
2025-05-03 16:04:00,736:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:04:00,763:INFO:plot type: summary
2025-05-03 16:04:00,763:INFO:Creating TreeExplainer
2025-05-03 16:04:00,796:INFO:Compiling shap values
2025-05-03 16:04:01,452:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:04:01,688:INFO:Visual Rendered Successfully
2025-05-03 16:04:01,688:INFO:interpret_model() successfully completed......................................
2025-05-03 16:04:01,830:INFO:Initializing interpret_model()
2025-05-03 16:04:01,830:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:04:01,830:INFO:Checking exceptions
2025-05-03 16:04:01,830:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:04:01,877:INFO:plot type: summary
2025-05-03 16:04:01,877:INFO:Creating TreeExplainer
2025-05-03 16:04:01,921:INFO:Compiling shap values
2025-05-03 16:04:02,610:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:04:04,088:INFO:Visual Rendered Successfully
2025-05-03 16:04:04,088:INFO:interpret_model() successfully completed......................................
2025-05-03 16:04:07,874:INFO:Initializing interpret_model()
2025-05-03 16:04:07,874:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:04:07,874:INFO:Checking exceptions
2025-05-03 16:04:07,874:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:04:07,904:INFO:plot type: summary
2025-05-03 16:04:07,904:INFO:Creating TreeExplainer
2025-05-03 16:04:07,938:INFO:Compiling shap values
2025-05-03 16:04:10,040:INFO:Visual Rendered Successfully
2025-05-03 16:04:10,040:INFO:interpret_model() successfully completed......................................
2025-05-03 16:04:10,194:INFO:Initializing interpret_model()
2025-05-03 16:04:10,194:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:04:10,194:INFO:Checking exceptions
2025-05-03 16:04:10,194:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:04:10,222:INFO:plot type: summary
2025-05-03 16:04:10,222:INFO:Creating TreeExplainer
2025-05-03 16:04:10,259:INFO:Compiling shap values
2025-05-03 16:04:13,364:INFO:Visual Rendered Successfully
2025-05-03 16:04:13,364:INFO:interpret_model() successfully completed......................................
2025-05-03 16:04:17,481:INFO:Initializing interpret_model()
2025-05-03 16:04:17,481:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=       ordinal__marital-status  ordinal__sex  target_enc__education  \
0                          4.0           1.0               0.191912   
1                          4.0           1.0               0.062443   
2                          4.0           1.0               0.160733   
3                          0.0           0.0               0.160733   
4                          0.0           1.0               0.251061   
...                        ...           ...                    ...   
39068                     -1.0           1.0               0.160733   
39069                      4.0           1.0               0.191912   
39070                      2.0           0.0               0.191912   
39071                      1.0           0.0               0.251061   
39072                      4.0           1.0               0.548190   

       target_enc__occupation  target_enc__relationship  target_enc__race  \
0                    0.124794                  0.448484          0.254406   
1                    0.225234                  0.448484          0.254406   
2                    0.225234                  0.448484          0.254406   
3                    0.137670                  0.102740          0.254406   
4                    0.207173                  0.102740          0.254406   
...                       ...                       ...               ...   
39068                0.042973                  0.102740          0.123228   
39069                0.225234                  0.448484          0.254406   
39070                0.476895                  0.102740          0.254406   
39071                0.137670                  0.060844          0.123228   
39072                0.458596                  0.448484          0.254406   

       target_enc__workclass  winsor_log_scale__age  winsor_log_scale__fnlwgt  \
0                   0.218236               0.408328                 -0.155278   
1                   0.281607               1.004967                  0.145313   
2                   0.293717              -0.179066                  0.123446   
3                   0.218236              -0.715668                 -0.385638   
4                   0.218236               0.662138                 -2.776625   
...                      ...                    ...                       ...   
39068               0.218236              -0.020498                  1.862292   
39069               0.218236              -0.179066                 -0.636347   
39070               0.218236              -0.715668                  0.719212   
39071               0.269962               0.662138                 -0.809178   
39072               0.218236               1.358948                  0.155282   

       scaler_only__educational-num  scaler_only__hours-per-week  \
0                         -0.029836                    -0.034370   
1                         -1.585248                    -0.435727   
2                         -0.418689                    -0.034370   
3                         -0.418689                    -0.034370   
4                          0.359017                     0.768345   
...                             ...                          ...   
39068                     -0.418689                    -0.034370   
39069                     -0.029836                     0.768345   
39070                     -0.029836                    -0.034370   
39071                      0.359017                    -0.034370   
39072                      1.525576                    -2.683328   

       binary__has_capital_gain  binary__has_capital_loss  \
0                           0.0                       0.0   
1                           0.0                       0.0   
2                           0.0                       0.0   
3                           0.0                       0.0   
4                           0.0                       0.0   
...                         ...                       ...   
39068                       0.0                       0.0   
39069                       0.0                       0.0   
39070                       0.0                       0.0   
39071                       0.0                       0.0   
39072                       0.0                       0.0   

       binary__es_estadounidense  
0                            1.0  
1                            1.0  
2                            1.0  
3                            1.0  
4                            1.0  
...                          ...  
39068                        1.0  
39069                        1.0  
39070                        1.0  
39071                        1.0  
39072                        1.0  

[39073 rows x 14 columns], y_new_sample=       income
0           0
1           0
2           0
3           0
4           0
...       ...
39068       0
39069       0
39070       0
39071       0
39072       1

[39073 rows x 1 columns], save=False, kwargs={})
2025-05-03 16:04:17,481:INFO:Checking exceptions
2025-05-03 16:04:17,481:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:04:48,130:INFO:Initializing interpret_model()
2025-05-03 16:04:48,130:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:04:48,130:INFO:Checking exceptions
2025-05-03 16:04:48,131:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:04:48,174:INFO:plot type: pfi
2025-05-03 16:04:48,584:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 16:04:50,115:INFO:Visual Rendered Successfully
2025-05-03 16:04:50,115:INFO:interpret_model() successfully completed......................................
2025-05-03 16:05:38,247:INFO:Initializing interpret_model()
2025-05-03 16:05:38,247:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:05:38,247:INFO:Checking exceptions
2025-05-03 16:05:38,248:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:05:38,286:INFO:plot type: summary
2025-05-03 16:05:38,286:INFO:Creating TreeExplainer
2025-05-03 16:05:38,336:INFO:Compiling shap values
2025-05-03 16:05:40,316:INFO:Visual Rendered Successfully
2025-05-03 16:05:40,323:INFO:interpret_model() successfully completed......................................
2025-05-03 16:05:50,645:INFO:Initializing interpret_model()
2025-05-03 16:05:50,645:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ADC49FBC50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:05:50,648:INFO:Checking exceptions
2025-05-03 16:05:50,648:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:05:50,675:INFO:plot type: summary
2025-05-03 16:05:50,675:INFO:Creating TreeExplainer
2025-05-03 16:05:50,715:INFO:Compiling shap values
2025-05-03 16:13:49,276:INFO:Visual Rendered Successfully
2025-05-03 16:13:49,276:INFO:interpret_model() successfully completed......................................
2025-05-03 16:22:28,497:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:22:28,497:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:22:28,497:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:22:28,497:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:22:31,630:INFO:PyCaret ClassificationExperiment
2025-05-03 16:22:31,630:INFO:Logging name: clf-default-name
2025-05-03 16:22:31,630:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 16:22:31,641:INFO:version 3.3.2
2025-05-03 16:22:31,641:INFO:Initializing setup()
2025-05-03 16:22:31,641:INFO:self.USI: 7953
2025-05-03 16:22:31,641:INFO:self._variable_keys: {'exp_id', 'USI', 'y_train', 'log_plots_param', 'X', 'fix_imbalance', '_ml_usecase', 'exp_name_log', '_available_plots', 'fold_generator', 'fold_groups_param', 'data', 'y', 'logging_param', 'gpu_n_jobs_param', 'html_param', 'fold_shuffle_param', 'memory', 'X_test', 'gpu_param', 'y_test', 'is_multiclass', 'seed', 'X_train', 'pipeline', 'target_param', 'idx', 'n_jobs_param'}
2025-05-03 16:22:31,641:INFO:Checking environment
2025-05-03 16:22:31,641:INFO:python_version: 3.11.11
2025-05-03 16:22:31,641:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 16:22:31,641:INFO:machine: AMD64
2025-05-03 16:22:31,641:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 16:22:31,645:INFO:Memory: svmem(total=16965230592, available=3523244032, percent=79.2, used=13441986560, free=3523244032)
2025-05-03 16:22:31,645:INFO:Physical Core: 4
2025-05-03 16:22:31,645:INFO:Logical Core: 8
2025-05-03 16:22:31,645:INFO:Checking libraries
2025-05-03 16:22:31,645:INFO:System:
2025-05-03 16:22:31,645:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 16:22:31,645:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 16:22:31,645:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 16:22:31,645:INFO:PyCaret required dependencies:
2025-05-03 16:22:31,647:INFO:                 pip: 25.0
2025-05-03 16:22:31,647:INFO:          setuptools: 75.8.0
2025-05-03 16:22:31,647:INFO:             pycaret: 3.3.2
2025-05-03 16:22:31,647:INFO:             IPython: 8.32.0
2025-05-03 16:22:31,647:INFO:          ipywidgets: 8.1.6
2025-05-03 16:22:31,647:INFO:                tqdm: 4.67.1
2025-05-03 16:22:31,647:INFO:               numpy: 1.26.4
2025-05-03 16:22:31,647:INFO:              pandas: 2.1.4
2025-05-03 16:22:31,647:INFO:              jinja2: 3.1.6
2025-05-03 16:22:31,647:INFO:               scipy: 1.11.4
2025-05-03 16:22:31,647:INFO:              joblib: 1.3.2
2025-05-03 16:22:31,647:INFO:             sklearn: 1.4.2
2025-05-03 16:22:31,647:INFO:                pyod: 2.0.5
2025-05-03 16:22:31,647:INFO:            imblearn: 0.13.0
2025-05-03 16:22:31,647:INFO:   category_encoders: 2.7.0
2025-05-03 16:22:31,647:INFO:            lightgbm: 4.6.0
2025-05-03 16:22:31,647:INFO:               numba: 0.61.0
2025-05-03 16:22:31,649:INFO:            requests: 2.32.3
2025-05-03 16:22:31,649:INFO:          matplotlib: 3.7.5
2025-05-03 16:22:31,649:INFO:          scikitplot: 0.3.7
2025-05-03 16:22:31,649:INFO:         yellowbrick: 1.5
2025-05-03 16:22:31,649:INFO:              plotly: 5.24.1
2025-05-03 16:22:31,649:INFO:    plotly-resampler: Not installed
2025-05-03 16:22:31,649:INFO:             kaleido: 0.2.1
2025-05-03 16:22:31,649:INFO:           schemdraw: 0.15
2025-05-03 16:22:31,649:INFO:         statsmodels: 0.14.4
2025-05-03 16:22:31,649:INFO:              sktime: 0.26.0
2025-05-03 16:22:31,649:INFO:               tbats: 1.1.3
2025-05-03 16:22:31,649:INFO:            pmdarima: 2.0.4
2025-05-03 16:22:31,649:INFO:              psutil: 6.1.1
2025-05-03 16:22:31,649:INFO:          markupsafe: 3.0.2
2025-05-03 16:22:31,649:INFO:             pickle5: Not installed
2025-05-03 16:22:31,649:INFO:         cloudpickle: 3.1.1
2025-05-03 16:22:31,649:INFO:         deprecation: 2.1.0
2025-05-03 16:22:31,649:INFO:              xxhash: 3.5.0
2025-05-03 16:22:31,649:INFO:           wurlitzer: Not installed
2025-05-03 16:22:31,649:INFO:PyCaret optional dependencies:
2025-05-03 16:22:32,009:INFO:                shap: 0.46.0
2025-05-03 16:22:32,009:INFO:           interpret: 0.6.9
2025-05-03 16:22:32,009:INFO:                umap: Not installed
2025-05-03 16:22:32,010:INFO:     ydata_profiling: Not installed
2025-05-03 16:22:32,010:INFO:  explainerdashboard: Not installed
2025-05-03 16:22:32,010:INFO:             autoviz: Not installed
2025-05-03 16:22:32,010:INFO:           fairlearn: Not installed
2025-05-03 16:22:32,010:INFO:          deepchecks: Not installed
2025-05-03 16:22:32,010:INFO:             xgboost: 3.0.0
2025-05-03 16:22:32,010:INFO:            catboost: Not installed
2025-05-03 16:22:32,010:INFO:              kmodes: Not installed
2025-05-03 16:22:32,010:INFO:             mlxtend: Not installed
2025-05-03 16:22:32,010:INFO:       statsforecast: Not installed
2025-05-03 16:22:32,010:INFO:        tune_sklearn: Not installed
2025-05-03 16:22:32,010:INFO:                 ray: Not installed
2025-05-03 16:22:32,010:INFO:            hyperopt: 0.2.7
2025-05-03 16:22:32,010:INFO:              optuna: Not installed
2025-05-03 16:22:32,010:INFO:               skopt: 0.10.2
2025-05-03 16:22:32,010:INFO:              mlflow: 2.22.0
2025-05-03 16:22:32,010:INFO:              gradio: Not installed
2025-05-03 16:22:32,010:INFO:             fastapi: 0.115.12
2025-05-03 16:22:32,010:INFO:             uvicorn: 0.34.2
2025-05-03 16:22:32,010:INFO:              m2cgen: Not installed
2025-05-03 16:22:32,010:INFO:           evidently: Not installed
2025-05-03 16:22:32,010:INFO:               fugue: Not installed
2025-05-03 16:22:32,010:INFO:           streamlit: Not installed
2025-05-03 16:22:32,010:INFO:             prophet: Not installed
2025-05-03 16:22:32,010:INFO:None
2025-05-03 16:22:32,010:INFO:Set up data.
2025-05-03 16:22:32,021:INFO:Set up folding strategy.
2025-05-03 16:22:32,021:INFO:Set up train/test split.
2025-05-03 16:22:32,038:INFO:Set up index.
2025-05-03 16:22:32,039:INFO:Assigning column types.
2025-05-03 16:22:32,045:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 16:22:32,079:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,079:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,108:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,108:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,144:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,144:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,178:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,180:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,181:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 16:22:32,212:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,241:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,241:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,274:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:22:32,299:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,301:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,301:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 16:22:32,359:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,359:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,429:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,430:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,430:INFO:Set up column name cleaning.
2025-05-03 16:22:32,450:INFO:Finished creating preprocessing pipeline.
2025-05-03 16:22:32,453:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 16:22:32,453:INFO:Creating final display dataframe.
2025-05-03 16:22:32,524:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 16:22:32,590:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,590:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,657:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:22:32,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:22:32,663:INFO:setup() successfully completed in 1.03s...............
2025-05-03 16:22:32,675:INFO:Initializing compare_models()
2025-05-03 16:22:32,675:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 16:22:32,675:INFO:Checking exceptions
2025-05-03 16:22:32,687:INFO:Preparing display monitor
2025-05-03 16:22:32,713:INFO:Initializing Logistic Regression
2025-05-03 16:22:32,713:INFO:Total runtime is 0.0 minutes
2025-05-03 16:22:32,716:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:32,716:INFO:Initializing create_model()
2025-05-03 16:22:32,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:32,716:INFO:Checking exceptions
2025-05-03 16:22:32,716:INFO:Importing libraries
2025-05-03 16:22:32,716:INFO:Copying training dataset
2025-05-03 16:22:32,733:INFO:Defining folds
2025-05-03 16:22:32,733:INFO:Declaring metric variables
2025-05-03 16:22:32,736:INFO:Importing untrained model
2025-05-03 16:22:32,739:INFO:Logistic Regression Imported successfully
2025-05-03 16:22:32,746:INFO:Starting cross validation
2025-05-03 16:22:32,747:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:36,930:INFO:Calculating mean and std
2025-05-03 16:22:36,932:INFO:Creating metrics dataframe
2025-05-03 16:22:36,934:INFO:Uploading results into container
2025-05-03 16:22:36,935:INFO:Uploading model into container now
2025-05-03 16:22:36,936:INFO:_master_model_container: 1
2025-05-03 16:22:36,936:INFO:_display_container: 2
2025-05-03 16:22:36,937:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 16:22:36,937:INFO:create_model() successfully completed......................................
2025-05-03 16:22:37,060:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:37,060:INFO:Creating metrics dataframe
2025-05-03 16:22:37,065:INFO:Initializing Random Forest Classifier
2025-05-03 16:22:37,065:INFO:Total runtime is 0.07254061301549276 minutes
2025-05-03 16:22:37,068:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:37,068:INFO:Initializing create_model()
2025-05-03 16:22:37,068:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:37,068:INFO:Checking exceptions
2025-05-03 16:22:37,070:INFO:Importing libraries
2025-05-03 16:22:37,070:INFO:Copying training dataset
2025-05-03 16:22:37,070:INFO:Defining folds
2025-05-03 16:22:37,070:INFO:Declaring metric variables
2025-05-03 16:22:37,086:INFO:Importing untrained model
2025-05-03 16:22:37,092:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:22:37,098:INFO:Starting cross validation
2025-05-03 16:22:37,098:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:41,853:INFO:Calculating mean and std
2025-05-03 16:22:41,855:INFO:Creating metrics dataframe
2025-05-03 16:22:41,857:INFO:Uploading results into container
2025-05-03 16:22:41,857:INFO:Uploading model into container now
2025-05-03 16:22:41,857:INFO:_master_model_container: 2
2025-05-03 16:22:41,857:INFO:_display_container: 2
2025-05-03 16:22:41,857:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:22:41,857:INFO:create_model() successfully completed......................................
2025-05-03 16:22:41,977:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:41,977:INFO:Creating metrics dataframe
2025-05-03 16:22:41,983:INFO:Initializing Extreme Gradient Boosting
2025-05-03 16:22:41,983:INFO:Total runtime is 0.15450688600540163 minutes
2025-05-03 16:22:41,983:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:41,983:INFO:Initializing create_model()
2025-05-03 16:22:41,983:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:41,983:INFO:Checking exceptions
2025-05-03 16:22:41,983:INFO:Importing libraries
2025-05-03 16:22:41,983:INFO:Copying training dataset
2025-05-03 16:22:41,998:INFO:Defining folds
2025-05-03 16:22:41,998:INFO:Declaring metric variables
2025-05-03 16:22:41,998:INFO:Importing untrained model
2025-05-03 16:22:42,018:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:22:42,019:INFO:Starting cross validation
2025-05-03 16:22:42,027:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:45,069:INFO:Calculating mean and std
2025-05-03 16:22:45,069:INFO:Creating metrics dataframe
2025-05-03 16:22:45,073:INFO:Uploading results into container
2025-05-03 16:22:45,073:INFO:Uploading model into container now
2025-05-03 16:22:45,073:INFO:_master_model_container: 3
2025-05-03 16:22:45,073:INFO:_display_container: 2
2025-05-03 16:22:45,073:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:22:45,073:INFO:create_model() successfully completed......................................
2025-05-03 16:22:45,180:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:45,180:INFO:Creating metrics dataframe
2025-05-03 16:22:45,180:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 16:22:45,180:INFO:Total runtime is 0.20779014825820924 minutes
2025-05-03 16:22:45,197:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:45,198:INFO:Initializing create_model()
2025-05-03 16:22:45,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:45,198:INFO:Checking exceptions
2025-05-03 16:22:45,198:INFO:Importing libraries
2025-05-03 16:22:45,198:INFO:Copying training dataset
2025-05-03 16:22:45,214:INFO:Defining folds
2025-05-03 16:22:45,214:INFO:Declaring metric variables
2025-05-03 16:22:45,214:INFO:Importing untrained model
2025-05-03 16:22:45,214:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:22:45,231:INFO:Starting cross validation
2025-05-03 16:22:45,231:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:45,776:INFO:Calculating mean and std
2025-05-03 16:22:45,778:INFO:Creating metrics dataframe
2025-05-03 16:22:45,781:INFO:Uploading results into container
2025-05-03 16:22:45,783:INFO:Uploading model into container now
2025-05-03 16:22:45,783:INFO:_master_model_container: 4
2025-05-03 16:22:45,783:INFO:_display_container: 2
2025-05-03 16:22:45,783:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:22:45,783:INFO:create_model() successfully completed......................................
2025-05-03 16:22:45,889:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:45,889:INFO:Creating metrics dataframe
2025-05-03 16:22:45,905:INFO:Initializing Extra Trees Classifier
2025-05-03 16:22:45,905:INFO:Total runtime is 0.2198643207550049 minutes
2025-05-03 16:22:45,911:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:45,911:INFO:Initializing create_model()
2025-05-03 16:22:45,911:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:45,911:INFO:Checking exceptions
2025-05-03 16:22:45,911:INFO:Importing libraries
2025-05-03 16:22:45,911:INFO:Copying training dataset
2025-05-03 16:22:45,920:INFO:Defining folds
2025-05-03 16:22:45,920:INFO:Declaring metric variables
2025-05-03 16:22:45,931:INFO:Importing untrained model
2025-05-03 16:22:45,936:INFO:Extra Trees Classifier Imported successfully
2025-05-03 16:22:45,937:INFO:Starting cross validation
2025-05-03 16:22:45,937:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:47,343:INFO:Calculating mean and std
2025-05-03 16:22:47,343:INFO:Creating metrics dataframe
2025-05-03 16:22:47,348:INFO:Uploading results into container
2025-05-03 16:22:47,349:INFO:Uploading model into container now
2025-05-03 16:22:47,350:INFO:_master_model_container: 5
2025-05-03 16:22:47,350:INFO:_display_container: 2
2025-05-03 16:22:47,351:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 16:22:47,351:INFO:create_model() successfully completed......................................
2025-05-03 16:22:47,485:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:47,485:INFO:Creating metrics dataframe
2025-05-03 16:22:47,491:INFO:Initializing Ridge Classifier
2025-05-03 16:22:47,492:INFO:Total runtime is 0.24630362590154015 minutes
2025-05-03 16:22:47,495:INFO:SubProcess create_model() called ==================================
2025-05-03 16:22:47,495:INFO:Initializing create_model()
2025-05-03 16:22:47,495:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A919A6EFD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:47,495:INFO:Checking exceptions
2025-05-03 16:22:47,495:INFO:Importing libraries
2025-05-03 16:22:47,495:INFO:Copying training dataset
2025-05-03 16:22:47,511:INFO:Defining folds
2025-05-03 16:22:47,511:INFO:Declaring metric variables
2025-05-03 16:22:47,511:INFO:Importing untrained model
2025-05-03 16:22:47,518:INFO:Ridge Classifier Imported successfully
2025-05-03 16:22:47,528:INFO:Starting cross validation
2025-05-03 16:22:47,529:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:47,609:INFO:Calculating mean and std
2025-05-03 16:22:47,609:INFO:Creating metrics dataframe
2025-05-03 16:22:47,609:INFO:Uploading results into container
2025-05-03 16:22:47,611:INFO:Uploading model into container now
2025-05-03 16:22:47,611:INFO:_master_model_container: 6
2025-05-03 16:22:47,611:INFO:_display_container: 2
2025-05-03 16:22:47,611:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 16:22:47,611:INFO:create_model() successfully completed......................................
2025-05-03 16:22:47,706:INFO:SubProcess create_model() end ==================================
2025-05-03 16:22:47,722:INFO:Creating metrics dataframe
2025-05-03 16:22:47,722:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 16:22:47,738:INFO:Initializing create_model()
2025-05-03 16:22:47,738:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:47,738:INFO:Checking exceptions
2025-05-03 16:22:47,738:INFO:Importing libraries
2025-05-03 16:22:47,742:INFO:Copying training dataset
2025-05-03 16:22:47,754:INFO:Defining folds
2025-05-03 16:22:47,754:INFO:Declaring metric variables
2025-05-03 16:22:47,754:INFO:Importing untrained model
2025-05-03 16:22:47,754:INFO:Declaring custom model
2025-05-03 16:22:47,754:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:22:47,754:INFO:Cross validation set to False
2025-05-03 16:22:47,754:INFO:Fitting Model
2025-05-03 16:22:47,791:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:22:47,795:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000939 seconds.
2025-05-03 16:22:47,795:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:22:47,795:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:22:47,795:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:22:47,795:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:22:47,795:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:22:47,795:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:22:48,011:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:22:48,011:INFO:create_model() successfully completed......................................
2025-05-03 16:22:48,167:INFO:_master_model_container: 6
2025-05-03 16:22:48,167:INFO:_display_container: 2
2025-05-03 16:22:48,167:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:22:48,167:INFO:compare_models() successfully completed......................................
2025-05-03 16:22:48,177:INFO:Initializing create_model()
2025-05-03 16:22:48,177:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:48,177:INFO:Checking exceptions
2025-05-03 16:22:48,195:INFO:Importing libraries
2025-05-03 16:22:48,195:INFO:Copying training dataset
2025-05-03 16:22:48,215:INFO:Defining folds
2025-05-03 16:22:48,215:INFO:Declaring metric variables
2025-05-03 16:22:48,219:INFO:Importing untrained model
2025-05-03 16:22:48,224:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:22:48,230:INFO:Starting cross validation
2025-05-03 16:22:48,232:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:48,883:INFO:Calculating mean and std
2025-05-03 16:22:48,883:INFO:Creating metrics dataframe
2025-05-03 16:22:48,891:INFO:Finalizing model
2025-05-03 16:22:48,919:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:22:48,923:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.
2025-05-03 16:22:48,923:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:22:48,923:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:22:48,923:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:22:48,923:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:22:48,923:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:22:48,923:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:22:49,127:INFO:Uploading results into container
2025-05-03 16:22:49,129:INFO:Uploading model into container now
2025-05-03 16:22:49,141:INFO:_master_model_container: 7
2025-05-03 16:22:49,142:INFO:_display_container: 3
2025-05-03 16:22:49,143:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:22:49,143:INFO:create_model() successfully completed......................................
2025-05-03 16:22:49,289:INFO:Initializing create_model()
2025-05-03 16:22:49,289:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:49,289:INFO:Checking exceptions
2025-05-03 16:22:49,289:INFO:Importing libraries
2025-05-03 16:22:49,303:INFO:Copying training dataset
2025-05-03 16:22:49,325:INFO:Defining folds
2025-05-03 16:22:49,325:INFO:Declaring metric variables
2025-05-03 16:22:49,329:INFO:Importing untrained model
2025-05-03 16:22:49,333:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:22:49,340:INFO:Starting cross validation
2025-05-03 16:22:49,341:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:49,733:INFO:Calculating mean and std
2025-05-03 16:22:49,733:INFO:Creating metrics dataframe
2025-05-03 16:22:49,737:INFO:Finalizing model
2025-05-03 16:22:49,906:INFO:Uploading results into container
2025-05-03 16:22:49,909:INFO:Uploading model into container now
2025-05-03 16:22:49,921:INFO:_master_model_container: 8
2025-05-03 16:22:49,921:INFO:_display_container: 4
2025-05-03 16:22:49,923:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:22:49,923:INFO:create_model() successfully completed......................................
2025-05-03 16:22:50,078:INFO:Initializing create_model()
2025-05-03 16:22:50,078:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:22:50,078:INFO:Checking exceptions
2025-05-03 16:22:50,097:INFO:Importing libraries
2025-05-03 16:22:50,097:INFO:Copying training dataset
2025-05-03 16:22:50,115:INFO:Defining folds
2025-05-03 16:22:50,115:INFO:Declaring metric variables
2025-05-03 16:22:50,119:INFO:Importing untrained model
2025-05-03 16:22:50,122:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:22:50,129:INFO:Starting cross validation
2025-05-03 16:22:50,130:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:51,569:INFO:Calculating mean and std
2025-05-03 16:22:51,569:INFO:Creating metrics dataframe
2025-05-03 16:22:51,571:INFO:Finalizing model
2025-05-03 16:22:52,215:INFO:Uploading results into container
2025-05-03 16:22:52,219:INFO:Uploading model into container now
2025-05-03 16:22:52,224:INFO:_master_model_container: 9
2025-05-03 16:22:52,224:INFO:_display_container: 5
2025-05-03 16:22:52,224:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:22:52,224:INFO:create_model() successfully completed......................................
2025-05-03 16:22:52,340:INFO:Initializing create_model()
2025-05-03 16:22:52,340:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 16:22:52,340:INFO:Checking exceptions
2025-05-03 16:22:52,360:INFO:Importing libraries
2025-05-03 16:22:52,361:INFO:Copying training dataset
2025-05-03 16:22:52,388:INFO:Defining folds
2025-05-03 16:22:52,388:INFO:Declaring metric variables
2025-05-03 16:22:52,406:INFO:Importing untrained model
2025-05-03 16:22:52,422:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:22:52,436:INFO:Starting cross validation
2025-05-03 16:22:52,437:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:52,900:INFO:Calculating mean and std
2025-05-03 16:22:52,900:INFO:Creating metrics dataframe
2025-05-03 16:22:52,907:INFO:Finalizing model
2025-05-03 16:22:52,921:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:22:52,921:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:22:52,921:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:22:52,938:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:22:52,938:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:22:52,940:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:22:52,940:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:22:52,943:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.
2025-05-03 16:22:52,944:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:22:52,944:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:22:52,944:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:22:52,944:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:22:52,944:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:22:52,945:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:22:52,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:52,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,082:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,088:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,097:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,101:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,104:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,113:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,115:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,117:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,119:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,137:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,145:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,145:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,145:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,147:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,157:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,157:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,158:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,159:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,159:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,161:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,164:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,172:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,172:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,174:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,176:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,177:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,178:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,181:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,182:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,182:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,184:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,186:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,188:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,188:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:22:53,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:22:53,203:INFO:Uploading results into container
2025-05-03 16:22:53,204:INFO:Uploading model into container now
2025-05-03 16:22:53,216:INFO:_master_model_container: 10
2025-05-03 16:22:53,216:INFO:_display_container: 6
2025-05-03 16:22:53,218:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:22:53,218:INFO:create_model() successfully completed......................................
2025-05-03 16:22:53,354:INFO:Initializing create_model()
2025-05-03 16:22:53,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 16:22:53,355:INFO:Checking exceptions
2025-05-03 16:22:53,370:INFO:Importing libraries
2025-05-03 16:22:53,370:INFO:Copying training dataset
2025-05-03 16:22:53,395:INFO:Defining folds
2025-05-03 16:22:53,395:INFO:Declaring metric variables
2025-05-03 16:22:53,398:INFO:Importing untrained model
2025-05-03 16:22:53,402:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:22:53,409:INFO:Starting cross validation
2025-05-03 16:22:53,410:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:53,877:INFO:Calculating mean and std
2025-05-03 16:22:53,877:INFO:Creating metrics dataframe
2025-05-03 16:22:53,879:INFO:Finalizing model
2025-05-03 16:22:54,063:INFO:Uploading results into container
2025-05-03 16:22:54,064:INFO:Uploading model into container now
2025-05-03 16:22:54,076:INFO:_master_model_container: 11
2025-05-03 16:22:54,076:INFO:_display_container: 7
2025-05-03 16:22:54,078:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:22:54,078:INFO:create_model() successfully completed......................................
2025-05-03 16:22:54,205:INFO:Initializing create_model()
2025-05-03 16:22:54,205:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 16:22:54,205:INFO:Checking exceptions
2025-05-03 16:22:54,205:INFO:Importing libraries
2025-05-03 16:22:54,205:INFO:Copying training dataset
2025-05-03 16:22:54,236:INFO:Defining folds
2025-05-03 16:22:54,236:INFO:Declaring metric variables
2025-05-03 16:22:54,238:INFO:Importing untrained model
2025-05-03 16:22:54,242:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:22:54,249:INFO:Starting cross validation
2025-05-03 16:22:54,250:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:22:58,688:INFO:Calculating mean and std
2025-05-03 16:22:58,689:INFO:Creating metrics dataframe
2025-05-03 16:22:58,694:INFO:Finalizing model
2025-05-03 16:23:01,152:INFO:Uploading results into container
2025-05-03 16:23:01,153:INFO:Uploading model into container now
2025-05-03 16:23:01,155:INFO:_master_model_container: 12
2025-05-03 16:23:01,155:INFO:_display_container: 8
2025-05-03 16:23:01,155:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:23:01,155:INFO:create_model() successfully completed......................................
2025-05-03 16:23:01,275:INFO:Initializing interpret_model()
2025-05-03 16:23:01,275:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:01,275:INFO:Checking exceptions
2025-05-03 16:23:01,275:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:23:01,292:INFO:plot type: msa
2025-05-03 16:23:01,680:INFO:Visual Rendered Successfully
2025-05-03 16:23:01,680:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:01,949:INFO:Initializing interpret_model()
2025-05-03 16:23:01,949:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:01,950:INFO:Checking exceptions
2025-05-03 16:23:01,950:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:23:01,967:INFO:plot type: msa
2025-05-03 16:23:02,232:INFO:Visual Rendered Successfully
2025-05-03 16:23:02,232:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:02,485:INFO:Initializing interpret_model()
2025-05-03 16:23:02,486:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:02,487:INFO:Checking exceptions
2025-05-03 16:23:02,487:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:23:02,529:INFO:plot type: msa
2025-05-03 16:23:02,529:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:23:02,816:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:23:02,939:INFO:Visual Rendered Successfully
2025-05-03 16:23:02,939:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:03,090:INFO:Initializing save_model()
2025-05-03 16:23:03,090:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:23:03,090:INFO:Adding model into prep_pipe
2025-05-03 16:23:03,100:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 16:23:03,107:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 16:23:03,107:INFO:save_model() successfully completed......................................
2025-05-03 16:23:03,248:INFO:Initializing save_model()
2025-05-03 16:23:03,248:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:23:03,248:INFO:Adding model into prep_pipe
2025-05-03 16:23:03,253:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 16:23:03,259:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 16:23:03,259:INFO:save_model() successfully completed......................................
2025-05-03 16:23:03,409:INFO:Initializing save_model()
2025-05-03 16:23:03,410:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:23:03,410:INFO:Adding model into prep_pipe
2025-05-03 16:23:03,536:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 16:23:03,538:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 16:23:03,538:INFO:save_model() successfully completed......................................
2025-05-03 16:23:03,682:INFO:Initializing interpret_model()
2025-05-03 16:23:03,682:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:03,682:INFO:Checking exceptions
2025-05-03 16:23:03,682:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:23:03,720:INFO:plot type: pfi
2025-05-03 16:23:03,848:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 16:23:04,296:INFO:Visual Rendered Successfully
2025-05-03 16:23:04,298:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:04,431:INFO:Initializing interpret_model()
2025-05-03 16:23:04,431:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:04,431:INFO:Checking exceptions
2025-05-03 16:23:04,431:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:23:04,447:INFO:plot type: summary
2025-05-03 16:23:04,447:INFO:Creating TreeExplainer
2025-05-03 16:23:04,505:INFO:Compiling shap values
2025-05-03 16:23:05,335:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:23:06,466:INFO:Visual Rendered Successfully
2025-05-03 16:23:06,470:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:06,604:INFO:Initializing interpret_model()
2025-05-03 16:23:06,604:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:06,604:INFO:Checking exceptions
2025-05-03 16:23:06,604:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:23:06,647:INFO:plot type: pfi
2025-05-03 16:23:06,977:INFO:Visual Rendered Successfully
2025-05-03 16:23:06,977:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:07,119:INFO:Initializing interpret_model()
2025-05-03 16:23:07,119:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:07,119:INFO:Checking exceptions
2025-05-03 16:23:07,119:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:23:07,142:INFO:plot type: summary
2025-05-03 16:23:07,142:INFO:Creating TreeExplainer
2025-05-03 16:23:07,172:INFO:Compiling shap values
2025-05-03 16:23:10,273:INFO:Visual Rendered Successfully
2025-05-03 16:23:10,273:INFO:interpret_model() successfully completed......................................
2025-05-03 16:23:10,408:INFO:Initializing interpret_model()
2025-05-03 16:23:10,408:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A918F59750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:23:10,408:INFO:Checking exceptions
2025-05-03 16:23:10,408:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:23:10,423:INFO:plot type: summary
2025-05-03 16:23:10,423:INFO:Creating TreeExplainer
2025-05-03 16:23:10,462:INFO:Compiling shap values
2025-05-03 16:27:15,763:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:27:15,763:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:27:15,763:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:27:15,763:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:27:18,739:INFO:PyCaret ClassificationExperiment
2025-05-03 16:27:18,739:INFO:Logging name: clf-default-name
2025-05-03 16:27:18,739:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 16:27:18,739:INFO:version 3.3.2
2025-05-03 16:27:18,739:INFO:Initializing setup()
2025-05-03 16:27:18,739:INFO:self.USI: 3227
2025-05-03 16:27:18,739:INFO:self._variable_keys: {'is_multiclass', '_available_plots', 'y_test', 'fix_imbalance', 'X_train', 'data', 'gpu_n_jobs_param', 'pipeline', 'n_jobs_param', 'X', 'log_plots_param', 'idx', 'logging_param', 'gpu_param', 'X_test', 'exp_id', 'target_param', 'y', 'seed', 'fold_generator', 'memory', 'USI', 'y_train', 'exp_name_log', 'fold_groups_param', 'fold_shuffle_param', 'html_param', '_ml_usecase'}
2025-05-03 16:27:18,739:INFO:Checking environment
2025-05-03 16:27:18,739:INFO:python_version: 3.11.11
2025-05-03 16:27:18,739:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 16:27:18,739:INFO:machine: AMD64
2025-05-03 16:27:18,739:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 16:27:18,744:INFO:Memory: svmem(total=16965230592, available=4120424448, percent=75.7, used=12844806144, free=4120424448)
2025-05-03 16:27:18,744:INFO:Physical Core: 4
2025-05-03 16:27:18,744:INFO:Logical Core: 8
2025-05-03 16:27:18,744:INFO:Checking libraries
2025-05-03 16:27:18,744:INFO:System:
2025-05-03 16:27:18,744:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 16:27:18,744:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 16:27:18,744:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 16:27:18,744:INFO:PyCaret required dependencies:
2025-05-03 16:27:18,744:INFO:                 pip: 25.0
2025-05-03 16:27:18,744:INFO:          setuptools: 75.8.0
2025-05-03 16:27:18,744:INFO:             pycaret: 3.3.2
2025-05-03 16:27:18,744:INFO:             IPython: 8.32.0
2025-05-03 16:27:18,744:INFO:          ipywidgets: 8.1.6
2025-05-03 16:27:18,744:INFO:                tqdm: 4.67.1
2025-05-03 16:27:18,744:INFO:               numpy: 1.26.4
2025-05-03 16:27:18,744:INFO:              pandas: 2.1.4
2025-05-03 16:27:18,744:INFO:              jinja2: 3.1.6
2025-05-03 16:27:18,744:INFO:               scipy: 1.11.4
2025-05-03 16:27:18,744:INFO:              joblib: 1.3.2
2025-05-03 16:27:18,744:INFO:             sklearn: 1.4.2
2025-05-03 16:27:18,744:INFO:                pyod: 2.0.5
2025-05-03 16:27:18,744:INFO:            imblearn: 0.13.0
2025-05-03 16:27:18,744:INFO:   category_encoders: 2.7.0
2025-05-03 16:27:18,744:INFO:            lightgbm: 4.6.0
2025-05-03 16:27:18,744:INFO:               numba: 0.61.0
2025-05-03 16:27:18,744:INFO:            requests: 2.32.3
2025-05-03 16:27:18,744:INFO:          matplotlib: 3.7.5
2025-05-03 16:27:18,744:INFO:          scikitplot: 0.3.7
2025-05-03 16:27:18,744:INFO:         yellowbrick: 1.5
2025-05-03 16:27:18,744:INFO:              plotly: 5.24.1
2025-05-03 16:27:18,744:INFO:    plotly-resampler: Not installed
2025-05-03 16:27:18,744:INFO:             kaleido: 0.2.1
2025-05-03 16:27:18,744:INFO:           schemdraw: 0.15
2025-05-03 16:27:18,744:INFO:         statsmodels: 0.14.4
2025-05-03 16:27:18,744:INFO:              sktime: 0.26.0
2025-05-03 16:27:18,744:INFO:               tbats: 1.1.3
2025-05-03 16:27:18,744:INFO:            pmdarima: 2.0.4
2025-05-03 16:27:18,744:INFO:              psutil: 6.1.1
2025-05-03 16:27:18,744:INFO:          markupsafe: 3.0.2
2025-05-03 16:27:18,744:INFO:             pickle5: Not installed
2025-05-03 16:27:18,744:INFO:         cloudpickle: 3.1.1
2025-05-03 16:27:18,744:INFO:         deprecation: 2.1.0
2025-05-03 16:27:18,744:INFO:              xxhash: 3.5.0
2025-05-03 16:27:18,744:INFO:           wurlitzer: Not installed
2025-05-03 16:27:18,744:INFO:PyCaret optional dependencies:
2025-05-03 16:27:19,154:INFO:                shap: 0.46.0
2025-05-03 16:27:19,154:INFO:           interpret: 0.6.9
2025-05-03 16:27:19,154:INFO:                umap: Not installed
2025-05-03 16:27:19,154:INFO:     ydata_profiling: Not installed
2025-05-03 16:27:19,155:INFO:  explainerdashboard: Not installed
2025-05-03 16:27:19,155:INFO:             autoviz: Not installed
2025-05-03 16:27:19,155:INFO:           fairlearn: Not installed
2025-05-03 16:27:19,155:INFO:          deepchecks: Not installed
2025-05-03 16:27:19,155:INFO:             xgboost: 3.0.0
2025-05-03 16:27:19,155:INFO:            catboost: Not installed
2025-05-03 16:27:19,155:INFO:              kmodes: Not installed
2025-05-03 16:27:19,155:INFO:             mlxtend: Not installed
2025-05-03 16:27:19,155:INFO:       statsforecast: Not installed
2025-05-03 16:27:19,155:INFO:        tune_sklearn: Not installed
2025-05-03 16:27:19,155:INFO:                 ray: Not installed
2025-05-03 16:27:19,155:INFO:            hyperopt: 0.2.7
2025-05-03 16:27:19,155:INFO:              optuna: Not installed
2025-05-03 16:27:19,155:INFO:               skopt: 0.10.2
2025-05-03 16:27:19,155:INFO:              mlflow: 2.22.0
2025-05-03 16:27:19,155:INFO:              gradio: Not installed
2025-05-03 16:27:19,155:INFO:             fastapi: 0.115.12
2025-05-03 16:27:19,155:INFO:             uvicorn: 0.34.2
2025-05-03 16:27:19,155:INFO:              m2cgen: Not installed
2025-05-03 16:27:19,155:INFO:           evidently: Not installed
2025-05-03 16:27:19,155:INFO:               fugue: Not installed
2025-05-03 16:27:19,155:INFO:           streamlit: Not installed
2025-05-03 16:27:19,155:INFO:             prophet: Not installed
2025-05-03 16:27:19,155:INFO:None
2025-05-03 16:27:19,155:INFO:Set up data.
2025-05-03 16:27:19,167:INFO:Set up folding strategy.
2025-05-03 16:27:19,167:INFO:Set up train/test split.
2025-05-03 16:27:19,182:INFO:Set up index.
2025-05-03 16:27:19,182:INFO:Assigning column types.
2025-05-03 16:27:19,188:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 16:27:19,222:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,236:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,261:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,288:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,288:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,323:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,325:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 16:27:19,355:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,372:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,385:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,423:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:27:19,447:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,449:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,449:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 16:27:19,505:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,576:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,576:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,576:INFO:Set up column name cleaning.
2025-05-03 16:27:19,599:INFO:Finished creating preprocessing pipeline.
2025-05-03 16:27:19,600:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 16:27:19,600:INFO:Creating final display dataframe.
2025-05-03 16:27:19,683:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 16:27:19,752:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,826:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:27:19,827:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:27:19,827:INFO:setup() successfully completed in 1.1s...............
2025-05-03 16:27:19,842:INFO:Initializing compare_models()
2025-05-03 16:27:19,842:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 16:27:19,842:INFO:Checking exceptions
2025-05-03 16:27:19,857:INFO:Preparing display monitor
2025-05-03 16:27:19,882:INFO:Initializing Logistic Regression
2025-05-03 16:27:19,882:INFO:Total runtime is 0.0 minutes
2025-05-03 16:27:19,884:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:19,886:INFO:Initializing create_model()
2025-05-03 16:27:19,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:19,886:INFO:Checking exceptions
2025-05-03 16:27:19,886:INFO:Importing libraries
2025-05-03 16:27:19,886:INFO:Copying training dataset
2025-05-03 16:27:19,902:INFO:Defining folds
2025-05-03 16:27:19,902:INFO:Declaring metric variables
2025-05-03 16:27:19,906:INFO:Importing untrained model
2025-05-03 16:27:19,909:INFO:Logistic Regression Imported successfully
2025-05-03 16:27:19,915:INFO:Starting cross validation
2025-05-03 16:27:19,917:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:23,420:INFO:Calculating mean and std
2025-05-03 16:27:23,421:INFO:Creating metrics dataframe
2025-05-03 16:27:23,424:INFO:Uploading results into container
2025-05-03 16:27:23,425:INFO:Uploading model into container now
2025-05-03 16:27:23,425:INFO:_master_model_container: 1
2025-05-03 16:27:23,426:INFO:_display_container: 2
2025-05-03 16:27:23,427:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 16:27:23,427:INFO:create_model() successfully completed......................................
2025-05-03 16:27:23,547:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:23,547:INFO:Creating metrics dataframe
2025-05-03 16:27:23,552:INFO:Initializing Random Forest Classifier
2025-05-03 16:27:23,552:INFO:Total runtime is 0.06117676893870036 minutes
2025-05-03 16:27:23,552:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:23,552:INFO:Initializing create_model()
2025-05-03 16:27:23,552:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:23,552:INFO:Checking exceptions
2025-05-03 16:27:23,552:INFO:Importing libraries
2025-05-03 16:27:23,552:INFO:Copying training dataset
2025-05-03 16:27:23,568:INFO:Defining folds
2025-05-03 16:27:23,568:INFO:Declaring metric variables
2025-05-03 16:27:23,568:INFO:Importing untrained model
2025-05-03 16:27:23,584:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:27:23,590:INFO:Starting cross validation
2025-05-03 16:27:23,590:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:27,652:INFO:Calculating mean and std
2025-05-03 16:27:27,652:INFO:Creating metrics dataframe
2025-05-03 16:27:27,652:INFO:Uploading results into container
2025-05-03 16:27:27,652:INFO:Uploading model into container now
2025-05-03 16:27:27,652:INFO:_master_model_container: 2
2025-05-03 16:27:27,652:INFO:_display_container: 2
2025-05-03 16:27:27,652:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:27:27,652:INFO:create_model() successfully completed......................................
2025-05-03 16:27:27,747:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:27,747:INFO:Creating metrics dataframe
2025-05-03 16:27:27,764:INFO:Initializing Extreme Gradient Boosting
2025-05-03 16:27:27,764:INFO:Total runtime is 0.13136444091796876 minutes
2025-05-03 16:27:27,767:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:27,767:INFO:Initializing create_model()
2025-05-03 16:27:27,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:27,767:INFO:Checking exceptions
2025-05-03 16:27:27,767:INFO:Importing libraries
2025-05-03 16:27:27,767:INFO:Copying training dataset
2025-05-03 16:27:27,780:INFO:Defining folds
2025-05-03 16:27:27,780:INFO:Declaring metric variables
2025-05-03 16:27:27,780:INFO:Importing untrained model
2025-05-03 16:27:27,791:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:27:27,795:INFO:Starting cross validation
2025-05-03 16:27:27,795:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:30,759:INFO:Calculating mean and std
2025-05-03 16:27:30,759:INFO:Creating metrics dataframe
2025-05-03 16:27:30,762:INFO:Uploading results into container
2025-05-03 16:27:30,764:INFO:Uploading model into container now
2025-05-03 16:27:30,764:INFO:_master_model_container: 3
2025-05-03 16:27:30,765:INFO:_display_container: 2
2025-05-03 16:27:30,766:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:27:30,766:INFO:create_model() successfully completed......................................
2025-05-03 16:27:30,874:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:30,874:INFO:Creating metrics dataframe
2025-05-03 16:27:30,880:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 16:27:30,880:INFO:Total runtime is 0.18331185181935628 minutes
2025-05-03 16:27:30,883:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:30,883:INFO:Initializing create_model()
2025-05-03 16:27:30,883:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:30,883:INFO:Checking exceptions
2025-05-03 16:27:30,883:INFO:Importing libraries
2025-05-03 16:27:30,883:INFO:Copying training dataset
2025-05-03 16:27:30,895:INFO:Defining folds
2025-05-03 16:27:30,895:INFO:Declaring metric variables
2025-05-03 16:27:30,895:INFO:Importing untrained model
2025-05-03 16:27:30,911:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:27:30,917:INFO:Starting cross validation
2025-05-03 16:27:30,917:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:31,530:INFO:Calculating mean and std
2025-05-03 16:27:31,532:INFO:Creating metrics dataframe
2025-05-03 16:27:31,534:INFO:Uploading results into container
2025-05-03 16:27:31,534:INFO:Uploading model into container now
2025-05-03 16:27:31,536:INFO:_master_model_container: 4
2025-05-03 16:27:31,536:INFO:_display_container: 2
2025-05-03 16:27:31,536:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:27:31,536:INFO:create_model() successfully completed......................................
2025-05-03 16:27:31,659:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:31,659:INFO:Creating metrics dataframe
2025-05-03 16:27:31,659:INFO:Initializing Extra Trees Classifier
2025-05-03 16:27:31,659:INFO:Total runtime is 0.19628764788309733 minutes
2025-05-03 16:27:31,659:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:31,659:INFO:Initializing create_model()
2025-05-03 16:27:31,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:31,659:INFO:Checking exceptions
2025-05-03 16:27:31,659:INFO:Importing libraries
2025-05-03 16:27:31,659:INFO:Copying training dataset
2025-05-03 16:27:31,693:INFO:Defining folds
2025-05-03 16:27:31,693:INFO:Declaring metric variables
2025-05-03 16:27:31,695:INFO:Importing untrained model
2025-05-03 16:27:31,701:INFO:Extra Trees Classifier Imported successfully
2025-05-03 16:27:31,707:INFO:Starting cross validation
2025-05-03 16:27:31,707:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:32,945:INFO:Calculating mean and std
2025-05-03 16:27:32,945:INFO:Creating metrics dataframe
2025-05-03 16:27:32,947:INFO:Uploading results into container
2025-05-03 16:27:32,947:INFO:Uploading model into container now
2025-05-03 16:27:32,947:INFO:_master_model_container: 5
2025-05-03 16:27:32,947:INFO:_display_container: 2
2025-05-03 16:27:32,947:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 16:27:32,947:INFO:create_model() successfully completed......................................
2025-05-03 16:27:33,048:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:33,048:INFO:Creating metrics dataframe
2025-05-03 16:27:33,054:INFO:Initializing Ridge Classifier
2025-05-03 16:27:33,054:INFO:Total runtime is 0.21954251925150553 minutes
2025-05-03 16:27:33,058:INFO:SubProcess create_model() called ==================================
2025-05-03 16:27:33,058:INFO:Initializing create_model()
2025-05-03 16:27:33,058:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000143FCAC8F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:33,058:INFO:Checking exceptions
2025-05-03 16:27:33,058:INFO:Importing libraries
2025-05-03 16:27:33,058:INFO:Copying training dataset
2025-05-03 16:27:33,058:INFO:Defining folds
2025-05-03 16:27:33,058:INFO:Declaring metric variables
2025-05-03 16:27:33,074:INFO:Importing untrained model
2025-05-03 16:27:33,074:INFO:Ridge Classifier Imported successfully
2025-05-03 16:27:33,082:INFO:Starting cross validation
2025-05-03 16:27:33,082:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:33,153:INFO:Calculating mean and std
2025-05-03 16:27:33,153:INFO:Creating metrics dataframe
2025-05-03 16:27:33,153:INFO:Uploading results into container
2025-05-03 16:27:33,153:INFO:Uploading model into container now
2025-05-03 16:27:33,153:INFO:_master_model_container: 6
2025-05-03 16:27:33,153:INFO:_display_container: 2
2025-05-03 16:27:33,153:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 16:27:33,153:INFO:create_model() successfully completed......................................
2025-05-03 16:27:33,258:INFO:SubProcess create_model() end ==================================
2025-05-03 16:27:33,258:INFO:Creating metrics dataframe
2025-05-03 16:27:33,260:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 16:27:33,276:INFO:Initializing create_model()
2025-05-03 16:27:33,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:33,276:INFO:Checking exceptions
2025-05-03 16:27:33,276:INFO:Importing libraries
2025-05-03 16:27:33,276:INFO:Copying training dataset
2025-05-03 16:27:33,292:INFO:Defining folds
2025-05-03 16:27:33,292:INFO:Declaring metric variables
2025-05-03 16:27:33,292:INFO:Importing untrained model
2025-05-03 16:27:33,292:INFO:Declaring custom model
2025-05-03 16:27:33,292:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:27:33,292:INFO:Cross validation set to False
2025-05-03 16:27:33,292:INFO:Fitting Model
2025-05-03 16:27:33,315:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:27:33,319:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.
2025-05-03 16:27:33,319:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:27:33,319:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:27:33,319:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:27:33,319:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:27:33,320:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:27:33,320:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:27:33,479:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:27:33,479:INFO:create_model() successfully completed......................................
2025-05-03 16:27:33,622:INFO:_master_model_container: 6
2025-05-03 16:27:33,622:INFO:_display_container: 2
2025-05-03 16:27:33,622:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:27:33,622:INFO:compare_models() successfully completed......................................
2025-05-03 16:27:33,638:INFO:Initializing create_model()
2025-05-03 16:27:33,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:33,638:INFO:Checking exceptions
2025-05-03 16:27:33,662:INFO:Importing libraries
2025-05-03 16:27:33,663:INFO:Copying training dataset
2025-05-03 16:27:33,697:INFO:Defining folds
2025-05-03 16:27:33,697:INFO:Declaring metric variables
2025-05-03 16:27:33,712:INFO:Importing untrained model
2025-05-03 16:27:33,718:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:27:33,725:INFO:Starting cross validation
2025-05-03 16:27:33,726:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:34,286:INFO:Calculating mean and std
2025-05-03 16:27:34,287:INFO:Creating metrics dataframe
2025-05-03 16:27:34,293:INFO:Finalizing model
2025-05-03 16:27:34,317:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:27:34,319:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.
2025-05-03 16:27:34,319:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:27:34,319:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:27:34,321:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:27:34,321:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:27:34,321:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:27:34,321:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:27:34,443:INFO:Uploading results into container
2025-05-03 16:27:34,445:INFO:Uploading model into container now
2025-05-03 16:27:34,455:INFO:_master_model_container: 7
2025-05-03 16:27:34,455:INFO:_display_container: 3
2025-05-03 16:27:34,455:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:27:34,457:INFO:create_model() successfully completed......................................
2025-05-03 16:27:34,600:INFO:Initializing create_model()
2025-05-03 16:27:34,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:34,600:INFO:Checking exceptions
2025-05-03 16:27:34,615:INFO:Importing libraries
2025-05-03 16:27:34,615:INFO:Copying training dataset
2025-05-03 16:27:34,636:INFO:Defining folds
2025-05-03 16:27:34,636:INFO:Declaring metric variables
2025-05-03 16:27:34,639:INFO:Importing untrained model
2025-05-03 16:27:34,643:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:27:34,651:INFO:Starting cross validation
2025-05-03 16:27:34,651:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:34,982:INFO:Calculating mean and std
2025-05-03 16:27:34,982:INFO:Creating metrics dataframe
2025-05-03 16:27:34,982:INFO:Finalizing model
2025-05-03 16:27:35,149:INFO:Uploading results into container
2025-05-03 16:27:35,150:INFO:Uploading model into container now
2025-05-03 16:27:35,160:INFO:_master_model_container: 8
2025-05-03 16:27:35,160:INFO:_display_container: 4
2025-05-03 16:27:35,162:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:27:35,162:INFO:create_model() successfully completed......................................
2025-05-03 16:27:35,299:INFO:Initializing create_model()
2025-05-03 16:27:35,299:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:27:35,299:INFO:Checking exceptions
2025-05-03 16:27:35,313:INFO:Importing libraries
2025-05-03 16:27:35,313:INFO:Copying training dataset
2025-05-03 16:27:35,335:INFO:Defining folds
2025-05-03 16:27:35,335:INFO:Declaring metric variables
2025-05-03 16:27:35,337:INFO:Importing untrained model
2025-05-03 16:27:35,340:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:27:35,347:INFO:Starting cross validation
2025-05-03 16:27:35,348:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:36,660:INFO:Calculating mean and std
2025-05-03 16:27:36,660:INFO:Creating metrics dataframe
2025-05-03 16:27:36,660:INFO:Finalizing model
2025-05-03 16:27:37,309:INFO:Uploading results into container
2025-05-03 16:27:37,309:INFO:Uploading model into container now
2025-05-03 16:27:37,309:INFO:_master_model_container: 9
2025-05-03 16:27:37,309:INFO:_display_container: 5
2025-05-03 16:27:37,309:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:27:37,322:INFO:create_model() successfully completed......................................
2025-05-03 16:27:37,454:INFO:Initializing create_model()
2025-05-03 16:27:37,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 16:27:37,454:INFO:Checking exceptions
2025-05-03 16:27:37,468:INFO:Importing libraries
2025-05-03 16:27:37,468:INFO:Copying training dataset
2025-05-03 16:27:37,491:INFO:Defining folds
2025-05-03 16:27:37,491:INFO:Declaring metric variables
2025-05-03 16:27:37,493:INFO:Importing untrained model
2025-05-03 16:27:37,500:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:27:37,507:INFO:Starting cross validation
2025-05-03 16:27:37,508:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:38,064:INFO:Calculating mean and std
2025-05-03 16:27:38,064:INFO:Creating metrics dataframe
2025-05-03 16:27:38,070:INFO:Finalizing model
2025-05-03 16:27:38,083:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:27:38,084:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:27:38,084:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:27:38,102:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:27:38,102:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:27:38,102:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:27:38,102:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:27:38,108:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003970 seconds.
2025-05-03 16:27:38,108:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 16:27:38,108:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:27:38,108:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:27:38,109:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:27:38,109:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:27:38,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,240:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,241:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,242:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,242:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,245:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,250:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,253:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,254:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,257:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,257:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,260:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,262:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,264:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,264:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,265:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,265:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,266:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,267:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,268:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,268:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,269:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,270:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,271:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,272:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,273:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,275:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,276:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,278:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,279:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,280:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,281:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,282:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,283:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,284:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,284:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,286:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,286:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,287:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,288:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,288:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,290:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,290:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,292:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,293:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,294:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,301:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,301:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,302:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,302:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,304:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,305:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,307:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,308:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,309:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,310:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,310:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,311:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,312:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,313:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,315:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,316:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,318:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,319:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,319:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,320:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,320:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,321:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,323:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,323:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,325:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,326:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,327:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,327:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,328:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,329:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:27:38,331:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:27:38,341:INFO:Uploading results into container
2025-05-03 16:27:38,342:INFO:Uploading model into container now
2025-05-03 16:27:38,355:INFO:_master_model_container: 10
2025-05-03 16:27:38,355:INFO:_display_container: 6
2025-05-03 16:27:38,357:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:27:38,357:INFO:create_model() successfully completed......................................
2025-05-03 16:27:38,491:INFO:Initializing create_model()
2025-05-03 16:27:38,491:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 16:27:38,491:INFO:Checking exceptions
2025-05-03 16:27:38,502:INFO:Importing libraries
2025-05-03 16:27:38,502:INFO:Copying training dataset
2025-05-03 16:27:38,533:INFO:Defining folds
2025-05-03 16:27:38,533:INFO:Declaring metric variables
2025-05-03 16:27:38,544:INFO:Importing untrained model
2025-05-03 16:27:38,551:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:27:38,562:INFO:Starting cross validation
2025-05-03 16:27:38,562:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:39,008:INFO:Calculating mean and std
2025-05-03 16:27:39,008:INFO:Creating metrics dataframe
2025-05-03 16:27:39,011:INFO:Finalizing model
2025-05-03 16:27:39,202:INFO:Uploading results into container
2025-05-03 16:27:39,202:INFO:Uploading model into container now
2025-05-03 16:27:39,214:INFO:_master_model_container: 11
2025-05-03 16:27:39,214:INFO:_display_container: 7
2025-05-03 16:27:39,216:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:27:39,216:INFO:create_model() successfully completed......................................
2025-05-03 16:27:39,336:INFO:Initializing create_model()
2025-05-03 16:27:39,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 16:27:39,336:INFO:Checking exceptions
2025-05-03 16:27:39,366:INFO:Importing libraries
2025-05-03 16:27:39,366:INFO:Copying training dataset
2025-05-03 16:27:39,384:INFO:Defining folds
2025-05-03 16:27:39,384:INFO:Declaring metric variables
2025-05-03 16:27:39,388:INFO:Importing untrained model
2025-05-03 16:27:39,391:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:27:39,399:INFO:Starting cross validation
2025-05-03 16:27:39,399:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:27:43,856:INFO:Calculating mean and std
2025-05-03 16:27:43,858:INFO:Creating metrics dataframe
2025-05-03 16:27:43,864:INFO:Finalizing model
2025-05-03 16:27:46,229:INFO:Uploading results into container
2025-05-03 16:27:46,229:INFO:Uploading model into container now
2025-05-03 16:27:46,233:INFO:_master_model_container: 12
2025-05-03 16:27:46,233:INFO:_display_container: 8
2025-05-03 16:27:46,233:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:27:46,233:INFO:create_model() successfully completed......................................
2025-05-03 16:27:46,348:INFO:Initializing interpret_model()
2025-05-03 16:27:46,348:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:46,348:INFO:Checking exceptions
2025-05-03 16:27:46,348:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:27:46,359:INFO:plot type: msa
2025-05-03 16:27:46,724:INFO:Visual Rendered Successfully
2025-05-03 16:27:46,724:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:46,956:INFO:Initializing interpret_model()
2025-05-03 16:27:46,956:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:46,956:INFO:Checking exceptions
2025-05-03 16:27:46,956:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:27:46,971:INFO:plot type: msa
2025-05-03 16:27:47,219:INFO:Visual Rendered Successfully
2025-05-03 16:27:47,219:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:47,364:INFO:Initializing interpret_model()
2025-05-03 16:27:47,364:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:47,364:INFO:Checking exceptions
2025-05-03 16:27:47,364:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:27:47,379:INFO:plot type: msa
2025-05-03 16:27:47,379:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:27:47,615:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:27:47,714:INFO:Visual Rendered Successfully
2025-05-03 16:27:47,714:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:47,835:INFO:Initializing save_model()
2025-05-03 16:27:47,835:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:27:47,835:INFO:Adding model into prep_pipe
2025-05-03 16:27:47,848:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 16:27:47,853:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 16:27:47,853:INFO:save_model() successfully completed......................................
2025-05-03 16:27:47,983:INFO:Initializing save_model()
2025-05-03 16:27:47,983:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:27:47,983:INFO:Adding model into prep_pipe
2025-05-03 16:27:47,999:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 16:27:48,005:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 16:27:48,005:INFO:save_model() successfully completed......................................
2025-05-03 16:27:48,129:INFO:Initializing save_model()
2025-05-03 16:27:48,129:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:27:48,129:INFO:Adding model into prep_pipe
2025-05-03 16:27:48,233:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 16:27:48,233:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 16:27:48,233:INFO:save_model() successfully completed......................................
2025-05-03 16:27:48,379:INFO:Initializing interpret_model()
2025-05-03 16:27:48,380:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:48,380:INFO:Checking exceptions
2025-05-03 16:27:48,380:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:27:48,413:INFO:plot type: pfi
2025-05-03 16:27:48,531:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 16:27:48,939:INFO:Visual Rendered Successfully
2025-05-03 16:27:48,939:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:49,075:INFO:Initializing interpret_model()
2025-05-03 16:27:49,076:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:49,076:INFO:Checking exceptions
2025-05-03 16:27:49,076:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:27:49,080:INFO:plot type: summary
2025-05-03 16:27:49,080:INFO:Creating TreeExplainer
2025-05-03 16:27:49,134:INFO:Compiling shap values
2025-05-03 16:27:49,871:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:27:51,062:INFO:Visual Rendered Successfully
2025-05-03 16:27:51,062:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:51,205:INFO:Initializing interpret_model()
2025-05-03 16:27:51,205:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:51,205:INFO:Checking exceptions
2025-05-03 16:27:51,205:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:27:51,240:INFO:plot type: pfi
2025-05-03 16:27:51,478:INFO:Visual Rendered Successfully
2025-05-03 16:27:51,480:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:51,614:INFO:Initializing interpret_model()
2025-05-03 16:27:51,614:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:51,614:INFO:Checking exceptions
2025-05-03 16:27:51,614:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:27:51,628:INFO:plot type: summary
2025-05-03 16:27:51,628:INFO:Creating TreeExplainer
2025-05-03 16:27:51,667:INFO:Compiling shap values
2025-05-03 16:27:54,951:INFO:Visual Rendered Successfully
2025-05-03 16:27:54,951:INFO:interpret_model() successfully completed......................................
2025-05-03 16:27:55,080:INFO:Initializing interpret_model()
2025-05-03 16:27:55,081:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000143FC364A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:27:55,081:INFO:Checking exceptions
2025-05-03 16:27:55,081:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:27:55,096:INFO:plot type: summary
2025-05-03 16:27:55,096:INFO:Creating TreeExplainer
2025-05-03 16:27:55,125:INFO:Compiling shap values
2025-05-03 16:33:00,254:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:33:00,254:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:33:00,254:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:33:00,255:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:33:03,205:INFO:PyCaret ClassificationExperiment
2025-05-03 16:33:03,205:INFO:Logging name: clf-default-name
2025-05-03 16:33:03,205:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 16:33:03,205:INFO:version 3.3.2
2025-05-03 16:33:03,205:INFO:Initializing setup()
2025-05-03 16:33:03,205:INFO:self.USI: e0ae
2025-05-03 16:33:03,205:INFO:self._variable_keys: {'pipeline', 'target_param', 'y_train', 'fold_groups_param', 'html_param', 'y_test', 'is_multiclass', 'exp_id', 'USI', 'n_jobs_param', 'log_plots_param', '_ml_usecase', 'X', 'idx', 'gpu_n_jobs_param', 'seed', 'gpu_param', 'logging_param', 'y', 'X_test', 'X_train', 'fold_generator', 'fix_imbalance', 'memory', '_available_plots', 'data', 'fold_shuffle_param', 'exp_name_log'}
2025-05-03 16:33:03,205:INFO:Checking environment
2025-05-03 16:33:03,205:INFO:python_version: 3.11.11
2025-05-03 16:33:03,205:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 16:33:03,205:INFO:machine: AMD64
2025-05-03 16:33:03,205:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 16:33:03,214:INFO:Memory: svmem(total=16965230592, available=4145860608, percent=75.6, used=12819369984, free=4145860608)
2025-05-03 16:33:03,214:INFO:Physical Core: 4
2025-05-03 16:33:03,214:INFO:Logical Core: 8
2025-05-03 16:33:03,214:INFO:Checking libraries
2025-05-03 16:33:03,214:INFO:System:
2025-05-03 16:33:03,214:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 16:33:03,214:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 16:33:03,214:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 16:33:03,214:INFO:PyCaret required dependencies:
2025-05-03 16:33:03,216:INFO:                 pip: 25.0
2025-05-03 16:33:03,216:INFO:          setuptools: 75.8.0
2025-05-03 16:33:03,216:INFO:             pycaret: 3.3.2
2025-05-03 16:33:03,216:INFO:             IPython: 8.32.0
2025-05-03 16:33:03,216:INFO:          ipywidgets: 8.1.6
2025-05-03 16:33:03,216:INFO:                tqdm: 4.67.1
2025-05-03 16:33:03,216:INFO:               numpy: 1.26.4
2025-05-03 16:33:03,216:INFO:              pandas: 2.1.4
2025-05-03 16:33:03,216:INFO:              jinja2: 3.1.6
2025-05-03 16:33:03,216:INFO:               scipy: 1.11.4
2025-05-03 16:33:03,216:INFO:              joblib: 1.3.2
2025-05-03 16:33:03,216:INFO:             sklearn: 1.4.2
2025-05-03 16:33:03,216:INFO:                pyod: 2.0.5
2025-05-03 16:33:03,216:INFO:            imblearn: 0.13.0
2025-05-03 16:33:03,216:INFO:   category_encoders: 2.7.0
2025-05-03 16:33:03,216:INFO:            lightgbm: 4.6.0
2025-05-03 16:33:03,216:INFO:               numba: 0.61.0
2025-05-03 16:33:03,216:INFO:            requests: 2.32.3
2025-05-03 16:33:03,216:INFO:          matplotlib: 3.7.5
2025-05-03 16:33:03,216:INFO:          scikitplot: 0.3.7
2025-05-03 16:33:03,216:INFO:         yellowbrick: 1.5
2025-05-03 16:33:03,216:INFO:              plotly: 5.24.1
2025-05-03 16:33:03,216:INFO:    plotly-resampler: Not installed
2025-05-03 16:33:03,216:INFO:             kaleido: 0.2.1
2025-05-03 16:33:03,216:INFO:           schemdraw: 0.15
2025-05-03 16:33:03,216:INFO:         statsmodels: 0.14.4
2025-05-03 16:33:03,216:INFO:              sktime: 0.26.0
2025-05-03 16:33:03,216:INFO:               tbats: 1.1.3
2025-05-03 16:33:03,216:INFO:            pmdarima: 2.0.4
2025-05-03 16:33:03,216:INFO:              psutil: 6.1.1
2025-05-03 16:33:03,216:INFO:          markupsafe: 3.0.2
2025-05-03 16:33:03,216:INFO:             pickle5: Not installed
2025-05-03 16:33:03,216:INFO:         cloudpickle: 3.1.1
2025-05-03 16:33:03,216:INFO:         deprecation: 2.1.0
2025-05-03 16:33:03,216:INFO:              xxhash: 3.5.0
2025-05-03 16:33:03,216:INFO:           wurlitzer: Not installed
2025-05-03 16:33:03,216:INFO:PyCaret optional dependencies:
2025-05-03 16:33:03,572:INFO:                shap: 0.46.0
2025-05-03 16:33:03,572:INFO:           interpret: 0.6.9
2025-05-03 16:33:03,572:INFO:                umap: Not installed
2025-05-03 16:33:03,572:INFO:     ydata_profiling: Not installed
2025-05-03 16:33:03,572:INFO:  explainerdashboard: Not installed
2025-05-03 16:33:03,572:INFO:             autoviz: Not installed
2025-05-03 16:33:03,572:INFO:           fairlearn: Not installed
2025-05-03 16:33:03,572:INFO:          deepchecks: Not installed
2025-05-03 16:33:03,572:INFO:             xgboost: 3.0.0
2025-05-03 16:33:03,572:INFO:            catboost: Not installed
2025-05-03 16:33:03,572:INFO:              kmodes: Not installed
2025-05-03 16:33:03,572:INFO:             mlxtend: Not installed
2025-05-03 16:33:03,572:INFO:       statsforecast: Not installed
2025-05-03 16:33:03,572:INFO:        tune_sklearn: Not installed
2025-05-03 16:33:03,572:INFO:                 ray: Not installed
2025-05-03 16:33:03,572:INFO:            hyperopt: 0.2.7
2025-05-03 16:33:03,572:INFO:              optuna: Not installed
2025-05-03 16:33:03,572:INFO:               skopt: 0.10.2
2025-05-03 16:33:03,572:INFO:              mlflow: 2.22.0
2025-05-03 16:33:03,572:INFO:              gradio: Not installed
2025-05-03 16:33:03,572:INFO:             fastapi: 0.115.12
2025-05-03 16:33:03,572:INFO:             uvicorn: 0.34.2
2025-05-03 16:33:03,572:INFO:              m2cgen: Not installed
2025-05-03 16:33:03,572:INFO:           evidently: Not installed
2025-05-03 16:33:03,572:INFO:               fugue: Not installed
2025-05-03 16:33:03,572:INFO:           streamlit: Not installed
2025-05-03 16:33:03,572:INFO:             prophet: Not installed
2025-05-03 16:33:03,572:INFO:None
2025-05-03 16:33:03,572:INFO:Set up data.
2025-05-03 16:33:03,587:INFO:Set up folding strategy.
2025-05-03 16:33:03,588:INFO:Set up train/test split.
2025-05-03 16:33:03,607:INFO:Set up index.
2025-05-03 16:33:03,609:INFO:Assigning column types.
2025-05-03 16:33:03,621:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 16:33:03,663:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,667:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,688:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:03,734:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,734:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,754:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,754:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:03,754:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 16:33:03,788:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,804:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:03,854:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:33:03,872:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:03,872:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 16:33:03,935:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,937:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:03,988:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:03,988:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:04,004:INFO:Set up column name cleaning.
2025-05-03 16:33:04,021:INFO:Finished creating preprocessing pipeline.
2025-05-03 16:33:04,021:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 16:33:04,021:INFO:Creating final display dataframe.
2025-05-03 16:33:04,114:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 16:33:04,171:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:04,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:04,237:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:33:04,237:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:33:04,237:INFO:setup() successfully completed in 1.03s...............
2025-05-03 16:33:04,242:INFO:Initializing compare_models()
2025-05-03 16:33:04,242:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 16:33:04,242:INFO:Checking exceptions
2025-05-03 16:33:04,254:INFO:Preparing display monitor
2025-05-03 16:33:04,289:INFO:Initializing Logistic Regression
2025-05-03 16:33:04,289:INFO:Total runtime is 0.0 minutes
2025-05-03 16:33:04,293:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:04,294:INFO:Initializing create_model()
2025-05-03 16:33:04,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:04,294:INFO:Checking exceptions
2025-05-03 16:33:04,294:INFO:Importing libraries
2025-05-03 16:33:04,294:INFO:Copying training dataset
2025-05-03 16:33:04,318:INFO:Defining folds
2025-05-03 16:33:04,318:INFO:Declaring metric variables
2025-05-03 16:33:04,324:INFO:Importing untrained model
2025-05-03 16:33:04,329:INFO:Logistic Regression Imported successfully
2025-05-03 16:33:04,335:INFO:Starting cross validation
2025-05-03 16:33:04,336:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:07,946:INFO:Calculating mean and std
2025-05-03 16:33:07,948:INFO:Creating metrics dataframe
2025-05-03 16:33:07,950:INFO:Uploading results into container
2025-05-03 16:33:07,951:INFO:Uploading model into container now
2025-05-03 16:33:07,952:INFO:_master_model_container: 1
2025-05-03 16:33:07,952:INFO:_display_container: 2
2025-05-03 16:33:07,952:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 16:33:07,952:INFO:create_model() successfully completed......................................
2025-05-03 16:33:08,064:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:08,064:INFO:Creating metrics dataframe
2025-05-03 16:33:08,064:INFO:Initializing Random Forest Classifier
2025-05-03 16:33:08,064:INFO:Total runtime is 0.06291702191034952 minutes
2025-05-03 16:33:08,064:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:08,064:INFO:Initializing create_model()
2025-05-03 16:33:08,064:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:08,064:INFO:Checking exceptions
2025-05-03 16:33:08,064:INFO:Importing libraries
2025-05-03 16:33:08,064:INFO:Copying training dataset
2025-05-03 16:33:08,090:INFO:Defining folds
2025-05-03 16:33:08,090:INFO:Declaring metric variables
2025-05-03 16:33:08,090:INFO:Importing untrained model
2025-05-03 16:33:08,095:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:33:08,095:INFO:Starting cross validation
2025-05-03 16:33:08,095:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:11,960:INFO:Calculating mean and std
2025-05-03 16:33:11,962:INFO:Creating metrics dataframe
2025-05-03 16:33:11,965:INFO:Uploading results into container
2025-05-03 16:33:11,967:INFO:Uploading model into container now
2025-05-03 16:33:11,967:INFO:_master_model_container: 2
2025-05-03 16:33:11,967:INFO:_display_container: 2
2025-05-03 16:33:11,967:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:33:11,967:INFO:create_model() successfully completed......................................
2025-05-03 16:33:12,078:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:12,080:INFO:Creating metrics dataframe
2025-05-03 16:33:12,084:INFO:Initializing Extreme Gradient Boosting
2025-05-03 16:33:12,084:INFO:Total runtime is 0.12991607189178467 minutes
2025-05-03 16:33:12,084:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:12,084:INFO:Initializing create_model()
2025-05-03 16:33:12,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:12,084:INFO:Checking exceptions
2025-05-03 16:33:12,084:INFO:Importing libraries
2025-05-03 16:33:12,084:INFO:Copying training dataset
2025-05-03 16:33:12,097:INFO:Defining folds
2025-05-03 16:33:12,097:INFO:Declaring metric variables
2025-05-03 16:33:12,097:INFO:Importing untrained model
2025-05-03 16:33:12,097:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:33:12,113:INFO:Starting cross validation
2025-05-03 16:33:12,113:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:14,940:INFO:Calculating mean and std
2025-05-03 16:33:14,940:INFO:Creating metrics dataframe
2025-05-03 16:33:14,943:INFO:Uploading results into container
2025-05-03 16:33:14,943:INFO:Uploading model into container now
2025-05-03 16:33:14,943:INFO:_master_model_container: 3
2025-05-03 16:33:14,943:INFO:_display_container: 2
2025-05-03 16:33:14,945:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:33:14,945:INFO:create_model() successfully completed......................................
2025-05-03 16:33:15,051:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:15,051:INFO:Creating metrics dataframe
2025-05-03 16:33:15,058:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 16:33:15,058:INFO:Total runtime is 0.1794894297917684 minutes
2025-05-03 16:33:15,061:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:15,063:INFO:Initializing create_model()
2025-05-03 16:33:15,063:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:15,063:INFO:Checking exceptions
2025-05-03 16:33:15,063:INFO:Importing libraries
2025-05-03 16:33:15,063:INFO:Copying training dataset
2025-05-03 16:33:15,077:INFO:Defining folds
2025-05-03 16:33:15,078:INFO:Declaring metric variables
2025-05-03 16:33:15,080:INFO:Importing untrained model
2025-05-03 16:33:15,080:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:33:15,080:INFO:Starting cross validation
2025-05-03 16:33:15,080:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:15,631:INFO:Calculating mean and std
2025-05-03 16:33:15,633:INFO:Creating metrics dataframe
2025-05-03 16:33:15,635:INFO:Uploading results into container
2025-05-03 16:33:15,635:INFO:Uploading model into container now
2025-05-03 16:33:15,635:INFO:_master_model_container: 4
2025-05-03 16:33:15,635:INFO:_display_container: 2
2025-05-03 16:33:15,638:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:33:15,638:INFO:create_model() successfully completed......................................
2025-05-03 16:33:15,754:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:15,754:INFO:Creating metrics dataframe
2025-05-03 16:33:15,762:INFO:Initializing Extra Trees Classifier
2025-05-03 16:33:15,762:INFO:Total runtime is 0.19122143586476645 minutes
2025-05-03 16:33:15,770:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:15,770:INFO:Initializing create_model()
2025-05-03 16:33:15,770:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:15,770:INFO:Checking exceptions
2025-05-03 16:33:15,770:INFO:Importing libraries
2025-05-03 16:33:15,770:INFO:Copying training dataset
2025-05-03 16:33:15,786:INFO:Defining folds
2025-05-03 16:33:15,786:INFO:Declaring metric variables
2025-05-03 16:33:15,788:INFO:Importing untrained model
2025-05-03 16:33:15,788:INFO:Extra Trees Classifier Imported successfully
2025-05-03 16:33:15,795:INFO:Starting cross validation
2025-05-03 16:33:15,795:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:17,006:INFO:Calculating mean and std
2025-05-03 16:33:17,006:INFO:Creating metrics dataframe
2025-05-03 16:33:17,011:INFO:Uploading results into container
2025-05-03 16:33:17,011:INFO:Uploading model into container now
2025-05-03 16:33:17,011:INFO:_master_model_container: 5
2025-05-03 16:33:17,011:INFO:_display_container: 2
2025-05-03 16:33:17,011:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 16:33:17,011:INFO:create_model() successfully completed......................................
2025-05-03 16:33:17,135:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:17,135:INFO:Creating metrics dataframe
2025-05-03 16:33:17,141:INFO:Initializing Ridge Classifier
2025-05-03 16:33:17,141:INFO:Total runtime is 0.21420523325602214 minutes
2025-05-03 16:33:17,144:INFO:SubProcess create_model() called ==================================
2025-05-03 16:33:17,144:INFO:Initializing create_model()
2025-05-03 16:33:17,146:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028E14629E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:17,146:INFO:Checking exceptions
2025-05-03 16:33:17,146:INFO:Importing libraries
2025-05-03 16:33:17,146:INFO:Copying training dataset
2025-05-03 16:33:17,163:INFO:Defining folds
2025-05-03 16:33:17,163:INFO:Declaring metric variables
2025-05-03 16:33:17,163:INFO:Importing untrained model
2025-05-03 16:33:17,172:INFO:Ridge Classifier Imported successfully
2025-05-03 16:33:17,179:INFO:Starting cross validation
2025-05-03 16:33:17,181:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:17,250:INFO:Calculating mean and std
2025-05-03 16:33:17,250:INFO:Creating metrics dataframe
2025-05-03 16:33:17,250:INFO:Uploading results into container
2025-05-03 16:33:17,250:INFO:Uploading model into container now
2025-05-03 16:33:17,250:INFO:_master_model_container: 6
2025-05-03 16:33:17,250:INFO:_display_container: 2
2025-05-03 16:33:17,250:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 16:33:17,250:INFO:create_model() successfully completed......................................
2025-05-03 16:33:17,364:INFO:SubProcess create_model() end ==================================
2025-05-03 16:33:17,364:INFO:Creating metrics dataframe
2025-05-03 16:33:17,375:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 16:33:17,386:INFO:Initializing create_model()
2025-05-03 16:33:17,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:17,386:INFO:Checking exceptions
2025-05-03 16:33:17,386:INFO:Importing libraries
2025-05-03 16:33:17,386:INFO:Copying training dataset
2025-05-03 16:33:17,391:INFO:Defining folds
2025-05-03 16:33:17,391:INFO:Declaring metric variables
2025-05-03 16:33:17,391:INFO:Importing untrained model
2025-05-03 16:33:17,391:INFO:Declaring custom model
2025-05-03 16:33:17,407:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:33:17,407:INFO:Cross validation set to False
2025-05-03 16:33:17,407:INFO:Fitting Model
2025-05-03 16:33:17,429:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:33:17,431:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.
2025-05-03 16:33:17,433:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:33:17,433:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:33:17,433:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:33:17,433:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:33:17,433:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:33:17,433:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:33:17,622:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:33:17,622:INFO:create_model() successfully completed......................................
2025-05-03 16:33:17,775:INFO:_master_model_container: 6
2025-05-03 16:33:17,775:INFO:_display_container: 2
2025-05-03 16:33:17,775:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:33:17,775:INFO:compare_models() successfully completed......................................
2025-05-03 16:33:17,790:INFO:Initializing create_model()
2025-05-03 16:33:17,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:17,790:INFO:Checking exceptions
2025-05-03 16:33:17,806:INFO:Importing libraries
2025-05-03 16:33:17,806:INFO:Copying training dataset
2025-05-03 16:33:17,826:INFO:Defining folds
2025-05-03 16:33:17,826:INFO:Declaring metric variables
2025-05-03 16:33:17,830:INFO:Importing untrained model
2025-05-03 16:33:17,835:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:33:17,840:INFO:Starting cross validation
2025-05-03 16:33:17,841:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:18,374:INFO:Calculating mean and std
2025-05-03 16:33:18,374:INFO:Creating metrics dataframe
2025-05-03 16:33:18,380:INFO:Finalizing model
2025-05-03 16:33:18,409:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:33:18,411:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002289 seconds.
2025-05-03 16:33:18,411:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 16:33:18,411:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:33:18,411:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:33:18,413:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:33:18,413:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:33:18,567:INFO:Uploading results into container
2025-05-03 16:33:18,568:INFO:Uploading model into container now
2025-05-03 16:33:18,578:INFO:_master_model_container: 7
2025-05-03 16:33:18,578:INFO:_display_container: 3
2025-05-03 16:33:18,581:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:33:18,581:INFO:create_model() successfully completed......................................
2025-05-03 16:33:18,741:INFO:Initializing create_model()
2025-05-03 16:33:18,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:18,741:INFO:Checking exceptions
2025-05-03 16:33:18,762:INFO:Importing libraries
2025-05-03 16:33:18,763:INFO:Copying training dataset
2025-05-03 16:33:18,781:INFO:Defining folds
2025-05-03 16:33:18,781:INFO:Declaring metric variables
2025-05-03 16:33:18,784:INFO:Importing untrained model
2025-05-03 16:33:18,788:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:33:18,794:INFO:Starting cross validation
2025-05-03 16:33:18,795:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:19,123:INFO:Calculating mean and std
2025-05-03 16:33:19,123:INFO:Creating metrics dataframe
2025-05-03 16:33:19,123:INFO:Finalizing model
2025-05-03 16:33:19,268:INFO:Uploading results into container
2025-05-03 16:33:19,270:INFO:Uploading model into container now
2025-05-03 16:33:19,290:INFO:_master_model_container: 8
2025-05-03 16:33:19,290:INFO:_display_container: 4
2025-05-03 16:33:19,292:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:33:19,292:INFO:create_model() successfully completed......................................
2025-05-03 16:33:19,435:INFO:Initializing create_model()
2025-05-03 16:33:19,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:33:19,436:INFO:Checking exceptions
2025-05-03 16:33:19,449:INFO:Importing libraries
2025-05-03 16:33:19,449:INFO:Copying training dataset
2025-05-03 16:33:19,476:INFO:Defining folds
2025-05-03 16:33:19,476:INFO:Declaring metric variables
2025-05-03 16:33:19,483:INFO:Importing untrained model
2025-05-03 16:33:19,504:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:33:19,513:INFO:Starting cross validation
2025-05-03 16:33:19,514:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:20,858:INFO:Calculating mean and std
2025-05-03 16:33:20,858:INFO:Creating metrics dataframe
2025-05-03 16:33:20,863:INFO:Finalizing model
2025-05-03 16:33:21,469:INFO:Uploading results into container
2025-05-03 16:33:21,479:INFO:Uploading model into container now
2025-05-03 16:33:21,487:INFO:_master_model_container: 9
2025-05-03 16:33:21,490:INFO:_display_container: 5
2025-05-03 16:33:21,490:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:33:21,491:INFO:create_model() successfully completed......................................
2025-05-03 16:33:46,234:INFO:Initializing create_model()
2025-05-03 16:33:46,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 16:33:46,235:INFO:Checking exceptions
2025-05-03 16:33:46,255:INFO:Importing libraries
2025-05-03 16:33:46,255:INFO:Copying training dataset
2025-05-03 16:33:46,292:INFO:Defining folds
2025-05-03 16:33:46,292:INFO:Declaring metric variables
2025-05-03 16:33:46,295:INFO:Importing untrained model
2025-05-03 16:33:46,298:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:33:46,343:INFO:Starting cross validation
2025-05-03 16:33:46,343:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:46,745:INFO:Calculating mean and std
2025-05-03 16:33:46,745:INFO:Creating metrics dataframe
2025-05-03 16:33:46,750:INFO:Finalizing model
2025-05-03 16:33:46,762:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:33:46,762:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:33:46,762:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:33:46,778:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:33:46,778:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:33:46,780:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:33:46,780:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:33:46,782:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.
2025-05-03 16:33:46,782:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:33:46,782:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:33:46,782:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:33:46,782:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:33:46,782:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:33:46,782:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:33:46,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,883:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,889:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,891:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,892:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,893:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,893:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,894:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,897:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,899:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,902:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,918:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,952:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,952:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,954:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,954:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:33:46,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:33:46,966:INFO:Uploading results into container
2025-05-03 16:33:46,968:INFO:Uploading model into container now
2025-05-03 16:33:46,976:INFO:_master_model_container: 10
2025-05-03 16:33:46,976:INFO:_display_container: 6
2025-05-03 16:33:46,976:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:33:46,976:INFO:create_model() successfully completed......................................
2025-05-03 16:33:47,104:INFO:Initializing create_model()
2025-05-03 16:33:47,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 16:33:47,104:INFO:Checking exceptions
2025-05-03 16:33:47,122:INFO:Importing libraries
2025-05-03 16:33:47,122:INFO:Copying training dataset
2025-05-03 16:33:47,144:INFO:Defining folds
2025-05-03 16:33:47,144:INFO:Declaring metric variables
2025-05-03 16:33:47,147:INFO:Importing untrained model
2025-05-03 16:33:47,151:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:33:47,160:INFO:Starting cross validation
2025-05-03 16:33:47,161:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:47,646:INFO:Calculating mean and std
2025-05-03 16:33:47,646:INFO:Creating metrics dataframe
2025-05-03 16:33:47,650:INFO:Finalizing model
2025-05-03 16:33:47,817:INFO:Uploading results into container
2025-05-03 16:33:47,818:INFO:Uploading model into container now
2025-05-03 16:33:47,827:INFO:_master_model_container: 11
2025-05-03 16:33:47,827:INFO:_display_container: 7
2025-05-03 16:33:47,831:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:33:47,831:INFO:create_model() successfully completed......................................
2025-05-03 16:33:47,951:INFO:Initializing create_model()
2025-05-03 16:33:47,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 16:33:47,951:INFO:Checking exceptions
2025-05-03 16:33:47,970:INFO:Importing libraries
2025-05-03 16:33:47,970:INFO:Copying training dataset
2025-05-03 16:33:48,030:INFO:Defining folds
2025-05-03 16:33:48,030:INFO:Declaring metric variables
2025-05-03 16:33:48,039:INFO:Importing untrained model
2025-05-03 16:33:48,043:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:33:48,047:INFO:Starting cross validation
2025-05-03 16:33:48,048:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:33:51,985:INFO:Calculating mean and std
2025-05-03 16:33:51,985:INFO:Creating metrics dataframe
2025-05-03 16:33:51,985:INFO:Finalizing model
2025-05-03 16:33:54,267:INFO:Uploading results into container
2025-05-03 16:33:54,268:INFO:Uploading model into container now
2025-05-03 16:33:54,276:INFO:_master_model_container: 12
2025-05-03 16:33:54,276:INFO:_display_container: 8
2025-05-03 16:33:54,277:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:33:54,277:INFO:create_model() successfully completed......................................
2025-05-03 16:33:54,405:INFO:Initializing interpret_model()
2025-05-03 16:33:54,405:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:33:54,405:INFO:Checking exceptions
2025-05-03 16:33:54,405:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:33:54,426:INFO:plot type: msa
2025-05-03 16:33:54,823:INFO:Visual Rendered Successfully
2025-05-03 16:33:54,823:INFO:interpret_model() successfully completed......................................
2025-05-03 16:33:55,064:INFO:Initializing interpret_model()
2025-05-03 16:33:55,064:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:33:55,064:INFO:Checking exceptions
2025-05-03 16:33:55,064:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:33:55,084:INFO:plot type: msa
2025-05-03 16:33:55,334:INFO:Visual Rendered Successfully
2025-05-03 16:33:55,334:INFO:interpret_model() successfully completed......................................
2025-05-03 16:33:55,473:INFO:Initializing interpret_model()
2025-05-03 16:33:55,473:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:33:55,473:INFO:Checking exceptions
2025-05-03 16:33:55,473:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:33:55,492:INFO:plot type: msa
2025-05-03 16:33:55,493:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:33:55,713:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:33:55,811:INFO:Visual Rendered Successfully
2025-05-03 16:33:55,811:INFO:interpret_model() successfully completed......................................
2025-05-03 16:33:55,965:INFO:Initializing save_model()
2025-05-03 16:33:55,965:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:33:55,965:INFO:Adding model into prep_pipe
2025-05-03 16:33:55,971:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 16:33:55,976:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 16:33:55,976:INFO:save_model() successfully completed......................................
2025-05-03 16:33:56,112:INFO:Initializing save_model()
2025-05-03 16:33:56,113:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:33:56,113:INFO:Adding model into prep_pipe
2025-05-03 16:33:56,117:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 16:33:56,125:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 16:33:56,125:INFO:save_model() successfully completed......................................
2025-05-03 16:33:56,242:INFO:Initializing save_model()
2025-05-03 16:33:56,242:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:33:56,242:INFO:Adding model into prep_pipe
2025-05-03 16:33:56,344:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 16:33:56,344:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 16:33:56,344:INFO:save_model() successfully completed......................................
2025-05-03 16:33:56,499:INFO:Initializing interpret_model()
2025-05-03 16:33:56,501:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:33:56,501:INFO:Checking exceptions
2025-05-03 16:33:56,501:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:33:56,529:INFO:plot type: summary
2025-05-03 16:33:56,529:INFO:Creating TreeExplainer
2025-05-03 16:33:56,569:INFO:Compiling shap values
2025-05-03 16:33:57,330:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:33:57,535:INFO:Visual Rendered Successfully
2025-05-03 16:33:57,536:INFO:interpret_model() successfully completed......................................
2025-05-03 16:33:57,647:INFO:Initializing interpret_model()
2025-05-03 16:33:57,647:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:33:57,647:INFO:Checking exceptions
2025-05-03 16:33:57,647:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:33:57,665:INFO:plot type: summary
2025-05-03 16:33:57,665:INFO:Creating TreeExplainer
2025-05-03 16:33:57,711:INFO:Compiling shap values
2025-05-03 16:33:58,481:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:33:59,609:INFO:Visual Rendered Successfully
2025-05-03 16:33:59,609:INFO:interpret_model() successfully completed......................................
2025-05-03 16:33:59,744:INFO:Initializing interpret_model()
2025-05-03 16:33:59,744:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:33:59,744:INFO:Checking exceptions
2025-05-03 16:33:59,744:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:33:59,762:INFO:plot type: summary
2025-05-03 16:33:59,762:INFO:Creating TreeExplainer
2025-05-03 16:33:59,796:INFO:Compiling shap values
2025-05-03 16:34:01,713:INFO:Visual Rendered Successfully
2025-05-03 16:34:01,713:INFO:interpret_model() successfully completed......................................
2025-05-03 16:34:01,811:INFO:Initializing interpret_model()
2025-05-03 16:34:01,811:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:34:01,811:INFO:Checking exceptions
2025-05-03 16:34:01,811:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:34:01,842:INFO:plot type: summary
2025-05-03 16:34:01,843:INFO:Creating TreeExplainer
2025-05-03 16:34:01,867:INFO:Compiling shap values
2025-05-03 16:34:04,743:INFO:Visual Rendered Successfully
2025-05-03 16:34:04,743:INFO:interpret_model() successfully completed......................................
2025-05-03 16:34:04,902:INFO:Initializing interpret_model()
2025-05-03 16:34:04,903:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:34:04,903:INFO:Checking exceptions
2025-05-03 16:34:04,903:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:34:04,941:INFO:plot type: pfi
2025-05-03 16:34:05,052:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 16:34:06,443:INFO:Visual Rendered Successfully
2025-05-03 16:34:06,443:INFO:interpret_model() successfully completed......................................
2025-05-03 16:34:06,570:INFO:Initializing interpret_model()
2025-05-03 16:34:06,570:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028E13CC5690>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:34:06,570:INFO:Checking exceptions
2025-05-03 16:34:06,570:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:34:06,601:INFO:plot type: summary
2025-05-03 16:34:06,601:INFO:Creating TreeExplainer
2025-05-03 16:34:06,633:INFO:Compiling shap values
2025-05-03 16:35:00,034:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:35:00,034:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:35:00,034:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:35:00,034:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 16:35:02,840:INFO:PyCaret ClassificationExperiment
2025-05-03 16:35:02,840:INFO:Logging name: clf-default-name
2025-05-03 16:35:02,840:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 16:35:02,840:INFO:version 3.3.2
2025-05-03 16:35:02,840:INFO:Initializing setup()
2025-05-03 16:35:02,840:INFO:self.USI: 0384
2025-05-03 16:35:02,840:INFO:self._variable_keys: {'y_train', 'X', '_ml_usecase', 'gpu_param', 'y', 'target_param', 'html_param', 'pipeline', 'X_test', 'idx', 'n_jobs_param', 'X_train', 'memory', 'fold_shuffle_param', 'gpu_n_jobs_param', 'fix_imbalance', 'USI', 'data', 'fold_groups_param', 'is_multiclass', 'exp_id', 'logging_param', 'y_test', 'log_plots_param', 'fold_generator', 'exp_name_log', '_available_plots', 'seed'}
2025-05-03 16:35:02,840:INFO:Checking environment
2025-05-03 16:35:02,840:INFO:python_version: 3.11.11
2025-05-03 16:35:02,840:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 16:35:02,840:INFO:machine: AMD64
2025-05-03 16:35:02,840:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 16:35:02,840:INFO:Memory: svmem(total=16965230592, available=4001357824, percent=76.4, used=12963872768, free=4001357824)
2025-05-03 16:35:02,840:INFO:Physical Core: 4
2025-05-03 16:35:02,840:INFO:Logical Core: 8
2025-05-03 16:35:02,840:INFO:Checking libraries
2025-05-03 16:35:02,840:INFO:System:
2025-05-03 16:35:02,840:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 16:35:02,840:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 16:35:02,840:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 16:35:02,840:INFO:PyCaret required dependencies:
2025-05-03 16:35:02,840:INFO:                 pip: 25.0
2025-05-03 16:35:02,840:INFO:          setuptools: 75.8.0
2025-05-03 16:35:02,840:INFO:             pycaret: 3.3.2
2025-05-03 16:35:02,840:INFO:             IPython: 8.32.0
2025-05-03 16:35:02,840:INFO:          ipywidgets: 8.1.6
2025-05-03 16:35:02,840:INFO:                tqdm: 4.67.1
2025-05-03 16:35:02,840:INFO:               numpy: 1.26.4
2025-05-03 16:35:02,840:INFO:              pandas: 2.1.4
2025-05-03 16:35:02,840:INFO:              jinja2: 3.1.6
2025-05-03 16:35:02,840:INFO:               scipy: 1.11.4
2025-05-03 16:35:02,840:INFO:              joblib: 1.3.2
2025-05-03 16:35:02,840:INFO:             sklearn: 1.4.2
2025-05-03 16:35:02,840:INFO:                pyod: 2.0.5
2025-05-03 16:35:02,840:INFO:            imblearn: 0.13.0
2025-05-03 16:35:02,840:INFO:   category_encoders: 2.7.0
2025-05-03 16:35:02,840:INFO:            lightgbm: 4.6.0
2025-05-03 16:35:02,840:INFO:               numba: 0.61.0
2025-05-03 16:35:02,840:INFO:            requests: 2.32.3
2025-05-03 16:35:02,840:INFO:          matplotlib: 3.7.5
2025-05-03 16:35:02,840:INFO:          scikitplot: 0.3.7
2025-05-03 16:35:02,840:INFO:         yellowbrick: 1.5
2025-05-03 16:35:02,840:INFO:              plotly: 5.24.1
2025-05-03 16:35:02,840:INFO:    plotly-resampler: Not installed
2025-05-03 16:35:02,840:INFO:             kaleido: 0.2.1
2025-05-03 16:35:02,840:INFO:           schemdraw: 0.15
2025-05-03 16:35:02,840:INFO:         statsmodels: 0.14.4
2025-05-03 16:35:02,840:INFO:              sktime: 0.26.0
2025-05-03 16:35:02,840:INFO:               tbats: 1.1.3
2025-05-03 16:35:02,840:INFO:            pmdarima: 2.0.4
2025-05-03 16:35:02,840:INFO:              psutil: 6.1.1
2025-05-03 16:35:02,840:INFO:          markupsafe: 3.0.2
2025-05-03 16:35:02,840:INFO:             pickle5: Not installed
2025-05-03 16:35:02,840:INFO:         cloudpickle: 3.1.1
2025-05-03 16:35:02,840:INFO:         deprecation: 2.1.0
2025-05-03 16:35:02,840:INFO:              xxhash: 3.5.0
2025-05-03 16:35:02,840:INFO:           wurlitzer: Not installed
2025-05-03 16:35:02,840:INFO:PyCaret optional dependencies:
2025-05-03 16:35:03,197:INFO:                shap: 0.46.0
2025-05-03 16:35:03,197:INFO:           interpret: 0.6.9
2025-05-03 16:35:03,197:INFO:                umap: Not installed
2025-05-03 16:35:03,197:INFO:     ydata_profiling: Not installed
2025-05-03 16:35:03,198:INFO:  explainerdashboard: Not installed
2025-05-03 16:35:03,198:INFO:             autoviz: Not installed
2025-05-03 16:35:03,198:INFO:           fairlearn: Not installed
2025-05-03 16:35:03,198:INFO:          deepchecks: Not installed
2025-05-03 16:35:03,198:INFO:             xgboost: 3.0.0
2025-05-03 16:35:03,198:INFO:            catboost: Not installed
2025-05-03 16:35:03,198:INFO:              kmodes: Not installed
2025-05-03 16:35:03,198:INFO:             mlxtend: Not installed
2025-05-03 16:35:03,198:INFO:       statsforecast: Not installed
2025-05-03 16:35:03,198:INFO:        tune_sklearn: Not installed
2025-05-03 16:35:03,198:INFO:                 ray: Not installed
2025-05-03 16:35:03,198:INFO:            hyperopt: 0.2.7
2025-05-03 16:35:03,198:INFO:              optuna: Not installed
2025-05-03 16:35:03,198:INFO:               skopt: 0.10.2
2025-05-03 16:35:03,198:INFO:              mlflow: 2.22.0
2025-05-03 16:35:03,198:INFO:              gradio: Not installed
2025-05-03 16:35:03,198:INFO:             fastapi: 0.115.12
2025-05-03 16:35:03,198:INFO:             uvicorn: 0.34.2
2025-05-03 16:35:03,198:INFO:              m2cgen: Not installed
2025-05-03 16:35:03,198:INFO:           evidently: Not installed
2025-05-03 16:35:03,198:INFO:               fugue: Not installed
2025-05-03 16:35:03,198:INFO:           streamlit: Not installed
2025-05-03 16:35:03,198:INFO:             prophet: Not installed
2025-05-03 16:35:03,198:INFO:None
2025-05-03 16:35:03,198:INFO:Set up data.
2025-05-03 16:35:03,207:INFO:Set up folding strategy.
2025-05-03 16:35:03,207:INFO:Set up train/test split.
2025-05-03 16:35:03,223:INFO:Set up index.
2025-05-03 16:35:03,223:INFO:Assigning column types.
2025-05-03 16:35:03,223:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 16:35:03,272:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,273:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,291:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,323:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,323:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,356:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,356:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 16:35:03,389:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,421:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,456:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 16:35:03,473:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,483:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,483:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 16:35:03,539:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,589:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,606:INFO:Set up column name cleaning.
2025-05-03 16:35:03,623:INFO:Finished creating preprocessing pipeline.
2025-05-03 16:35:03,623:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 16:35:03,623:INFO:Creating final display dataframe.
2025-05-03 16:35:03,689:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 16:35:03,756:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,805:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 16:35:03,805:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 16:35:03,805:INFO:setup() successfully completed in 0.97s...............
2025-05-03 16:35:03,826:INFO:Initializing compare_models()
2025-05-03 16:35:03,826:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 16:35:03,826:INFO:Checking exceptions
2025-05-03 16:35:03,839:INFO:Preparing display monitor
2025-05-03 16:35:03,867:INFO:Initializing Logistic Regression
2025-05-03 16:35:03,867:INFO:Total runtime is 0.0 minutes
2025-05-03 16:35:03,870:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:03,870:INFO:Initializing create_model()
2025-05-03 16:35:03,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:03,870:INFO:Checking exceptions
2025-05-03 16:35:03,870:INFO:Importing libraries
2025-05-03 16:35:03,870:INFO:Copying training dataset
2025-05-03 16:35:03,885:INFO:Defining folds
2025-05-03 16:35:03,885:INFO:Declaring metric variables
2025-05-03 16:35:03,888:INFO:Importing untrained model
2025-05-03 16:35:03,892:INFO:Logistic Regression Imported successfully
2025-05-03 16:35:03,897:INFO:Starting cross validation
2025-05-03 16:35:03,899:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:07,205:INFO:Calculating mean and std
2025-05-03 16:35:07,205:INFO:Creating metrics dataframe
2025-05-03 16:35:07,205:INFO:Uploading results into container
2025-05-03 16:35:07,205:INFO:Uploading model into container now
2025-05-03 16:35:07,205:INFO:_master_model_container: 1
2025-05-03 16:35:07,205:INFO:_display_container: 2
2025-05-03 16:35:07,205:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 16:35:07,205:INFO:create_model() successfully completed......................................
2025-05-03 16:35:07,321:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:07,321:INFO:Creating metrics dataframe
2025-05-03 16:35:07,321:INFO:Initializing Random Forest Classifier
2025-05-03 16:35:07,321:INFO:Total runtime is 0.05756378173828125 minutes
2025-05-03 16:35:07,329:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:07,329:INFO:Initializing create_model()
2025-05-03 16:35:07,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:07,329:INFO:Checking exceptions
2025-05-03 16:35:07,329:INFO:Importing libraries
2025-05-03 16:35:07,329:INFO:Copying training dataset
2025-05-03 16:35:07,336:INFO:Defining folds
2025-05-03 16:35:07,336:INFO:Declaring metric variables
2025-05-03 16:35:07,336:INFO:Importing untrained model
2025-05-03 16:35:07,353:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:35:07,358:INFO:Starting cross validation
2025-05-03 16:35:07,358:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:11,266:INFO:Calculating mean and std
2025-05-03 16:35:11,266:INFO:Creating metrics dataframe
2025-05-03 16:35:11,269:INFO:Uploading results into container
2025-05-03 16:35:11,271:INFO:Uploading model into container now
2025-05-03 16:35:11,271:INFO:_master_model_container: 2
2025-05-03 16:35:11,271:INFO:_display_container: 2
2025-05-03 16:35:11,271:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:35:11,271:INFO:create_model() successfully completed......................................
2025-05-03 16:35:11,383:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:11,383:INFO:Creating metrics dataframe
2025-05-03 16:35:11,383:INFO:Initializing Extreme Gradient Boosting
2025-05-03 16:35:11,383:INFO:Total runtime is 0.1252664804458618 minutes
2025-05-03 16:35:11,394:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:11,394:INFO:Initializing create_model()
2025-05-03 16:35:11,394:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:11,394:INFO:Checking exceptions
2025-05-03 16:35:11,394:INFO:Importing libraries
2025-05-03 16:35:11,394:INFO:Copying training dataset
2025-05-03 16:35:11,399:INFO:Defining folds
2025-05-03 16:35:11,399:INFO:Declaring metric variables
2025-05-03 16:35:11,413:INFO:Importing untrained model
2025-05-03 16:35:11,416:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:35:11,423:INFO:Starting cross validation
2025-05-03 16:35:11,423:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:14,203:INFO:Calculating mean and std
2025-05-03 16:35:14,203:INFO:Creating metrics dataframe
2025-05-03 16:35:14,203:INFO:Uploading results into container
2025-05-03 16:35:14,207:INFO:Uploading model into container now
2025-05-03 16:35:14,207:INFO:_master_model_container: 3
2025-05-03 16:35:14,207:INFO:_display_container: 2
2025-05-03 16:35:14,207:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:35:14,207:INFO:create_model() successfully completed......................................
2025-05-03 16:35:14,315:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:14,315:INFO:Creating metrics dataframe
2025-05-03 16:35:14,315:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 16:35:14,315:INFO:Total runtime is 0.17413280407587686 minutes
2025-05-03 16:35:14,315:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:14,315:INFO:Initializing create_model()
2025-05-03 16:35:14,315:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:14,315:INFO:Checking exceptions
2025-05-03 16:35:14,315:INFO:Importing libraries
2025-05-03 16:35:14,315:INFO:Copying training dataset
2025-05-03 16:35:14,341:INFO:Defining folds
2025-05-03 16:35:14,341:INFO:Declaring metric variables
2025-05-03 16:35:14,343:INFO:Importing untrained model
2025-05-03 16:35:14,347:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:35:14,352:INFO:Starting cross validation
2025-05-03 16:35:14,353:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:14,893:INFO:Calculating mean and std
2025-05-03 16:35:14,895:INFO:Creating metrics dataframe
2025-05-03 16:35:14,897:INFO:Uploading results into container
2025-05-03 16:35:14,899:INFO:Uploading model into container now
2025-05-03 16:35:14,899:INFO:_master_model_container: 4
2025-05-03 16:35:14,899:INFO:_display_container: 2
2025-05-03 16:35:14,901:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:35:14,901:INFO:create_model() successfully completed......................................
2025-05-03 16:35:15,038:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:15,038:INFO:Creating metrics dataframe
2025-05-03 16:35:15,046:INFO:Initializing Extra Trees Classifier
2025-05-03 16:35:15,046:INFO:Total runtime is 0.18631558418273925 minutes
2025-05-03 16:35:15,047:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:15,047:INFO:Initializing create_model()
2025-05-03 16:35:15,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:15,047:INFO:Checking exceptions
2025-05-03 16:35:15,047:INFO:Importing libraries
2025-05-03 16:35:15,047:INFO:Copying training dataset
2025-05-03 16:35:15,069:INFO:Defining folds
2025-05-03 16:35:15,069:INFO:Declaring metric variables
2025-05-03 16:35:15,072:INFO:Importing untrained model
2025-05-03 16:35:15,075:INFO:Extra Trees Classifier Imported successfully
2025-05-03 16:35:15,079:INFO:Starting cross validation
2025-05-03 16:35:15,079:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:16,272:INFO:Calculating mean and std
2025-05-03 16:35:16,272:INFO:Creating metrics dataframe
2025-05-03 16:35:16,272:INFO:Uploading results into container
2025-05-03 16:35:16,276:INFO:Uploading model into container now
2025-05-03 16:35:16,276:INFO:_master_model_container: 5
2025-05-03 16:35:16,276:INFO:_display_container: 2
2025-05-03 16:35:16,277:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 16:35:16,278:INFO:create_model() successfully completed......................................
2025-05-03 16:35:16,395:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:16,395:INFO:Creating metrics dataframe
2025-05-03 16:35:16,402:INFO:Initializing Ridge Classifier
2025-05-03 16:35:16,402:INFO:Total runtime is 0.20892686049143472 minutes
2025-05-03 16:35:16,402:INFO:SubProcess create_model() called ==================================
2025-05-03 16:35:16,402:INFO:Initializing create_model()
2025-05-03 16:35:16,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C27D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:16,402:INFO:Checking exceptions
2025-05-03 16:35:16,402:INFO:Importing libraries
2025-05-03 16:35:16,402:INFO:Copying training dataset
2025-05-03 16:35:16,427:INFO:Defining folds
2025-05-03 16:35:16,427:INFO:Declaring metric variables
2025-05-03 16:35:16,429:INFO:Importing untrained model
2025-05-03 16:35:16,433:INFO:Ridge Classifier Imported successfully
2025-05-03 16:35:16,433:INFO:Starting cross validation
2025-05-03 16:35:16,433:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:16,522:INFO:Calculating mean and std
2025-05-03 16:35:16,522:INFO:Creating metrics dataframe
2025-05-03 16:35:16,522:INFO:Uploading results into container
2025-05-03 16:35:16,522:INFO:Uploading model into container now
2025-05-03 16:35:16,522:INFO:_master_model_container: 6
2025-05-03 16:35:16,522:INFO:_display_container: 2
2025-05-03 16:35:16,522:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 16:35:16,522:INFO:create_model() successfully completed......................................
2025-05-03 16:35:16,630:INFO:SubProcess create_model() end ==================================
2025-05-03 16:35:16,630:INFO:Creating metrics dataframe
2025-05-03 16:35:16,630:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 16:35:16,647:INFO:Initializing create_model()
2025-05-03 16:35:16,649:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:16,649:INFO:Checking exceptions
2025-05-03 16:35:16,650:INFO:Importing libraries
2025-05-03 16:35:16,650:INFO:Copying training dataset
2025-05-03 16:35:16,662:INFO:Defining folds
2025-05-03 16:35:16,662:INFO:Declaring metric variables
2025-05-03 16:35:16,662:INFO:Importing untrained model
2025-05-03 16:35:16,662:INFO:Declaring custom model
2025-05-03 16:35:16,662:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:35:16,662:INFO:Cross validation set to False
2025-05-03 16:35:16,662:INFO:Fitting Model
2025-05-03 16:35:16,710:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:35:16,714:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001278 seconds.
2025-05-03 16:35:16,714:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:35:16,714:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:35:16,714:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:35:16,714:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:35:16,715:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:35:16,715:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:35:16,897:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:35:16,899:INFO:create_model() successfully completed......................................
2025-05-03 16:35:17,088:INFO:_master_model_container: 6
2025-05-03 16:35:17,089:INFO:_display_container: 2
2025-05-03 16:35:17,091:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:35:17,091:INFO:compare_models() successfully completed......................................
2025-05-03 16:35:17,102:INFO:Initializing create_model()
2025-05-03 16:35:17,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:17,102:INFO:Checking exceptions
2025-05-03 16:35:17,121:INFO:Importing libraries
2025-05-03 16:35:17,121:INFO:Copying training dataset
2025-05-03 16:35:17,143:INFO:Defining folds
2025-05-03 16:35:17,143:INFO:Declaring metric variables
2025-05-03 16:35:17,146:INFO:Importing untrained model
2025-05-03 16:35:17,150:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:35:17,160:INFO:Starting cross validation
2025-05-03 16:35:17,161:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:17,761:INFO:Calculating mean and std
2025-05-03 16:35:17,761:INFO:Creating metrics dataframe
2025-05-03 16:35:17,768:INFO:Finalizing model
2025-05-03 16:35:17,797:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:35:17,799:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001269 seconds.
2025-05-03 16:35:17,799:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:35:17,799:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:35:17,801:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:35:17,801:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:35:17,801:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:35:17,801:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:35:17,922:INFO:Uploading results into container
2025-05-03 16:35:17,924:INFO:Uploading model into container now
2025-05-03 16:35:17,934:INFO:_master_model_container: 7
2025-05-03 16:35:17,934:INFO:_display_container: 3
2025-05-03 16:35:17,934:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:35:17,934:INFO:create_model() successfully completed......................................
2025-05-03 16:35:18,077:INFO:Initializing create_model()
2025-05-03 16:35:18,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:18,077:INFO:Checking exceptions
2025-05-03 16:35:18,096:INFO:Importing libraries
2025-05-03 16:35:18,096:INFO:Copying training dataset
2025-05-03 16:35:18,118:INFO:Defining folds
2025-05-03 16:35:18,119:INFO:Declaring metric variables
2025-05-03 16:35:18,124:INFO:Importing untrained model
2025-05-03 16:35:18,128:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:35:18,135:INFO:Starting cross validation
2025-05-03 16:35:18,136:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:18,505:INFO:Calculating mean and std
2025-05-03 16:35:18,505:INFO:Creating metrics dataframe
2025-05-03 16:35:18,506:INFO:Finalizing model
2025-05-03 16:35:18,646:INFO:Uploading results into container
2025-05-03 16:35:18,646:INFO:Uploading model into container now
2025-05-03 16:35:18,656:INFO:_master_model_container: 8
2025-05-03 16:35:18,656:INFO:_display_container: 4
2025-05-03 16:35:18,658:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:35:18,660:INFO:create_model() successfully completed......................................
2025-05-03 16:35:18,791:INFO:Initializing create_model()
2025-05-03 16:35:18,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:35:18,791:INFO:Checking exceptions
2025-05-03 16:35:18,811:INFO:Importing libraries
2025-05-03 16:35:18,811:INFO:Copying training dataset
2025-05-03 16:35:18,829:INFO:Defining folds
2025-05-03 16:35:18,829:INFO:Declaring metric variables
2025-05-03 16:35:18,832:INFO:Importing untrained model
2025-05-03 16:35:18,836:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:35:18,842:INFO:Starting cross validation
2025-05-03 16:35:18,843:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:35:20,233:INFO:Calculating mean and std
2025-05-03 16:35:20,233:INFO:Creating metrics dataframe
2025-05-03 16:35:20,238:INFO:Finalizing model
2025-05-03 16:35:20,826:INFO:Uploading results into container
2025-05-03 16:35:20,826:INFO:Uploading model into container now
2025-05-03 16:35:20,826:INFO:_master_model_container: 9
2025-05-03 16:35:20,826:INFO:_display_container: 5
2025-05-03 16:35:20,826:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:35:20,826:INFO:create_model() successfully completed......................................
2025-05-03 16:35:20,945:INFO:Initializing tune_model()
2025-05-03 16:35:20,945:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 16:35:20,945:INFO:Checking exceptions
2025-05-03 16:35:20,945:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 16:35:20,971:INFO:Copying training dataset
2025-05-03 16:35:20,981:INFO:Checking base model
2025-05-03 16:35:20,982:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 16:35:20,985:INFO:Declaring metric variables
2025-05-03 16:35:20,988:INFO:Defining Hyperparameters
2025-05-03 16:35:21,090:INFO:Tuning with n_jobs=-1
2025-05-03 16:35:21,091:INFO:Initializing skopt.BayesSearchCV
2025-05-03 16:37:01,926:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 16:37:01,926:INFO:Hyperparameter search completed
2025-05-03 16:37:01,926:INFO:SubProcess create_model() called ==================================
2025-05-03 16:37:01,927:INFO:Initializing create_model()
2025-05-03 16:37:01,927:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD96C0B50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 16:37:01,927:INFO:Checking exceptions
2025-05-03 16:37:01,928:INFO:Importing libraries
2025-05-03 16:37:01,928:INFO:Copying training dataset
2025-05-03 16:37:01,942:INFO:Defining folds
2025-05-03 16:37:01,942:INFO:Declaring metric variables
2025-05-03 16:37:01,942:INFO:Importing untrained model
2025-05-03 16:37:01,942:INFO:Declaring custom model
2025-05-03 16:37:01,947:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:37:01,947:INFO:Starting cross validation
2025-05-03 16:37:01,947:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:37:02,341:INFO:Calculating mean and std
2025-05-03 16:37:02,341:INFO:Creating metrics dataframe
2025-05-03 16:37:02,349:INFO:Finalizing model
2025-05-03 16:37:02,360:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:37:02,360:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:37:02,360:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:37:02,377:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:37:02,377:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:37:02,377:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:37:02,377:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:37:02,381:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.
2025-05-03 16:37:02,381:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:37:02,381:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:37:02,381:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:37:02,381:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:37:02,381:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:37:02,381:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:37:02,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,485:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,491:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,493:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,497:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,499:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,503:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,505:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,506:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,508:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,510:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,512:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,514:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,516:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,518:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,520:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,522:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,523:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,524:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,526:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,528:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,532:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,539:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,539:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,541:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,541:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,542:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,543:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,543:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,544:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,545:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,548:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,550:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,552:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:37:02,554:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:37:02,564:INFO:Uploading results into container
2025-05-03 16:37:02,566:INFO:Uploading model into container now
2025-05-03 16:37:02,566:INFO:_master_model_container: 10
2025-05-03 16:37:02,566:INFO:_display_container: 6
2025-05-03 16:37:02,568:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:37:02,568:INFO:create_model() successfully completed......................................
2025-05-03 16:37:02,746:INFO:SubProcess create_model() end ==================================
2025-05-03 16:37:02,746:INFO:choose_better activated
2025-05-03 16:37:02,751:INFO:SubProcess create_model() called ==================================
2025-05-03 16:37:02,759:INFO:Initializing create_model()
2025-05-03 16:37:02,759:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:37:02,759:INFO:Checking exceptions
2025-05-03 16:37:02,764:INFO:Importing libraries
2025-05-03 16:37:02,764:INFO:Copying training dataset
2025-05-03 16:37:02,817:INFO:Defining folds
2025-05-03 16:37:02,817:INFO:Declaring metric variables
2025-05-03 16:37:02,817:INFO:Importing untrained model
2025-05-03 16:37:02,817:INFO:Declaring custom model
2025-05-03 16:37:02,817:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:37:02,817:INFO:Starting cross validation
2025-05-03 16:37:02,824:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:37:03,378:INFO:Calculating mean and std
2025-05-03 16:37:03,378:INFO:Creating metrics dataframe
2025-05-03 16:37:03,380:INFO:Finalizing model
2025-05-03 16:37:03,407:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:37:03,409:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.
2025-05-03 16:37:03,409:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:37:03,409:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:37:03,411:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:37:03,411:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:37:03,411:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:37:03,411:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:37:03,540:INFO:Uploading results into container
2025-05-03 16:37:03,540:INFO:Uploading model into container now
2025-05-03 16:37:03,542:INFO:_master_model_container: 11
2025-05-03 16:37:03,542:INFO:_display_container: 7
2025-05-03 16:37:03,542:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:37:03,542:INFO:create_model() successfully completed......................................
2025-05-03 16:37:03,674:INFO:SubProcess create_model() end ==================================
2025-05-03 16:37:03,674:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 16:37:03,674:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 16:37:03,674:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 16:37:03,674:INFO:choose_better completed
2025-05-03 16:37:03,690:INFO:_master_model_container: 11
2025-05-03 16:37:03,690:INFO:_display_container: 6
2025-05-03 16:37:03,690:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:37:03,690:INFO:tune_model() successfully completed......................................
2025-05-03 16:37:03,808:INFO:Initializing tune_model()
2025-05-03 16:37:03,809:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 16:37:03,809:INFO:Checking exceptions
2025-05-03 16:37:03,809:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 16:37:03,829:INFO:Copying training dataset
2025-05-03 16:37:03,841:INFO:Checking base model
2025-05-03 16:37:03,841:INFO:Base model : Extreme Gradient Boosting
2025-05-03 16:37:03,844:INFO:Declaring metric variables
2025-05-03 16:37:03,853:INFO:Defining Hyperparameters
2025-05-03 16:37:04,055:INFO:Tuning with n_jobs=-1
2025-05-03 16:37:04,065:INFO:Initializing skopt.BayesSearchCV
2025-05-03 16:38:31,601:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 16:38:31,601:INFO:Hyperparameter search completed
2025-05-03 16:38:31,601:INFO:SubProcess create_model() called ==================================
2025-05-03 16:38:31,601:INFO:Initializing create_model()
2025-05-03 16:38:31,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD0ED8DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 16:38:31,601:INFO:Checking exceptions
2025-05-03 16:38:31,601:INFO:Importing libraries
2025-05-03 16:38:31,617:INFO:Copying training dataset
2025-05-03 16:38:31,618:INFO:Defining folds
2025-05-03 16:38:31,618:INFO:Declaring metric variables
2025-05-03 16:38:31,634:INFO:Importing untrained model
2025-05-03 16:38:31,634:INFO:Declaring custom model
2025-05-03 16:38:31,637:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:38:31,637:INFO:Starting cross validation
2025-05-03 16:38:31,637:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:38:32,099:INFO:Calculating mean and std
2025-05-03 16:38:32,100:INFO:Creating metrics dataframe
2025-05-03 16:38:32,101:INFO:Finalizing model
2025-05-03 16:38:32,336:INFO:Uploading results into container
2025-05-03 16:38:32,337:INFO:Uploading model into container now
2025-05-03 16:38:32,337:INFO:_master_model_container: 12
2025-05-03 16:38:32,337:INFO:_display_container: 7
2025-05-03 16:38:32,339:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:38:32,339:INFO:create_model() successfully completed......................................
2025-05-03 16:38:32,467:INFO:SubProcess create_model() end ==================================
2025-05-03 16:38:32,467:INFO:choose_better activated
2025-05-03 16:38:32,467:INFO:SubProcess create_model() called ==================================
2025-05-03 16:38:32,467:INFO:Initializing create_model()
2025-05-03 16:38:32,467:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:38:32,467:INFO:Checking exceptions
2025-05-03 16:38:32,467:INFO:Importing libraries
2025-05-03 16:38:32,467:INFO:Copying training dataset
2025-05-03 16:38:32,485:INFO:Defining folds
2025-05-03 16:38:32,485:INFO:Declaring metric variables
2025-05-03 16:38:32,485:INFO:Importing untrained model
2025-05-03 16:38:32,485:INFO:Declaring custom model
2025-05-03 16:38:32,485:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:38:32,485:INFO:Starting cross validation
2025-05-03 16:38:32,485:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:38:32,777:INFO:Calculating mean and std
2025-05-03 16:38:32,777:INFO:Creating metrics dataframe
2025-05-03 16:38:32,777:INFO:Finalizing model
2025-05-03 16:38:32,943:INFO:Uploading results into container
2025-05-03 16:38:32,945:INFO:Uploading model into container now
2025-05-03 16:38:32,945:INFO:_master_model_container: 13
2025-05-03 16:38:32,945:INFO:_display_container: 8
2025-05-03 16:38:32,947:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 16:38:32,947:INFO:create_model() successfully completed......................................
2025-05-03 16:38:33,085:INFO:SubProcess create_model() end ==================================
2025-05-03 16:38:33,085:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 16:38:33,087:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 16:38:33,087:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 16:38:33,087:INFO:choose_better completed
2025-05-03 16:38:33,093:INFO:_master_model_container: 13
2025-05-03 16:38:33,093:INFO:_display_container: 7
2025-05-03 16:38:33,094:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:38:33,094:INFO:tune_model() successfully completed......................................
2025-05-03 16:38:33,200:INFO:Initializing tune_model()
2025-05-03 16:38:33,200:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 16:38:33,200:INFO:Checking exceptions
2025-05-03 16:38:33,200:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 16:38:33,229:INFO:Copying training dataset
2025-05-03 16:38:33,243:INFO:Checking base model
2025-05-03 16:38:33,243:INFO:Base model : Random Forest Classifier
2025-05-03 16:38:33,246:INFO:Declaring metric variables
2025-05-03 16:38:33,249:INFO:Defining Hyperparameters
2025-05-03 16:38:33,354:INFO:Tuning with n_jobs=-1
2025-05-03 16:38:33,366:INFO:Initializing skopt.BayesSearchCV
2025-05-03 16:42:15,350:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 16:42:15,350:INFO:Hyperparameter search completed
2025-05-03 16:42:15,350:INFO:SubProcess create_model() called ==================================
2025-05-03 16:42:15,350:INFO:Initializing create_model()
2025-05-03 16:42:15,350:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002BAD95A3510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 16:42:15,350:INFO:Checking exceptions
2025-05-03 16:42:15,350:INFO:Importing libraries
2025-05-03 16:42:15,350:INFO:Copying training dataset
2025-05-03 16:42:15,367:INFO:Defining folds
2025-05-03 16:42:15,367:INFO:Declaring metric variables
2025-05-03 16:42:15,367:INFO:Importing untrained model
2025-05-03 16:42:15,367:INFO:Declaring custom model
2025-05-03 16:42:15,384:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:42:15,384:INFO:Starting cross validation
2025-05-03 16:42:15,384:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:42:19,876:INFO:Calculating mean and std
2025-05-03 16:42:19,877:INFO:Creating metrics dataframe
2025-05-03 16:42:19,881:INFO:Finalizing model
2025-05-03 16:42:22,228:INFO:Uploading results into container
2025-05-03 16:42:22,228:INFO:Uploading model into container now
2025-05-03 16:42:22,228:INFO:_master_model_container: 14
2025-05-03 16:42:22,228:INFO:_display_container: 8
2025-05-03 16:42:22,228:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:42:22,228:INFO:create_model() successfully completed......................................
2025-05-03 16:42:22,328:INFO:SubProcess create_model() end ==================================
2025-05-03 16:42:22,328:INFO:choose_better activated
2025-05-03 16:42:22,328:INFO:SubProcess create_model() called ==================================
2025-05-03 16:42:22,328:INFO:Initializing create_model()
2025-05-03 16:42:22,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 16:42:22,328:INFO:Checking exceptions
2025-05-03 16:42:22,328:INFO:Importing libraries
2025-05-03 16:42:22,328:INFO:Copying training dataset
2025-05-03 16:42:22,344:INFO:Defining folds
2025-05-03 16:42:22,344:INFO:Declaring metric variables
2025-05-03 16:42:22,344:INFO:Importing untrained model
2025-05-03 16:42:22,344:INFO:Declaring custom model
2025-05-03 16:42:22,344:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:42:22,344:INFO:Starting cross validation
2025-05-03 16:42:22,344:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:42:23,704:INFO:Calculating mean and std
2025-05-03 16:42:23,704:INFO:Creating metrics dataframe
2025-05-03 16:42:23,704:INFO:Finalizing model
2025-05-03 16:42:24,321:INFO:Uploading results into container
2025-05-03 16:42:24,321:INFO:Uploading model into container now
2025-05-03 16:42:24,321:INFO:_master_model_container: 15
2025-05-03 16:42:24,321:INFO:_display_container: 9
2025-05-03 16:42:24,321:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 16:42:24,321:INFO:create_model() successfully completed......................................
2025-05-03 16:42:24,415:INFO:SubProcess create_model() end ==================================
2025-05-03 16:42:24,415:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 16:42:24,426:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 16:42:24,426:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 16:42:24,426:INFO:choose_better completed
2025-05-03 16:42:24,432:INFO:_master_model_container: 15
2025-05-03 16:42:24,432:INFO:_display_container: 8
2025-05-03 16:42:24,433:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:42:24,433:INFO:tune_model() successfully completed......................................
2025-05-03 16:42:24,562:INFO:Initializing create_model()
2025-05-03 16:42:24,562:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 16:42:24,562:INFO:Checking exceptions
2025-05-03 16:42:24,574:INFO:Importing libraries
2025-05-03 16:42:24,574:INFO:Copying training dataset
2025-05-03 16:42:24,591:INFO:Defining folds
2025-05-03 16:42:24,591:INFO:Declaring metric variables
2025-05-03 16:42:24,595:INFO:Importing untrained model
2025-05-03 16:42:24,599:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 16:42:24,605:INFO:Starting cross validation
2025-05-03 16:42:24,606:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:42:25,046:INFO:Calculating mean and std
2025-05-03 16:42:25,046:INFO:Creating metrics dataframe
2025-05-03 16:42:25,052:INFO:Finalizing model
2025-05-03 16:42:25,065:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:42:25,065:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:42:25,065:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:42:25,084:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 16:42:25,084:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 16:42:25,084:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 16:42:25,084:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 16:42:25,088:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.
2025-05-03 16:42:25,088:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 16:42:25,088:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 16:42:25,088:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 16:42:25,088:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 16:42:25,088:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 16:42:25,088:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 16:42:25,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,181:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,181:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,185:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,192:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,192:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,193:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,194:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,196:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,197:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,200:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,200:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,200:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,201:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,202:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,203:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,204:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,205:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,205:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,206:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,206:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,207:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,207:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,208:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,208:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,210:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,210:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,215:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,219:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,219:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,220:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,220:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,220:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,221:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,221:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,222:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,222:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,223:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,223:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,224:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,224:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,225:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,227:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,235:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,237:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,239:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,241:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,241:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,243:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,244:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,246:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,248:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,250:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 16:42:25,250:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 16:42:25,260:INFO:Uploading results into container
2025-05-03 16:42:25,262:INFO:Uploading model into container now
2025-05-03 16:42:25,270:INFO:_master_model_container: 16
2025-05-03 16:42:25,270:INFO:_display_container: 9
2025-05-03 16:42:25,273:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 16:42:25,273:INFO:create_model() successfully completed......................................
2025-05-03 16:42:25,392:INFO:Initializing create_model()
2025-05-03 16:42:25,392:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 16:42:25,392:INFO:Checking exceptions
2025-05-03 16:42:25,417:INFO:Importing libraries
2025-05-03 16:42:25,417:INFO:Copying training dataset
2025-05-03 16:42:25,432:INFO:Defining folds
2025-05-03 16:42:25,433:INFO:Declaring metric variables
2025-05-03 16:42:25,436:INFO:Importing untrained model
2025-05-03 16:42:25,440:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 16:42:25,445:INFO:Starting cross validation
2025-05-03 16:42:25,446:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:42:25,926:INFO:Calculating mean and std
2025-05-03 16:42:25,926:INFO:Creating metrics dataframe
2025-05-03 16:42:25,926:INFO:Finalizing model
2025-05-03 16:42:26,125:INFO:Uploading results into container
2025-05-03 16:42:26,128:INFO:Uploading model into container now
2025-05-03 16:42:26,136:INFO:_master_model_container: 17
2025-05-03 16:42:26,136:INFO:_display_container: 10
2025-05-03 16:42:26,139:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 16:42:26,139:INFO:create_model() successfully completed......................................
2025-05-03 16:42:26,275:INFO:Initializing create_model()
2025-05-03 16:42:26,275:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 16:42:26,275:INFO:Checking exceptions
2025-05-03 16:42:26,284:INFO:Importing libraries
2025-05-03 16:42:26,286:INFO:Copying training dataset
2025-05-03 16:42:26,301:INFO:Defining folds
2025-05-03 16:42:26,301:INFO:Declaring metric variables
2025-05-03 16:42:26,305:INFO:Importing untrained model
2025-05-03 16:42:26,309:INFO:Random Forest Classifier Imported successfully
2025-05-03 16:42:26,315:INFO:Starting cross validation
2025-05-03 16:42:26,315:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 16:42:30,673:INFO:Calculating mean and std
2025-05-03 16:42:30,673:INFO:Creating metrics dataframe
2025-05-03 16:42:30,679:INFO:Finalizing model
2025-05-03 16:42:33,239:INFO:Uploading results into container
2025-05-03 16:42:33,239:INFO:Uploading model into container now
2025-05-03 16:42:33,250:INFO:_master_model_container: 18
2025-05-03 16:42:33,250:INFO:_display_container: 11
2025-05-03 16:42:33,250:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 16:42:33,250:INFO:create_model() successfully completed......................................
2025-05-03 16:42:33,368:INFO:Initializing interpret_model()
2025-05-03 16:42:33,368:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:33,368:INFO:Checking exceptions
2025-05-03 16:42:33,368:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:42:33,388:INFO:plot type: msa
2025-05-03 16:42:33,766:INFO:Visual Rendered Successfully
2025-05-03 16:42:33,766:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:34,010:INFO:Initializing interpret_model()
2025-05-03 16:42:34,010:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:34,010:INFO:Checking exceptions
2025-05-03 16:42:34,010:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:42:34,022:INFO:plot type: msa
2025-05-03 16:42:34,283:INFO:Visual Rendered Successfully
2025-05-03 16:42:34,283:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:34,422:INFO:Initializing interpret_model()
2025-05-03 16:42:34,422:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:34,422:INFO:Checking exceptions
2025-05-03 16:42:34,422:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 16:42:34,435:INFO:plot type: msa
2025-05-03 16:42:34,435:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:42:34,668:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 16:42:34,891:INFO:Visual Rendered Successfully
2025-05-03 16:42:34,891:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:35,004:INFO:Initializing save_model()
2025-05-03 16:42:35,004:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:42:35,004:INFO:Adding model into prep_pipe
2025-05-03 16:42:35,022:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 16:42:35,026:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 16:42:35,026:INFO:save_model() successfully completed......................................
2025-05-03 16:42:35,166:INFO:Initializing save_model()
2025-05-03 16:42:35,166:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:42:35,166:INFO:Adding model into prep_pipe
2025-05-03 16:42:35,171:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 16:42:35,177:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 16:42:35,177:INFO:save_model() successfully completed......................................
2025-05-03 16:42:35,320:INFO:Initializing save_model()
2025-05-03 16:42:35,320:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:42:35,320:INFO:Adding model into prep_pipe
2025-05-03 16:42:35,418:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 16:42:35,418:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 16:42:35,418:INFO:save_model() successfully completed......................................
2025-05-03 16:42:35,596:INFO:Initializing interpret_model()
2025-05-03 16:42:35,597:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:42:35,597:INFO:Checking exceptions
2025-05-03 16:42:35,597:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:42:35,623:INFO:plot type: summary
2025-05-03 16:42:35,623:INFO:Creating TreeExplainer
2025-05-03 16:42:35,658:INFO:Compiling shap values
2025-05-03 16:42:36,574:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:42:36,788:INFO:Visual Rendered Successfully
2025-05-03 16:42:36,788:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:36,901:INFO:Initializing interpret_model()
2025-05-03 16:42:36,902:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:36,902:INFO:Checking exceptions
2025-05-03 16:42:36,902:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:42:36,926:INFO:plot type: summary
2025-05-03 16:42:36,926:INFO:Creating TreeExplainer
2025-05-03 16:42:36,959:INFO:Compiling shap values
2025-05-03 16:42:37,966:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 16:42:39,480:INFO:Visual Rendered Successfully
2025-05-03 16:42:39,480:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:39,650:INFO:Initializing interpret_model()
2025-05-03 16:42:39,650:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 16:42:39,650:INFO:Checking exceptions
2025-05-03 16:42:39,650:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:42:39,669:INFO:plot type: summary
2025-05-03 16:42:39,669:INFO:Creating TreeExplainer
2025-05-03 16:42:39,699:INFO:Compiling shap values
2025-05-03 16:42:41,823:INFO:Visual Rendered Successfully
2025-05-03 16:42:41,823:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:41,966:INFO:Initializing interpret_model()
2025-05-03 16:42:41,966:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:41,966:INFO:Checking exceptions
2025-05-03 16:42:41,966:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:42:41,983:INFO:plot type: summary
2025-05-03 16:42:41,983:INFO:Creating TreeExplainer
2025-05-03 16:42:42,020:INFO:Compiling shap values
2025-05-03 16:42:45,169:INFO:Visual Rendered Successfully
2025-05-03 16:42:45,181:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:45,305:INFO:Initializing interpret_model()
2025-05-03 16:42:45,305:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:45,305:INFO:Checking exceptions
2025-05-03 16:42:45,305:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:42:45,344:INFO:plot type: pfi
2025-05-03 16:42:45,462:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 16:42:46,881:INFO:Visual Rendered Successfully
2025-05-03 16:42:46,881:INFO:interpret_model() successfully completed......................................
2025-05-03 16:42:46,991:INFO:Initializing interpret_model()
2025-05-03 16:42:46,991:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:42:46,991:INFO:Checking exceptions
2025-05-03 16:42:46,991:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 16:42:47,007:INFO:plot type: summary
2025-05-03 16:42:47,007:INFO:Creating TreeExplainer
2025-05-03 16:42:47,039:INFO:Compiling shap values
2025-05-03 16:49:53,140:INFO:Visual Rendered Successfully
2025-05-03 16:49:53,140:INFO:interpret_model() successfully completed......................................
2025-05-03 16:49:53,357:INFO:Initializing save_model()
2025-05-03 16:49:53,357:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 16:49:53,357:INFO:Adding model into prep_pipe
2025-05-03 16:49:53,505:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 16:49:53,508:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 16:49:53,508:INFO:save_model() successfully completed......................................
2025-05-03 16:49:53,861:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:97: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 16:49:53,861:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:132: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 16:51:05,900:INFO:Initializing interpret_model()
2025-05-03 16:51:05,900:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002BAD8A99A50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 16:51:05,900:INFO:Checking exceptions
2025-05-03 16:51:05,900:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 16:51:05,965:INFO:plot type: pfi
2025-05-03 16:51:07,382:INFO:Visual Rendered Successfully
2025-05-03 16:51:07,382:INFO:interpret_model() successfully completed......................................
2025-05-03 17:37:54,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:37:54,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:37:54,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:37:54,266:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:37:57,621:INFO:PyCaret ClassificationExperiment
2025-05-03 17:37:57,621:INFO:Logging name: clf-default-name
2025-05-03 17:37:57,621:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 17:37:57,621:INFO:version 3.3.2
2025-05-03 17:37:57,621:INFO:Initializing setup()
2025-05-03 17:37:57,621:INFO:self.USI: ae22
2025-05-03 17:37:57,621:INFO:self._variable_keys: {'html_param', 'seed', 'fix_imbalance', 'is_multiclass', 'n_jobs_param', 'idx', 'USI', 'y', 'fold_generator', 'y_test', 'X', 'y_train', 'exp_id', 'X_test', 'exp_name_log', '_ml_usecase', 'fold_groups_param', 'X_train', 'log_plots_param', 'gpu_n_jobs_param', 'pipeline', '_available_plots', 'fold_shuffle_param', 'logging_param', 'memory', 'data', 'target_param', 'gpu_param'}
2025-05-03 17:37:57,621:INFO:Checking environment
2025-05-03 17:37:57,631:INFO:python_version: 3.11.11
2025-05-03 17:37:57,631:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 17:37:57,631:INFO:machine: AMD64
2025-05-03 17:37:57,631:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 17:37:57,631:INFO:Memory: svmem(total=16965230592, available=4716761088, percent=72.2, used=12248469504, free=4716761088)
2025-05-03 17:37:57,631:INFO:Physical Core: 4
2025-05-03 17:37:57,631:INFO:Logical Core: 8
2025-05-03 17:37:57,631:INFO:Checking libraries
2025-05-03 17:37:57,631:INFO:System:
2025-05-03 17:37:57,631:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 17:37:57,631:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 17:37:57,631:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 17:37:57,631:INFO:PyCaret required dependencies:
2025-05-03 17:37:57,631:INFO:                 pip: 25.0
2025-05-03 17:37:57,631:INFO:          setuptools: 75.8.0
2025-05-03 17:37:57,631:INFO:             pycaret: 3.3.2
2025-05-03 17:37:57,631:INFO:             IPython: 8.32.0
2025-05-03 17:37:57,631:INFO:          ipywidgets: 8.1.6
2025-05-03 17:37:57,631:INFO:                tqdm: 4.67.1
2025-05-03 17:37:57,631:INFO:               numpy: 1.26.4
2025-05-03 17:37:57,631:INFO:              pandas: 2.1.4
2025-05-03 17:37:57,631:INFO:              jinja2: 3.1.6
2025-05-03 17:37:57,631:INFO:               scipy: 1.11.4
2025-05-03 17:37:57,631:INFO:              joblib: 1.3.2
2025-05-03 17:37:57,631:INFO:             sklearn: 1.4.2
2025-05-03 17:37:57,631:INFO:                pyod: 2.0.5
2025-05-03 17:37:57,631:INFO:            imblearn: 0.13.0
2025-05-03 17:37:57,631:INFO:   category_encoders: 2.7.0
2025-05-03 17:37:57,631:INFO:            lightgbm: 4.6.0
2025-05-03 17:37:57,631:INFO:               numba: 0.61.0
2025-05-03 17:37:57,631:INFO:            requests: 2.32.3
2025-05-03 17:37:57,631:INFO:          matplotlib: 3.7.5
2025-05-03 17:37:57,631:INFO:          scikitplot: 0.3.7
2025-05-03 17:37:57,631:INFO:         yellowbrick: 1.5
2025-05-03 17:37:57,631:INFO:              plotly: 5.24.1
2025-05-03 17:37:57,631:INFO:    plotly-resampler: Not installed
2025-05-03 17:37:57,631:INFO:             kaleido: 0.2.1
2025-05-03 17:37:57,631:INFO:           schemdraw: 0.15
2025-05-03 17:37:57,631:INFO:         statsmodels: 0.14.4
2025-05-03 17:37:57,631:INFO:              sktime: 0.26.0
2025-05-03 17:37:57,631:INFO:               tbats: 1.1.3
2025-05-03 17:37:57,631:INFO:            pmdarima: 2.0.4
2025-05-03 17:37:57,631:INFO:              psutil: 6.1.1
2025-05-03 17:37:57,631:INFO:          markupsafe: 3.0.2
2025-05-03 17:37:57,631:INFO:             pickle5: Not installed
2025-05-03 17:37:57,631:INFO:         cloudpickle: 3.1.1
2025-05-03 17:37:57,631:INFO:         deprecation: 2.1.0
2025-05-03 17:37:57,631:INFO:              xxhash: 3.5.0
2025-05-03 17:37:57,631:INFO:           wurlitzer: Not installed
2025-05-03 17:37:57,631:INFO:PyCaret optional dependencies:
2025-05-03 17:37:58,016:INFO:                shap: 0.46.0
2025-05-03 17:37:58,016:INFO:           interpret: 0.6.9
2025-05-03 17:37:58,016:INFO:                umap: Not installed
2025-05-03 17:37:58,016:INFO:     ydata_profiling: Not installed
2025-05-03 17:37:58,016:INFO:  explainerdashboard: Not installed
2025-05-03 17:37:58,016:INFO:             autoviz: Not installed
2025-05-03 17:37:58,016:INFO:           fairlearn: Not installed
2025-05-03 17:37:58,016:INFO:          deepchecks: Not installed
2025-05-03 17:37:58,016:INFO:             xgboost: 3.0.0
2025-05-03 17:37:58,016:INFO:            catboost: Not installed
2025-05-03 17:37:58,016:INFO:              kmodes: Not installed
2025-05-03 17:37:58,016:INFO:             mlxtend: Not installed
2025-05-03 17:37:58,016:INFO:       statsforecast: Not installed
2025-05-03 17:37:58,016:INFO:        tune_sklearn: Not installed
2025-05-03 17:37:58,016:INFO:                 ray: Not installed
2025-05-03 17:37:58,016:INFO:            hyperopt: 0.2.7
2025-05-03 17:37:58,016:INFO:              optuna: Not installed
2025-05-03 17:37:58,016:INFO:               skopt: 0.10.2
2025-05-03 17:37:58,016:INFO:              mlflow: 2.22.0
2025-05-03 17:37:58,016:INFO:              gradio: Not installed
2025-05-03 17:37:58,016:INFO:             fastapi: 0.115.12
2025-05-03 17:37:58,016:INFO:             uvicorn: 0.34.2
2025-05-03 17:37:58,016:INFO:              m2cgen: Not installed
2025-05-03 17:37:58,016:INFO:           evidently: Not installed
2025-05-03 17:37:58,016:INFO:               fugue: Not installed
2025-05-03 17:37:58,016:INFO:           streamlit: Not installed
2025-05-03 17:37:58,016:INFO:             prophet: Not installed
2025-05-03 17:37:58,016:INFO:None
2025-05-03 17:37:58,016:INFO:Set up data.
2025-05-03 17:37:58,035:INFO:Set up folding strategy.
2025-05-03 17:37:58,035:INFO:Set up train/test split.
2025-05-03 17:37:58,050:INFO:Set up index.
2025-05-03 17:37:58,050:INFO:Assigning column types.
2025-05-03 17:37:58,067:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 17:37:58,112:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,118:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,152:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,154:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,199:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,199:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,276:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,283:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,284:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 17:37:58,397:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,423:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,425:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,461:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:37:58,477:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,493:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,493:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 17:37:58,557:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,620:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,622:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,624:INFO:Set up column name cleaning.
2025-05-03 17:37:58,641:INFO:Finished creating preprocessing pipeline.
2025-05-03 17:37:58,645:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 17:37:58,645:INFO:Creating final display dataframe.
2025-05-03 17:37:58,741:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 17:37:58,804:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,867:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:37:58,867:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:37:58,867:INFO:setup() successfully completed in 1.25s...............
2025-05-03 17:37:58,892:INFO:Initializing compare_models()
2025-05-03 17:37:58,893:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 17:37:58,893:INFO:Checking exceptions
2025-05-03 17:37:58,905:INFO:Preparing display monitor
2025-05-03 17:37:58,928:INFO:Initializing Logistic Regression
2025-05-03 17:37:58,928:INFO:Total runtime is 0.0 minutes
2025-05-03 17:37:58,932:INFO:SubProcess create_model() called ==================================
2025-05-03 17:37:58,933:INFO:Initializing create_model()
2025-05-03 17:37:58,933:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:37:58,933:INFO:Checking exceptions
2025-05-03 17:37:58,933:INFO:Importing libraries
2025-05-03 17:37:58,933:INFO:Copying training dataset
2025-05-03 17:37:58,950:INFO:Defining folds
2025-05-03 17:37:58,950:INFO:Declaring metric variables
2025-05-03 17:37:58,953:INFO:Importing untrained model
2025-05-03 17:37:58,957:INFO:Logistic Regression Imported successfully
2025-05-03 17:37:58,963:INFO:Starting cross validation
2025-05-03 17:37:58,964:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:02,941:INFO:Calculating mean and std
2025-05-03 17:38:02,941:INFO:Creating metrics dataframe
2025-05-03 17:38:02,943:INFO:Uploading results into container
2025-05-03 17:38:02,945:INFO:Uploading model into container now
2025-05-03 17:38:02,945:INFO:_master_model_container: 1
2025-05-03 17:38:02,945:INFO:_display_container: 2
2025-05-03 17:38:02,945:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 17:38:02,945:INFO:create_model() successfully completed......................................
2025-05-03 17:38:03,045:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:03,045:INFO:Creating metrics dataframe
2025-05-03 17:38:03,045:INFO:Initializing Random Forest Classifier
2025-05-03 17:38:03,059:INFO:Total runtime is 0.06885161399841308 minutes
2025-05-03 17:38:03,059:INFO:SubProcess create_model() called ==================================
2025-05-03 17:38:03,059:INFO:Initializing create_model()
2025-05-03 17:38:03,059:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:03,059:INFO:Checking exceptions
2025-05-03 17:38:03,059:INFO:Importing libraries
2025-05-03 17:38:03,059:INFO:Copying training dataset
2025-05-03 17:38:03,075:INFO:Defining folds
2025-05-03 17:38:03,075:INFO:Declaring metric variables
2025-05-03 17:38:03,075:INFO:Importing untrained model
2025-05-03 17:38:03,083:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:38:03,090:INFO:Starting cross validation
2025-05-03 17:38:03,091:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:07,561:INFO:Calculating mean and std
2025-05-03 17:38:07,561:INFO:Creating metrics dataframe
2025-05-03 17:38:07,561:INFO:Uploading results into container
2025-05-03 17:38:07,561:INFO:Uploading model into container now
2025-05-03 17:38:07,561:INFO:_master_model_container: 2
2025-05-03 17:38:07,561:INFO:_display_container: 2
2025-05-03 17:38:07,561:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:38:07,561:INFO:create_model() successfully completed......................................
2025-05-03 17:38:07,674:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:07,675:INFO:Creating metrics dataframe
2025-05-03 17:38:07,675:INFO:Initializing Extreme Gradient Boosting
2025-05-03 17:38:07,675:INFO:Total runtime is 0.14577234586079915 minutes
2025-05-03 17:38:07,682:INFO:SubProcess create_model() called ==================================
2025-05-03 17:38:07,682:INFO:Initializing create_model()
2025-05-03 17:38:07,682:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:07,682:INFO:Checking exceptions
2025-05-03 17:38:07,682:INFO:Importing libraries
2025-05-03 17:38:07,682:INFO:Copying training dataset
2025-05-03 17:38:07,694:INFO:Defining folds
2025-05-03 17:38:07,694:INFO:Declaring metric variables
2025-05-03 17:38:07,703:INFO:Importing untrained model
2025-05-03 17:38:07,708:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:38:07,713:INFO:Starting cross validation
2025-05-03 17:38:07,713:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:10,531:INFO:Calculating mean and std
2025-05-03 17:38:10,531:INFO:Creating metrics dataframe
2025-05-03 17:38:10,531:INFO:Uploading results into container
2025-05-03 17:38:10,531:INFO:Uploading model into container now
2025-05-03 17:38:10,531:INFO:_master_model_container: 3
2025-05-03 17:38:10,531:INFO:_display_container: 2
2025-05-03 17:38:10,531:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:38:10,531:INFO:create_model() successfully completed......................................
2025-05-03 17:38:10,650:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:10,650:INFO:Creating metrics dataframe
2025-05-03 17:38:10,656:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 17:38:10,656:INFO:Total runtime is 0.19546265999476115 minutes
2025-05-03 17:38:10,660:INFO:SubProcess create_model() called ==================================
2025-05-03 17:38:10,660:INFO:Initializing create_model()
2025-05-03 17:38:10,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:10,660:INFO:Checking exceptions
2025-05-03 17:38:10,660:INFO:Importing libraries
2025-05-03 17:38:10,660:INFO:Copying training dataset
2025-05-03 17:38:10,673:INFO:Defining folds
2025-05-03 17:38:10,673:INFO:Declaring metric variables
2025-05-03 17:38:10,680:INFO:Importing untrained model
2025-05-03 17:38:10,683:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:38:10,690:INFO:Starting cross validation
2025-05-03 17:38:10,690:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:11,236:INFO:Calculating mean and std
2025-05-03 17:38:11,238:INFO:Creating metrics dataframe
2025-05-03 17:38:11,240:INFO:Uploading results into container
2025-05-03 17:38:11,240:INFO:Uploading model into container now
2025-05-03 17:38:11,242:INFO:_master_model_container: 4
2025-05-03 17:38:11,242:INFO:_display_container: 2
2025-05-03 17:38:11,242:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:38:11,242:INFO:create_model() successfully completed......................................
2025-05-03 17:38:11,354:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:11,354:INFO:Creating metrics dataframe
2025-05-03 17:38:11,370:INFO:Initializing Extra Trees Classifier
2025-05-03 17:38:11,370:INFO:Total runtime is 0.2073658506075541 minutes
2025-05-03 17:38:11,380:INFO:SubProcess create_model() called ==================================
2025-05-03 17:38:11,380:INFO:Initializing create_model()
2025-05-03 17:38:11,380:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:11,380:INFO:Checking exceptions
2025-05-03 17:38:11,380:INFO:Importing libraries
2025-05-03 17:38:11,380:INFO:Copying training dataset
2025-05-03 17:38:11,402:INFO:Defining folds
2025-05-03 17:38:11,402:INFO:Declaring metric variables
2025-05-03 17:38:11,402:INFO:Importing untrained model
2025-05-03 17:38:11,402:INFO:Extra Trees Classifier Imported successfully
2025-05-03 17:38:11,417:INFO:Starting cross validation
2025-05-03 17:38:11,417:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:12,597:INFO:Calculating mean and std
2025-05-03 17:38:12,598:INFO:Creating metrics dataframe
2025-05-03 17:38:12,600:INFO:Uploading results into container
2025-05-03 17:38:12,600:INFO:Uploading model into container now
2025-05-03 17:38:12,602:INFO:_master_model_container: 5
2025-05-03 17:38:12,602:INFO:_display_container: 2
2025-05-03 17:38:12,602:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 17:38:12,602:INFO:create_model() successfully completed......................................
2025-05-03 17:38:12,707:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:12,709:INFO:Creating metrics dataframe
2025-05-03 17:38:12,714:INFO:Initializing Ridge Classifier
2025-05-03 17:38:12,714:INFO:Total runtime is 0.22975744406382242 minutes
2025-05-03 17:38:12,716:INFO:SubProcess create_model() called ==================================
2025-05-03 17:38:12,717:INFO:Initializing create_model()
2025-05-03 17:38:12,717:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002D00DB84850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:12,717:INFO:Checking exceptions
2025-05-03 17:38:12,717:INFO:Importing libraries
2025-05-03 17:38:12,717:INFO:Copying training dataset
2025-05-03 17:38:12,721:INFO:Defining folds
2025-05-03 17:38:12,721:INFO:Declaring metric variables
2025-05-03 17:38:12,738:INFO:Importing untrained model
2025-05-03 17:38:12,738:INFO:Ridge Classifier Imported successfully
2025-05-03 17:38:12,738:INFO:Starting cross validation
2025-05-03 17:38:12,738:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:12,826:INFO:Calculating mean and std
2025-05-03 17:38:12,826:INFO:Creating metrics dataframe
2025-05-03 17:38:12,826:INFO:Uploading results into container
2025-05-03 17:38:12,826:INFO:Uploading model into container now
2025-05-03 17:38:12,826:INFO:_master_model_container: 6
2025-05-03 17:38:12,826:INFO:_display_container: 2
2025-05-03 17:38:12,826:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 17:38:12,826:INFO:create_model() successfully completed......................................
2025-05-03 17:38:12,937:INFO:SubProcess create_model() end ==================================
2025-05-03 17:38:12,937:INFO:Creating metrics dataframe
2025-05-03 17:38:12,937:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 17:38:12,954:INFO:Initializing create_model()
2025-05-03 17:38:12,954:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:12,954:INFO:Checking exceptions
2025-05-03 17:38:12,954:INFO:Importing libraries
2025-05-03 17:38:12,954:INFO:Copying training dataset
2025-05-03 17:38:13,009:INFO:Defining folds
2025-05-03 17:38:13,009:INFO:Declaring metric variables
2025-05-03 17:38:13,009:INFO:Importing untrained model
2025-05-03 17:38:13,009:INFO:Declaring custom model
2025-05-03 17:38:13,009:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:38:13,009:INFO:Cross validation set to False
2025-05-03 17:38:13,009:INFO:Fitting Model
2025-05-03 17:38:13,042:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:38:13,046:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000642 seconds.
2025-05-03 17:38:13,046:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:38:13,046:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:38:13,046:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:38:13,046:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:38:13,046:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:38:13,046:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:38:13,174:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:38:13,174:INFO:create_model() successfully completed......................................
2025-05-03 17:38:13,320:INFO:_master_model_container: 6
2025-05-03 17:38:13,320:INFO:_display_container: 2
2025-05-03 17:38:13,320:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:38:13,320:INFO:compare_models() successfully completed......................................
2025-05-03 17:38:13,338:INFO:Initializing create_model()
2025-05-03 17:38:13,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:13,338:INFO:Checking exceptions
2025-05-03 17:38:13,353:INFO:Importing libraries
2025-05-03 17:38:13,353:INFO:Copying training dataset
2025-05-03 17:38:13,371:INFO:Defining folds
2025-05-03 17:38:13,371:INFO:Declaring metric variables
2025-05-03 17:38:13,376:INFO:Importing untrained model
2025-05-03 17:38:13,380:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:38:13,386:INFO:Starting cross validation
2025-05-03 17:38:13,387:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:13,940:INFO:Calculating mean and std
2025-05-03 17:38:13,940:INFO:Creating metrics dataframe
2025-05-03 17:38:13,946:INFO:Finalizing model
2025-05-03 17:38:13,973:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:38:13,975:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000942 seconds.
2025-05-03 17:38:13,975:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:38:13,977:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:38:13,977:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:38:13,977:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:38:13,977:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:38:13,977:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:38:14,105:INFO:Uploading results into container
2025-05-03 17:38:14,107:INFO:Uploading model into container now
2025-05-03 17:38:14,116:INFO:_master_model_container: 7
2025-05-03 17:38:14,116:INFO:_display_container: 3
2025-05-03 17:38:14,118:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:38:14,118:INFO:create_model() successfully completed......................................
2025-05-03 17:38:14,253:INFO:Initializing create_model()
2025-05-03 17:38:14,253:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:14,253:INFO:Checking exceptions
2025-05-03 17:38:14,278:INFO:Importing libraries
2025-05-03 17:38:14,278:INFO:Copying training dataset
2025-05-03 17:38:14,297:INFO:Defining folds
2025-05-03 17:38:14,297:INFO:Declaring metric variables
2025-05-03 17:38:14,299:INFO:Importing untrained model
2025-05-03 17:38:14,304:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:38:14,311:INFO:Starting cross validation
2025-05-03 17:38:14,311:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:14,673:INFO:Calculating mean and std
2025-05-03 17:38:14,673:INFO:Creating metrics dataframe
2025-05-03 17:38:14,675:INFO:Finalizing model
2025-05-03 17:38:14,876:INFO:Uploading results into container
2025-05-03 17:38:14,878:INFO:Uploading model into container now
2025-05-03 17:38:14,889:INFO:_master_model_container: 8
2025-05-03 17:38:14,889:INFO:_display_container: 4
2025-05-03 17:38:14,889:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:38:14,891:INFO:create_model() successfully completed......................................
2025-05-03 17:38:15,035:INFO:Initializing create_model()
2025-05-03 17:38:15,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:38:15,035:INFO:Checking exceptions
2025-05-03 17:38:15,050:INFO:Importing libraries
2025-05-03 17:38:15,051:INFO:Copying training dataset
2025-05-03 17:38:15,069:INFO:Defining folds
2025-05-03 17:38:15,069:INFO:Declaring metric variables
2025-05-03 17:38:15,073:INFO:Importing untrained model
2025-05-03 17:38:15,076:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:38:15,083:INFO:Starting cross validation
2025-05-03 17:38:15,083:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:38:16,552:INFO:Calculating mean and std
2025-05-03 17:38:16,554:INFO:Creating metrics dataframe
2025-05-03 17:38:16,560:INFO:Finalizing model
2025-05-03 17:38:17,174:INFO:Uploading results into container
2025-05-03 17:38:17,175:INFO:Uploading model into container now
2025-05-03 17:38:17,183:INFO:_master_model_container: 9
2025-05-03 17:38:17,185:INFO:_display_container: 5
2025-05-03 17:38:17,185:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:38:17,185:INFO:create_model() successfully completed......................................
2025-05-03 17:38:17,304:INFO:Initializing tune_model()
2025-05-03 17:38:17,304:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002D00CF4FC90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 17:38:17,304:INFO:Checking exceptions
2025-05-03 17:38:17,304:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 17:38:17,329:INFO:Copying training dataset
2025-05-03 17:38:17,348:INFO:Checking base model
2025-05-03 17:38:17,349:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 17:38:17,353:INFO:Declaring metric variables
2025-05-03 17:38:17,359:INFO:Defining Hyperparameters
2025-05-03 17:38:17,485:INFO:Tuning with n_jobs=-1
2025-05-03 17:38:17,500:INFO:Initializing skopt.BayesSearchCV
2025-05-03 17:56:12,447:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:56:12,447:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:56:12,447:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:56:12,447:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:56:15,292:INFO:PyCaret ClassificationExperiment
2025-05-03 17:56:15,292:INFO:Logging name: clf-default-name
2025-05-03 17:56:15,292:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 17:56:15,292:INFO:version 3.3.2
2025-05-03 17:56:15,292:INFO:Initializing setup()
2025-05-03 17:56:15,292:INFO:self.USI: 3907
2025-05-03 17:56:15,292:INFO:self._variable_keys: {'X_test', 'data', 'fold_groups_param', 'n_jobs_param', 'fix_imbalance', 'log_plots_param', 'seed', 'html_param', 'idx', 'gpu_param', 'fold_shuffle_param', 'gpu_n_jobs_param', 'USI', 'pipeline', 'exp_id', 'logging_param', 'y_test', 'X_train', '_available_plots', '_ml_usecase', 'memory', 'X', 'is_multiclass', 'exp_name_log', 'target_param', 'y', 'y_train', 'fold_generator'}
2025-05-03 17:56:15,292:INFO:Checking environment
2025-05-03 17:56:15,293:INFO:python_version: 3.11.11
2025-05-03 17:56:15,293:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 17:56:15,293:INFO:machine: AMD64
2025-05-03 17:56:15,293:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 17:56:15,295:INFO:Memory: svmem(total=16965230592, available=4743569408, percent=72.0, used=12221661184, free=4743569408)
2025-05-03 17:56:15,295:INFO:Physical Core: 4
2025-05-03 17:56:15,295:INFO:Logical Core: 8
2025-05-03 17:56:15,295:INFO:Checking libraries
2025-05-03 17:56:15,295:INFO:System:
2025-05-03 17:56:15,295:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 17:56:15,295:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 17:56:15,295:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 17:56:15,295:INFO:PyCaret required dependencies:
2025-05-03 17:56:15,295:INFO:                 pip: 25.0
2025-05-03 17:56:15,295:INFO:          setuptools: 75.8.0
2025-05-03 17:56:15,295:INFO:             pycaret: 3.3.2
2025-05-03 17:56:15,295:INFO:             IPython: 8.32.0
2025-05-03 17:56:15,295:INFO:          ipywidgets: 8.1.6
2025-05-03 17:56:15,295:INFO:                tqdm: 4.67.1
2025-05-03 17:56:15,295:INFO:               numpy: 1.26.4
2025-05-03 17:56:15,295:INFO:              pandas: 2.1.4
2025-05-03 17:56:15,295:INFO:              jinja2: 3.1.6
2025-05-03 17:56:15,295:INFO:               scipy: 1.11.4
2025-05-03 17:56:15,295:INFO:              joblib: 1.3.2
2025-05-03 17:56:15,295:INFO:             sklearn: 1.4.2
2025-05-03 17:56:15,295:INFO:                pyod: 2.0.5
2025-05-03 17:56:15,295:INFO:            imblearn: 0.13.0
2025-05-03 17:56:15,295:INFO:   category_encoders: 2.7.0
2025-05-03 17:56:15,295:INFO:            lightgbm: 4.6.0
2025-05-03 17:56:15,295:INFO:               numba: 0.61.0
2025-05-03 17:56:15,295:INFO:            requests: 2.32.3
2025-05-03 17:56:15,295:INFO:          matplotlib: 3.7.5
2025-05-03 17:56:15,295:INFO:          scikitplot: 0.3.7
2025-05-03 17:56:15,295:INFO:         yellowbrick: 1.5
2025-05-03 17:56:15,295:INFO:              plotly: 5.24.1
2025-05-03 17:56:15,295:INFO:    plotly-resampler: Not installed
2025-05-03 17:56:15,295:INFO:             kaleido: 0.2.1
2025-05-03 17:56:15,295:INFO:           schemdraw: 0.15
2025-05-03 17:56:15,295:INFO:         statsmodels: 0.14.4
2025-05-03 17:56:15,295:INFO:              sktime: 0.26.0
2025-05-03 17:56:15,295:INFO:               tbats: 1.1.3
2025-05-03 17:56:15,295:INFO:            pmdarima: 2.0.4
2025-05-03 17:56:15,295:INFO:              psutil: 6.1.1
2025-05-03 17:56:15,295:INFO:          markupsafe: 3.0.2
2025-05-03 17:56:15,295:INFO:             pickle5: Not installed
2025-05-03 17:56:15,295:INFO:         cloudpickle: 3.1.1
2025-05-03 17:56:15,295:INFO:         deprecation: 2.1.0
2025-05-03 17:56:15,295:INFO:              xxhash: 3.5.0
2025-05-03 17:56:15,295:INFO:           wurlitzer: Not installed
2025-05-03 17:56:15,295:INFO:PyCaret optional dependencies:
2025-05-03 17:56:15,645:INFO:                shap: 0.46.0
2025-05-03 17:56:15,645:INFO:           interpret: 0.6.9
2025-05-03 17:56:15,645:INFO:                umap: Not installed
2025-05-03 17:56:15,645:INFO:     ydata_profiling: Not installed
2025-05-03 17:56:15,645:INFO:  explainerdashboard: Not installed
2025-05-03 17:56:15,645:INFO:             autoviz: Not installed
2025-05-03 17:56:15,645:INFO:           fairlearn: Not installed
2025-05-03 17:56:15,645:INFO:          deepchecks: Not installed
2025-05-03 17:56:15,645:INFO:             xgboost: 3.0.0
2025-05-03 17:56:15,645:INFO:            catboost: Not installed
2025-05-03 17:56:15,645:INFO:              kmodes: Not installed
2025-05-03 17:56:15,645:INFO:             mlxtend: Not installed
2025-05-03 17:56:15,645:INFO:       statsforecast: Not installed
2025-05-03 17:56:15,645:INFO:        tune_sklearn: Not installed
2025-05-03 17:56:15,645:INFO:                 ray: Not installed
2025-05-03 17:56:15,645:INFO:            hyperopt: 0.2.7
2025-05-03 17:56:15,645:INFO:              optuna: Not installed
2025-05-03 17:56:15,645:INFO:               skopt: 0.10.2
2025-05-03 17:56:15,645:INFO:              mlflow: 2.22.0
2025-05-03 17:56:15,645:INFO:              gradio: Not installed
2025-05-03 17:56:15,645:INFO:             fastapi: 0.115.12
2025-05-03 17:56:15,645:INFO:             uvicorn: 0.34.2
2025-05-03 17:56:15,645:INFO:              m2cgen: Not installed
2025-05-03 17:56:15,645:INFO:           evidently: Not installed
2025-05-03 17:56:15,645:INFO:               fugue: Not installed
2025-05-03 17:56:15,645:INFO:           streamlit: Not installed
2025-05-03 17:56:15,645:INFO:             prophet: Not installed
2025-05-03 17:56:15,645:INFO:None
2025-05-03 17:56:15,645:INFO:Set up data.
2025-05-03 17:56:15,651:INFO:Set up folding strategy.
2025-05-03 17:56:15,651:INFO:Set up train/test split.
2025-05-03 17:56:15,662:INFO:Set up index.
2025-05-03 17:56:15,662:INFO:Assigning column types.
2025-05-03 17:56:15,679:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 17:56:15,712:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,712:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,746:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:15,746:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:15,779:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,779:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,795:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:15,795:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:15,795:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 17:56:15,829:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,862:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:15,862:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:15,895:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:56:15,912:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:15,912:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:15,912:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 17:56:15,962:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:15,979:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:16,036:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:16,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:16,042:INFO:Set up column name cleaning.
2025-05-03 17:56:16,045:INFO:Finished creating preprocessing pipeline.
2025-05-03 17:56:16,045:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 17:56:16,045:INFO:Creating final display dataframe.
2025-05-03 17:56:16,129:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 17:56:16,178:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:16,178:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:16,245:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:56:16,245:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:56:16,245:INFO:setup() successfully completed in 0.96s...............
2025-05-03 17:56:16,261:INFO:Initializing compare_models()
2025-05-03 17:56:16,261:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 17:56:16,261:INFO:Checking exceptions
2025-05-03 17:56:16,278:INFO:Preparing display monitor
2025-05-03 17:56:16,326:INFO:Initializing Logistic Regression
2025-05-03 17:56:16,327:INFO:Total runtime is 2.3066997528076172e-05 minutes
2025-05-03 17:56:16,332:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:16,334:INFO:Initializing create_model()
2025-05-03 17:56:16,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:16,334:INFO:Checking exceptions
2025-05-03 17:56:16,334:INFO:Importing libraries
2025-05-03 17:56:16,334:INFO:Copying training dataset
2025-05-03 17:56:16,349:INFO:Defining folds
2025-05-03 17:56:16,350:INFO:Declaring metric variables
2025-05-03 17:56:16,352:INFO:Importing untrained model
2025-05-03 17:56:16,356:INFO:Logistic Regression Imported successfully
2025-05-03 17:56:16,362:INFO:Starting cross validation
2025-05-03 17:56:16,364:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:19,646:INFO:Calculating mean and std
2025-05-03 17:56:19,646:INFO:Creating metrics dataframe
2025-05-03 17:56:19,646:INFO:Uploading results into container
2025-05-03 17:56:19,646:INFO:Uploading model into container now
2025-05-03 17:56:19,646:INFO:_master_model_container: 1
2025-05-03 17:56:19,646:INFO:_display_container: 2
2025-05-03 17:56:19,646:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 17:56:19,646:INFO:create_model() successfully completed......................................
2025-05-03 17:56:19,790:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:19,790:INFO:Creating metrics dataframe
2025-05-03 17:56:19,806:INFO:Initializing Random Forest Classifier
2025-05-03 17:56:19,806:INFO:Total runtime is 0.058003322283426924 minutes
2025-05-03 17:56:19,806:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:19,806:INFO:Initializing create_model()
2025-05-03 17:56:19,806:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:19,806:INFO:Checking exceptions
2025-05-03 17:56:19,806:INFO:Importing libraries
2025-05-03 17:56:19,806:INFO:Copying training dataset
2025-05-03 17:56:19,822:INFO:Defining folds
2025-05-03 17:56:19,822:INFO:Declaring metric variables
2025-05-03 17:56:19,822:INFO:Importing untrained model
2025-05-03 17:56:19,822:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:56:19,822:INFO:Starting cross validation
2025-05-03 17:56:19,838:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:23,422:INFO:Calculating mean and std
2025-05-03 17:56:23,423:INFO:Creating metrics dataframe
2025-05-03 17:56:23,426:INFO:Uploading results into container
2025-05-03 17:56:23,426:INFO:Uploading model into container now
2025-05-03 17:56:23,427:INFO:_master_model_container: 2
2025-05-03 17:56:23,427:INFO:_display_container: 2
2025-05-03 17:56:23,427:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:56:23,428:INFO:create_model() successfully completed......................................
2025-05-03 17:56:23,535:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:23,535:INFO:Creating metrics dataframe
2025-05-03 17:56:23,535:INFO:Initializing Extreme Gradient Boosting
2025-05-03 17:56:23,535:INFO:Total runtime is 0.12014702955881755 minutes
2025-05-03 17:56:23,550:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:23,550:INFO:Initializing create_model()
2025-05-03 17:56:23,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:23,550:INFO:Checking exceptions
2025-05-03 17:56:23,550:INFO:Importing libraries
2025-05-03 17:56:23,550:INFO:Copying training dataset
2025-05-03 17:56:23,550:INFO:Defining folds
2025-05-03 17:56:23,550:INFO:Declaring metric variables
2025-05-03 17:56:23,566:INFO:Importing untrained model
2025-05-03 17:56:23,566:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:56:23,566:INFO:Starting cross validation
2025-05-03 17:56:23,566:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:26,290:INFO:Calculating mean and std
2025-05-03 17:56:26,290:INFO:Creating metrics dataframe
2025-05-03 17:56:26,290:INFO:Uploading results into container
2025-05-03 17:56:26,290:INFO:Uploading model into container now
2025-05-03 17:56:26,290:INFO:_master_model_container: 3
2025-05-03 17:56:26,295:INFO:_display_container: 2
2025-05-03 17:56:26,295:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:56:26,295:INFO:create_model() successfully completed......................................
2025-05-03 17:56:26,400:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:26,400:INFO:Creating metrics dataframe
2025-05-03 17:56:26,406:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 17:56:26,406:INFO:Total runtime is 0.16800266106923423 minutes
2025-05-03 17:56:26,408:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:26,408:INFO:Initializing create_model()
2025-05-03 17:56:26,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:26,408:INFO:Checking exceptions
2025-05-03 17:56:26,408:INFO:Importing libraries
2025-05-03 17:56:26,408:INFO:Copying training dataset
2025-05-03 17:56:26,420:INFO:Defining folds
2025-05-03 17:56:26,420:INFO:Declaring metric variables
2025-05-03 17:56:26,420:INFO:Importing untrained model
2025-05-03 17:56:26,433:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:56:26,436:INFO:Starting cross validation
2025-05-03 17:56:26,436:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:26,942:INFO:Calculating mean and std
2025-05-03 17:56:26,944:INFO:Creating metrics dataframe
2025-05-03 17:56:26,945:INFO:Uploading results into container
2025-05-03 17:56:26,946:INFO:Uploading model into container now
2025-05-03 17:56:26,946:INFO:_master_model_container: 4
2025-05-03 17:56:26,946:INFO:_display_container: 2
2025-05-03 17:56:26,947:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:56:26,947:INFO:create_model() successfully completed......................................
2025-05-03 17:56:27,069:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:27,069:INFO:Creating metrics dataframe
2025-05-03 17:56:27,069:INFO:Initializing Extra Trees Classifier
2025-05-03 17:56:27,069:INFO:Total runtime is 0.17905582189559938 minutes
2025-05-03 17:56:27,069:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:27,069:INFO:Initializing create_model()
2025-05-03 17:56:27,069:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:27,069:INFO:Checking exceptions
2025-05-03 17:56:27,069:INFO:Importing libraries
2025-05-03 17:56:27,069:INFO:Copying training dataset
2025-05-03 17:56:27,086:INFO:Defining folds
2025-05-03 17:56:27,086:INFO:Declaring metric variables
2025-05-03 17:56:27,086:INFO:Importing untrained model
2025-05-03 17:56:27,102:INFO:Extra Trees Classifier Imported successfully
2025-05-03 17:56:27,107:INFO:Starting cross validation
2025-05-03 17:56:27,108:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:28,211:INFO:Calculating mean and std
2025-05-03 17:56:28,213:INFO:Creating metrics dataframe
2025-05-03 17:56:28,214:INFO:Uploading results into container
2025-05-03 17:56:28,215:INFO:Uploading model into container now
2025-05-03 17:56:28,215:INFO:_master_model_container: 5
2025-05-03 17:56:28,215:INFO:_display_container: 2
2025-05-03 17:56:28,216:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 17:56:28,216:INFO:create_model() successfully completed......................................
2025-05-03 17:56:28,324:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:28,324:INFO:Creating metrics dataframe
2025-05-03 17:56:28,332:INFO:Initializing Ridge Classifier
2025-05-03 17:56:28,332:INFO:Total runtime is 0.20009756088256836 minutes
2025-05-03 17:56:28,335:INFO:SubProcess create_model() called ==================================
2025-05-03 17:56:28,335:INFO:Initializing create_model()
2025-05-03 17:56:28,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000193A3DE9490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:28,335:INFO:Checking exceptions
2025-05-03 17:56:28,335:INFO:Importing libraries
2025-05-03 17:56:28,335:INFO:Copying training dataset
2025-05-03 17:56:28,351:INFO:Defining folds
2025-05-03 17:56:28,351:INFO:Declaring metric variables
2025-05-03 17:56:28,351:INFO:Importing untrained model
2025-05-03 17:56:28,359:INFO:Ridge Classifier Imported successfully
2025-05-03 17:56:28,365:INFO:Starting cross validation
2025-05-03 17:56:28,365:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:28,434:INFO:Calculating mean and std
2025-05-03 17:56:28,434:INFO:Creating metrics dataframe
2025-05-03 17:56:28,435:INFO:Uploading results into container
2025-05-03 17:56:28,435:INFO:Uploading model into container now
2025-05-03 17:56:28,435:INFO:_master_model_container: 6
2025-05-03 17:56:28,435:INFO:_display_container: 2
2025-05-03 17:56:28,435:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 17:56:28,435:INFO:create_model() successfully completed......................................
2025-05-03 17:56:28,535:INFO:SubProcess create_model() end ==================================
2025-05-03 17:56:28,535:INFO:Creating metrics dataframe
2025-05-03 17:56:28,535:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 17:56:28,555:INFO:Initializing create_model()
2025-05-03 17:56:28,555:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:28,555:INFO:Checking exceptions
2025-05-03 17:56:28,555:INFO:Importing libraries
2025-05-03 17:56:28,555:INFO:Copying training dataset
2025-05-03 17:56:28,593:INFO:Defining folds
2025-05-03 17:56:28,593:INFO:Declaring metric variables
2025-05-03 17:56:28,593:INFO:Importing untrained model
2025-05-03 17:56:28,593:INFO:Declaring custom model
2025-05-03 17:56:28,593:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:56:28,593:INFO:Cross validation set to False
2025-05-03 17:56:28,593:INFO:Fitting Model
2025-05-03 17:56:28,658:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:56:28,660:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000912 seconds.
2025-05-03 17:56:28,660:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:56:28,660:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:56:28,660:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:56:28,660:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:56:28,662:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:56:28,662:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:56:28,807:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:56:28,807:INFO:create_model() successfully completed......................................
2025-05-03 17:56:28,955:INFO:_master_model_container: 6
2025-05-03 17:56:28,955:INFO:_display_container: 2
2025-05-03 17:56:28,955:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:56:28,955:INFO:compare_models() successfully completed......................................
2025-05-03 17:56:28,969:INFO:Initializing create_model()
2025-05-03 17:56:28,969:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:28,969:INFO:Checking exceptions
2025-05-03 17:56:29,003:INFO:Importing libraries
2025-05-03 17:56:29,003:INFO:Copying training dataset
2025-05-03 17:56:29,037:INFO:Defining folds
2025-05-03 17:56:29,040:INFO:Declaring metric variables
2025-05-03 17:56:29,046:INFO:Importing untrained model
2025-05-03 17:56:29,051:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:56:29,056:INFO:Starting cross validation
2025-05-03 17:56:29,057:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:29,514:INFO:Calculating mean and std
2025-05-03 17:56:29,514:INFO:Creating metrics dataframe
2025-05-03 17:56:29,519:INFO:Finalizing model
2025-05-03 17:56:29,550:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:56:29,562:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.
2025-05-03 17:56:29,562:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:56:29,562:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:56:29,562:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:56:29,562:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:56:29,569:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:56:29,569:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:56:29,835:INFO:Uploading results into container
2025-05-03 17:56:29,837:INFO:Uploading model into container now
2025-05-03 17:56:29,845:INFO:_master_model_container: 7
2025-05-03 17:56:29,845:INFO:_display_container: 3
2025-05-03 17:56:29,848:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:56:29,848:INFO:create_model() successfully completed......................................
2025-05-03 17:56:29,988:INFO:Initializing create_model()
2025-05-03 17:56:29,990:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:29,990:INFO:Checking exceptions
2025-05-03 17:56:29,994:INFO:Importing libraries
2025-05-03 17:56:29,994:INFO:Copying training dataset
2025-05-03 17:56:30,022:INFO:Defining folds
2025-05-03 17:56:30,022:INFO:Declaring metric variables
2025-05-03 17:56:30,025:INFO:Importing untrained model
2025-05-03 17:56:30,030:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:56:30,035:INFO:Starting cross validation
2025-05-03 17:56:30,036:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:30,367:INFO:Calculating mean and std
2025-05-03 17:56:30,367:INFO:Creating metrics dataframe
2025-05-03 17:56:30,369:INFO:Finalizing model
2025-05-03 17:56:30,488:INFO:Uploading results into container
2025-05-03 17:56:30,490:INFO:Uploading model into container now
2025-05-03 17:56:30,499:INFO:_master_model_container: 8
2025-05-03 17:56:30,499:INFO:_display_container: 4
2025-05-03 17:56:30,501:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:56:30,501:INFO:create_model() successfully completed......................................
2025-05-03 17:56:30,636:INFO:Initializing create_model()
2025-05-03 17:56:30,636:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:56:30,636:INFO:Checking exceptions
2025-05-03 17:56:30,655:INFO:Importing libraries
2025-05-03 17:56:30,655:INFO:Copying training dataset
2025-05-03 17:56:30,677:INFO:Defining folds
2025-05-03 17:56:30,677:INFO:Declaring metric variables
2025-05-03 17:56:30,709:INFO:Importing untrained model
2025-05-03 17:56:30,713:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:56:30,721:INFO:Starting cross validation
2025-05-03 17:56:30,727:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:56:32,228:INFO:Calculating mean and std
2025-05-03 17:56:32,228:INFO:Creating metrics dataframe
2025-05-03 17:56:32,228:INFO:Finalizing model
2025-05-03 17:56:32,884:INFO:Uploading results into container
2025-05-03 17:56:32,887:INFO:Uploading model into container now
2025-05-03 17:56:32,887:INFO:_master_model_container: 9
2025-05-03 17:56:32,887:INFO:_display_container: 5
2025-05-03 17:56:32,887:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:56:32,887:INFO:create_model() successfully completed......................................
2025-05-03 17:56:33,015:INFO:Initializing tune_model()
2025-05-03 17:56:33,015:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000193F6623A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 17:56:33,017:INFO:Checking exceptions
2025-05-03 17:56:33,017:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 17:56:33,043:INFO:Copying training dataset
2025-05-03 17:56:33,057:INFO:Checking base model
2025-05-03 17:56:33,057:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 17:56:33,061:INFO:Declaring metric variables
2025-05-03 17:56:33,065:INFO:Defining Hyperparameters
2025-05-03 17:56:33,247:INFO:Tuning with n_jobs=-1
2025-05-03 17:56:33,260:INFO:Initializing skopt.BayesSearchCV
2025-05-03 17:57:12,068:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:57:12,068:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:57:12,068:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:57:12,068:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 17:57:15,096:INFO:PyCaret ClassificationExperiment
2025-05-03 17:57:15,096:INFO:Logging name: clf-default-name
2025-05-03 17:57:15,096:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 17:57:15,096:INFO:version 3.3.2
2025-05-03 17:57:15,096:INFO:Initializing setup()
2025-05-03 17:57:15,096:INFO:self.USI: ce63
2025-05-03 17:57:15,096:INFO:self._variable_keys: {'gpu_n_jobs_param', 'n_jobs_param', 'pipeline', 'data', 'memory', 'exp_name_log', '_ml_usecase', 'y', 'target_param', 'logging_param', 'fix_imbalance', 'is_multiclass', 'log_plots_param', 'y_test', 'fold_shuffle_param', 'fold_groups_param', '_available_plots', 'X', 'fold_generator', 'exp_id', 'USI', 'idx', 'html_param', 'X_test', 'y_train', 'seed', 'gpu_param', 'X_train'}
2025-05-03 17:57:15,096:INFO:Checking environment
2025-05-03 17:57:15,096:INFO:python_version: 3.11.11
2025-05-03 17:57:15,096:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 17:57:15,096:INFO:machine: AMD64
2025-05-03 17:57:15,096:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 17:57:15,096:INFO:Memory: svmem(total=16965230592, available=4711350272, percent=72.2, used=12253880320, free=4711350272)
2025-05-03 17:57:15,096:INFO:Physical Core: 4
2025-05-03 17:57:15,096:INFO:Logical Core: 8
2025-05-03 17:57:15,096:INFO:Checking libraries
2025-05-03 17:57:15,096:INFO:System:
2025-05-03 17:57:15,096:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 17:57:15,096:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 17:57:15,096:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 17:57:15,096:INFO:PyCaret required dependencies:
2025-05-03 17:57:15,096:INFO:                 pip: 25.0
2025-05-03 17:57:15,096:INFO:          setuptools: 75.8.0
2025-05-03 17:57:15,096:INFO:             pycaret: 3.3.2
2025-05-03 17:57:15,096:INFO:             IPython: 8.32.0
2025-05-03 17:57:15,096:INFO:          ipywidgets: 8.1.6
2025-05-03 17:57:15,096:INFO:                tqdm: 4.67.1
2025-05-03 17:57:15,096:INFO:               numpy: 1.26.4
2025-05-03 17:57:15,096:INFO:              pandas: 2.1.4
2025-05-03 17:57:15,096:INFO:              jinja2: 3.1.6
2025-05-03 17:57:15,096:INFO:               scipy: 1.11.4
2025-05-03 17:57:15,096:INFO:              joblib: 1.3.2
2025-05-03 17:57:15,096:INFO:             sklearn: 1.4.2
2025-05-03 17:57:15,096:INFO:                pyod: 2.0.5
2025-05-03 17:57:15,096:INFO:            imblearn: 0.13.0
2025-05-03 17:57:15,096:INFO:   category_encoders: 2.7.0
2025-05-03 17:57:15,096:INFO:            lightgbm: 4.6.0
2025-05-03 17:57:15,096:INFO:               numba: 0.61.0
2025-05-03 17:57:15,096:INFO:            requests: 2.32.3
2025-05-03 17:57:15,096:INFO:          matplotlib: 3.7.5
2025-05-03 17:57:15,096:INFO:          scikitplot: 0.3.7
2025-05-03 17:57:15,096:INFO:         yellowbrick: 1.5
2025-05-03 17:57:15,096:INFO:              plotly: 5.24.1
2025-05-03 17:57:15,096:INFO:    plotly-resampler: Not installed
2025-05-03 17:57:15,096:INFO:             kaleido: 0.2.1
2025-05-03 17:57:15,096:INFO:           schemdraw: 0.15
2025-05-03 17:57:15,096:INFO:         statsmodels: 0.14.4
2025-05-03 17:57:15,096:INFO:              sktime: 0.26.0
2025-05-03 17:57:15,096:INFO:               tbats: 1.1.3
2025-05-03 17:57:15,096:INFO:            pmdarima: 2.0.4
2025-05-03 17:57:15,096:INFO:              psutil: 6.1.1
2025-05-03 17:57:15,096:INFO:          markupsafe: 3.0.2
2025-05-03 17:57:15,096:INFO:             pickle5: Not installed
2025-05-03 17:57:15,096:INFO:         cloudpickle: 3.1.1
2025-05-03 17:57:15,096:INFO:         deprecation: 2.1.0
2025-05-03 17:57:15,096:INFO:              xxhash: 3.5.0
2025-05-03 17:57:15,096:INFO:           wurlitzer: Not installed
2025-05-03 17:57:15,096:INFO:PyCaret optional dependencies:
2025-05-03 17:57:15,455:INFO:                shap: 0.46.0
2025-05-03 17:57:15,455:INFO:           interpret: 0.6.9
2025-05-03 17:57:15,455:INFO:                umap: Not installed
2025-05-03 17:57:15,455:INFO:     ydata_profiling: Not installed
2025-05-03 17:57:15,455:INFO:  explainerdashboard: Not installed
2025-05-03 17:57:15,455:INFO:             autoviz: Not installed
2025-05-03 17:57:15,455:INFO:           fairlearn: Not installed
2025-05-03 17:57:15,455:INFO:          deepchecks: Not installed
2025-05-03 17:57:15,455:INFO:             xgboost: 3.0.0
2025-05-03 17:57:15,455:INFO:            catboost: Not installed
2025-05-03 17:57:15,455:INFO:              kmodes: Not installed
2025-05-03 17:57:15,455:INFO:             mlxtend: Not installed
2025-05-03 17:57:15,455:INFO:       statsforecast: Not installed
2025-05-03 17:57:15,455:INFO:        tune_sklearn: Not installed
2025-05-03 17:57:15,455:INFO:                 ray: Not installed
2025-05-03 17:57:15,455:INFO:            hyperopt: 0.2.7
2025-05-03 17:57:15,455:INFO:              optuna: Not installed
2025-05-03 17:57:15,455:INFO:               skopt: 0.10.2
2025-05-03 17:57:15,455:INFO:              mlflow: 2.22.0
2025-05-03 17:57:15,455:INFO:              gradio: Not installed
2025-05-03 17:57:15,455:INFO:             fastapi: 0.115.12
2025-05-03 17:57:15,455:INFO:             uvicorn: 0.34.2
2025-05-03 17:57:15,455:INFO:              m2cgen: Not installed
2025-05-03 17:57:15,455:INFO:           evidently: Not installed
2025-05-03 17:57:15,455:INFO:               fugue: Not installed
2025-05-03 17:57:15,455:INFO:           streamlit: Not installed
2025-05-03 17:57:15,455:INFO:             prophet: Not installed
2025-05-03 17:57:15,455:INFO:None
2025-05-03 17:57:15,455:INFO:Set up data.
2025-05-03 17:57:15,463:INFO:Set up folding strategy.
2025-05-03 17:57:15,463:INFO:Set up train/test split.
2025-05-03 17:57:15,479:INFO:Set up index.
2025-05-03 17:57:15,479:INFO:Assigning column types.
2025-05-03 17:57:15,496:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 17:57:15,529:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,535:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,551:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,598:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,598:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,614:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,614:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,614:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 17:57:15,664:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,680:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,695:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,731:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 17:57:15,747:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,747:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,747:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 17:57:15,814:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,814:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,884:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:15,884:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:15,884:INFO:Set up column name cleaning.
2025-05-03 17:57:15,907:INFO:Finished creating preprocessing pipeline.
2025-05-03 17:57:15,909:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 17:57:15,909:INFO:Creating final display dataframe.
2025-05-03 17:57:15,979:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 17:57:16,046:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:16,046:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:16,139:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 17:57:16,143:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 17:57:16,146:INFO:setup() successfully completed in 1.05s...............
2025-05-03 17:57:16,164:INFO:Initializing compare_models()
2025-05-03 17:57:16,164:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 17:57:16,164:INFO:Checking exceptions
2025-05-03 17:57:16,174:INFO:Preparing display monitor
2025-05-03 17:57:16,200:INFO:Initializing Logistic Regression
2025-05-03 17:57:16,200:INFO:Total runtime is 0.0 minutes
2025-05-03 17:57:16,204:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:16,204:INFO:Initializing create_model()
2025-05-03 17:57:16,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:16,204:INFO:Checking exceptions
2025-05-03 17:57:16,204:INFO:Importing libraries
2025-05-03 17:57:16,204:INFO:Copying training dataset
2025-05-03 17:57:16,219:INFO:Defining folds
2025-05-03 17:57:16,219:INFO:Declaring metric variables
2025-05-03 17:57:16,223:INFO:Importing untrained model
2025-05-03 17:57:16,227:INFO:Logistic Regression Imported successfully
2025-05-03 17:57:16,235:INFO:Starting cross validation
2025-05-03 17:57:16,236:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:19,520:INFO:Calculating mean and std
2025-05-03 17:57:19,520:INFO:Creating metrics dataframe
2025-05-03 17:57:19,520:INFO:Uploading results into container
2025-05-03 17:57:19,520:INFO:Uploading model into container now
2025-05-03 17:57:19,520:INFO:_master_model_container: 1
2025-05-03 17:57:19,520:INFO:_display_container: 2
2025-05-03 17:57:19,526:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 17:57:19,526:INFO:create_model() successfully completed......................................
2025-05-03 17:57:19,636:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:19,636:INFO:Creating metrics dataframe
2025-05-03 17:57:19,642:INFO:Initializing Random Forest Classifier
2025-05-03 17:57:19,642:INFO:Total runtime is 0.057369232177734375 minutes
2025-05-03 17:57:19,643:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:19,643:INFO:Initializing create_model()
2025-05-03 17:57:19,643:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:19,643:INFO:Checking exceptions
2025-05-03 17:57:19,643:INFO:Importing libraries
2025-05-03 17:57:19,643:INFO:Copying training dataset
2025-05-03 17:57:19,659:INFO:Defining folds
2025-05-03 17:57:19,659:INFO:Declaring metric variables
2025-05-03 17:57:19,659:INFO:Importing untrained model
2025-05-03 17:57:19,659:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:57:19,659:INFO:Starting cross validation
2025-05-03 17:57:19,659:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:23,403:INFO:Calculating mean and std
2025-05-03 17:57:23,405:INFO:Creating metrics dataframe
2025-05-03 17:57:23,406:INFO:Uploading results into container
2025-05-03 17:57:23,406:INFO:Uploading model into container now
2025-05-03 17:57:23,409:INFO:_master_model_container: 2
2025-05-03 17:57:23,409:INFO:_display_container: 2
2025-05-03 17:57:23,409:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:57:23,409:INFO:create_model() successfully completed......................................
2025-05-03 17:57:23,506:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:23,506:INFO:Creating metrics dataframe
2025-05-03 17:57:23,523:INFO:Initializing Extreme Gradient Boosting
2025-05-03 17:57:23,523:INFO:Total runtime is 0.12205100854237874 minutes
2025-05-03 17:57:23,523:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:23,523:INFO:Initializing create_model()
2025-05-03 17:57:23,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:23,523:INFO:Checking exceptions
2025-05-03 17:57:23,523:INFO:Importing libraries
2025-05-03 17:57:23,523:INFO:Copying training dataset
2025-05-03 17:57:23,539:INFO:Defining folds
2025-05-03 17:57:23,539:INFO:Declaring metric variables
2025-05-03 17:57:23,539:INFO:Importing untrained model
2025-05-03 17:57:23,539:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:57:23,556:INFO:Starting cross validation
2025-05-03 17:57:23,556:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:26,245:INFO:Calculating mean and std
2025-05-03 17:57:26,245:INFO:Creating metrics dataframe
2025-05-03 17:57:26,245:INFO:Uploading results into container
2025-05-03 17:57:26,245:INFO:Uploading model into container now
2025-05-03 17:57:26,245:INFO:_master_model_container: 3
2025-05-03 17:57:26,245:INFO:_display_container: 2
2025-05-03 17:57:26,245:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:57:26,245:INFO:create_model() successfully completed......................................
2025-05-03 17:57:26,353:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:26,354:INFO:Creating metrics dataframe
2025-05-03 17:57:26,354:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 17:57:26,354:INFO:Total runtime is 0.16923972765604656 minutes
2025-05-03 17:57:26,354:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:26,354:INFO:Initializing create_model()
2025-05-03 17:57:26,354:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:26,354:INFO:Checking exceptions
2025-05-03 17:57:26,354:INFO:Importing libraries
2025-05-03 17:57:26,354:INFO:Copying training dataset
2025-05-03 17:57:26,378:INFO:Defining folds
2025-05-03 17:57:26,378:INFO:Declaring metric variables
2025-05-03 17:57:26,378:INFO:Importing untrained model
2025-05-03 17:57:26,386:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:57:26,387:INFO:Starting cross validation
2025-05-03 17:57:26,387:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:26,864:INFO:Calculating mean and std
2025-05-03 17:57:26,864:INFO:Creating metrics dataframe
2025-05-03 17:57:26,868:INFO:Uploading results into container
2025-05-03 17:57:26,868:INFO:Uploading model into container now
2025-05-03 17:57:26,869:INFO:_master_model_container: 4
2025-05-03 17:57:26,869:INFO:_display_container: 2
2025-05-03 17:57:26,870:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:57:26,870:INFO:create_model() successfully completed......................................
2025-05-03 17:57:26,986:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:26,986:INFO:Creating metrics dataframe
2025-05-03 17:57:26,986:INFO:Initializing Extra Trees Classifier
2025-05-03 17:57:26,986:INFO:Total runtime is 0.1797809402147929 minutes
2025-05-03 17:57:26,986:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:26,986:INFO:Initializing create_model()
2025-05-03 17:57:26,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:26,986:INFO:Checking exceptions
2025-05-03 17:57:26,986:INFO:Importing libraries
2025-05-03 17:57:26,986:INFO:Copying training dataset
2025-05-03 17:57:27,014:INFO:Defining folds
2025-05-03 17:57:27,014:INFO:Declaring metric variables
2025-05-03 17:57:27,017:INFO:Importing untrained model
2025-05-03 17:57:27,020:INFO:Extra Trees Classifier Imported successfully
2025-05-03 17:57:27,026:INFO:Starting cross validation
2025-05-03 17:57:27,027:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:28,166:INFO:Calculating mean and std
2025-05-03 17:57:28,167:INFO:Creating metrics dataframe
2025-05-03 17:57:28,171:INFO:Uploading results into container
2025-05-03 17:57:28,171:INFO:Uploading model into container now
2025-05-03 17:57:28,171:INFO:_master_model_container: 5
2025-05-03 17:57:28,171:INFO:_display_container: 2
2025-05-03 17:57:28,171:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 17:57:28,171:INFO:create_model() successfully completed......................................
2025-05-03 17:57:28,269:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:28,269:INFO:Creating metrics dataframe
2025-05-03 17:57:28,284:INFO:Initializing Ridge Classifier
2025-05-03 17:57:28,284:INFO:Total runtime is 0.20141067902247112 minutes
2025-05-03 17:57:28,286:INFO:SubProcess create_model() called ==================================
2025-05-03 17:57:28,286:INFO:Initializing create_model()
2025-05-03 17:57:28,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883257290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:28,286:INFO:Checking exceptions
2025-05-03 17:57:28,286:INFO:Importing libraries
2025-05-03 17:57:28,286:INFO:Copying training dataset
2025-05-03 17:57:28,302:INFO:Defining folds
2025-05-03 17:57:28,302:INFO:Declaring metric variables
2025-05-03 17:57:28,302:INFO:Importing untrained model
2025-05-03 17:57:28,310:INFO:Ridge Classifier Imported successfully
2025-05-03 17:57:28,314:INFO:Starting cross validation
2025-05-03 17:57:28,316:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:28,385:INFO:Calculating mean and std
2025-05-03 17:57:28,385:INFO:Creating metrics dataframe
2025-05-03 17:57:28,385:INFO:Uploading results into container
2025-05-03 17:57:28,385:INFO:Uploading model into container now
2025-05-03 17:57:28,385:INFO:_master_model_container: 6
2025-05-03 17:57:28,385:INFO:_display_container: 2
2025-05-03 17:57:28,385:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 17:57:28,385:INFO:create_model() successfully completed......................................
2025-05-03 17:57:28,486:INFO:SubProcess create_model() end ==================================
2025-05-03 17:57:28,486:INFO:Creating metrics dataframe
2025-05-03 17:57:28,486:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 17:57:28,502:INFO:Initializing create_model()
2025-05-03 17:57:28,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:28,502:INFO:Checking exceptions
2025-05-03 17:57:28,502:INFO:Importing libraries
2025-05-03 17:57:28,502:INFO:Copying training dataset
2025-05-03 17:57:28,502:INFO:Defining folds
2025-05-03 17:57:28,502:INFO:Declaring metric variables
2025-05-03 17:57:28,502:INFO:Importing untrained model
2025-05-03 17:57:28,502:INFO:Declaring custom model
2025-05-03 17:57:28,518:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:57:28,518:INFO:Cross validation set to False
2025-05-03 17:57:28,518:INFO:Fitting Model
2025-05-03 17:57:28,546:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:57:28,548:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000655 seconds.
2025-05-03 17:57:28,548:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:57:28,548:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:57:28,550:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:57:28,550:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:57:28,550:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:57:28,550:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:57:28,656:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:57:28,658:INFO:create_model() successfully completed......................................
2025-05-03 17:57:28,802:INFO:_master_model_container: 6
2025-05-03 17:57:28,802:INFO:_display_container: 2
2025-05-03 17:57:28,818:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:57:28,818:INFO:compare_models() successfully completed......................................
2025-05-03 17:57:28,820:INFO:Initializing create_model()
2025-05-03 17:57:28,820:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:28,820:INFO:Checking exceptions
2025-05-03 17:57:28,840:INFO:Importing libraries
2025-05-03 17:57:28,840:INFO:Copying training dataset
2025-05-03 17:57:28,856:INFO:Defining folds
2025-05-03 17:57:28,857:INFO:Declaring metric variables
2025-05-03 17:57:28,860:INFO:Importing untrained model
2025-05-03 17:57:28,862:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:57:28,867:INFO:Starting cross validation
2025-05-03 17:57:28,868:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:29,338:INFO:Calculating mean and std
2025-05-03 17:57:29,338:INFO:Creating metrics dataframe
2025-05-03 17:57:29,344:INFO:Finalizing model
2025-05-03 17:57:29,370:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:57:29,374:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.
2025-05-03 17:57:29,374:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:57:29,374:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:57:29,374:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:57:29,374:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:57:29,374:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:57:29,374:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:57:29,500:INFO:Uploading results into container
2025-05-03 17:57:29,501:INFO:Uploading model into container now
2025-05-03 17:57:29,511:INFO:_master_model_container: 7
2025-05-03 17:57:29,511:INFO:_display_container: 3
2025-05-03 17:57:29,513:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:57:29,513:INFO:create_model() successfully completed......................................
2025-05-03 17:57:29,655:INFO:Initializing create_model()
2025-05-03 17:57:29,655:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:29,655:INFO:Checking exceptions
2025-05-03 17:57:29,668:INFO:Importing libraries
2025-05-03 17:57:29,668:INFO:Copying training dataset
2025-05-03 17:57:29,689:INFO:Defining folds
2025-05-03 17:57:29,689:INFO:Declaring metric variables
2025-05-03 17:57:29,691:INFO:Importing untrained model
2025-05-03 17:57:29,695:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 17:57:29,700:INFO:Starting cross validation
2025-05-03 17:57:29,701:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:30,057:INFO:Calculating mean and std
2025-05-03 17:57:30,057:INFO:Creating metrics dataframe
2025-05-03 17:57:30,057:INFO:Finalizing model
2025-05-03 17:57:30,188:INFO:Uploading results into container
2025-05-03 17:57:30,188:INFO:Uploading model into container now
2025-05-03 17:57:30,197:INFO:_master_model_container: 8
2025-05-03 17:57:30,197:INFO:_display_container: 4
2025-05-03 17:57:30,199:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 17:57:30,199:INFO:create_model() successfully completed......................................
2025-05-03 17:57:30,334:INFO:Initializing create_model()
2025-05-03 17:57:30,334:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:57:30,334:INFO:Checking exceptions
2025-05-03 17:57:30,355:INFO:Importing libraries
2025-05-03 17:57:30,355:INFO:Copying training dataset
2025-05-03 17:57:30,374:INFO:Defining folds
2025-05-03 17:57:30,374:INFO:Declaring metric variables
2025-05-03 17:57:30,376:INFO:Importing untrained model
2025-05-03 17:57:30,381:INFO:Random Forest Classifier Imported successfully
2025-05-03 17:57:30,388:INFO:Starting cross validation
2025-05-03 17:57:30,390:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:57:31,723:INFO:Calculating mean and std
2025-05-03 17:57:31,723:INFO:Creating metrics dataframe
2025-05-03 17:57:31,723:INFO:Finalizing model
2025-05-03 17:57:32,336:INFO:Uploading results into container
2025-05-03 17:57:32,336:INFO:Uploading model into container now
2025-05-03 17:57:32,344:INFO:_master_model_container: 9
2025-05-03 17:57:32,344:INFO:_display_container: 5
2025-05-03 17:57:32,344:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 17:57:32,344:INFO:create_model() successfully completed......................................
2025-05-03 17:57:32,467:INFO:Initializing tune_model()
2025-05-03 17:57:32,467:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 17:57:32,467:INFO:Checking exceptions
2025-05-03 17:57:32,467:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 17:57:32,491:INFO:Copying training dataset
2025-05-03 17:57:32,506:INFO:Checking base model
2025-05-03 17:57:32,507:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 17:57:32,511:INFO:Declaring metric variables
2025-05-03 17:57:32,513:INFO:Defining Hyperparameters
2025-05-03 17:57:32,620:INFO:Tuning with n_jobs=-1
2025-05-03 17:57:32,624:INFO:Initializing skopt.BayesSearchCV
2025-05-03 17:59:17,446:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 17:59:17,446:INFO:Hyperparameter search completed
2025-05-03 17:59:17,446:INFO:SubProcess create_model() called ==================================
2025-05-03 17:59:17,446:INFO:Initializing create_model()
2025-05-03 17:59:17,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F882CADD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 17:59:17,446:INFO:Checking exceptions
2025-05-03 17:59:17,446:INFO:Importing libraries
2025-05-03 17:59:17,446:INFO:Copying training dataset
2025-05-03 17:59:17,462:INFO:Defining folds
2025-05-03 17:59:17,462:INFO:Declaring metric variables
2025-05-03 17:59:17,462:INFO:Importing untrained model
2025-05-03 17:59:17,462:INFO:Declaring custom model
2025-05-03 17:59:17,462:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:59:17,479:INFO:Starting cross validation
2025-05-03 17:59:17,479:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:59:17,873:INFO:Calculating mean and std
2025-05-03 17:59:17,873:INFO:Creating metrics dataframe
2025-05-03 17:59:17,881:INFO:Finalizing model
2025-05-03 17:59:17,893:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 17:59:17,893:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 17:59:17,893:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 17:59:17,907:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 17:59:17,907:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 17:59:17,907:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 17:59:17,907:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:59:17,912:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002611 seconds.
2025-05-03 17:59:17,912:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 17:59:17,912:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:59:17,912:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:59:17,912:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:59:17,912:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:59:17,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:17,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:17,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:17,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,016:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,027:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,041:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,049:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,059:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,059:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,060:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,060:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 17:59:18,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 17:59:18,071:INFO:Uploading results into container
2025-05-03 17:59:18,073:INFO:Uploading model into container now
2025-05-03 17:59:18,073:INFO:_master_model_container: 10
2025-05-03 17:59:18,073:INFO:_display_container: 6
2025-05-03 17:59:18,075:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:59:18,075:INFO:create_model() successfully completed......................................
2025-05-03 17:59:18,195:INFO:SubProcess create_model() end ==================================
2025-05-03 17:59:18,195:INFO:choose_better activated
2025-05-03 17:59:18,195:INFO:SubProcess create_model() called ==================================
2025-05-03 17:59:18,211:INFO:Initializing create_model()
2025-05-03 17:59:18,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 17:59:18,211:INFO:Checking exceptions
2025-05-03 17:59:18,212:INFO:Importing libraries
2025-05-03 17:59:18,212:INFO:Copying training dataset
2025-05-03 17:59:18,212:INFO:Defining folds
2025-05-03 17:59:18,212:INFO:Declaring metric variables
2025-05-03 17:59:18,212:INFO:Importing untrained model
2025-05-03 17:59:18,227:INFO:Declaring custom model
2025-05-03 17:59:18,228:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 17:59:18,228:INFO:Starting cross validation
2025-05-03 17:59:18,228:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 17:59:18,701:INFO:Calculating mean and std
2025-05-03 17:59:18,701:INFO:Creating metrics dataframe
2025-05-03 17:59:18,703:INFO:Finalizing model
2025-05-03 17:59:18,728:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 17:59:18,732:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.
2025-05-03 17:59:18,732:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 17:59:18,732:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 17:59:18,732:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 17:59:18,732:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 17:59:18,732:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 17:59:18,732:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 17:59:18,877:INFO:Uploading results into container
2025-05-03 17:59:18,878:INFO:Uploading model into container now
2025-05-03 17:59:18,879:INFO:_master_model_container: 11
2025-05-03 17:59:18,879:INFO:_display_container: 7
2025-05-03 17:59:18,880:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:59:18,880:INFO:create_model() successfully completed......................................
2025-05-03 17:59:19,011:INFO:SubProcess create_model() end ==================================
2025-05-03 17:59:19,011:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 17:59:19,011:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 17:59:19,011:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 17:59:19,011:INFO:choose_better completed
2025-05-03 17:59:19,011:INFO:_master_model_container: 11
2025-05-03 17:59:19,011:INFO:_display_container: 6
2025-05-03 17:59:19,011:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 17:59:19,011:INFO:tune_model() successfully completed......................................
2025-05-03 17:59:19,127:INFO:Initializing tune_model()
2025-05-03 17:59:19,127:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 17:59:19,127:INFO:Checking exceptions
2025-05-03 17:59:19,127:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 17:59:19,154:INFO:Copying training dataset
2025-05-03 17:59:19,166:INFO:Checking base model
2025-05-03 17:59:19,166:INFO:Base model : Extreme Gradient Boosting
2025-05-03 17:59:19,171:INFO:Declaring metric variables
2025-05-03 17:59:19,174:INFO:Defining Hyperparameters
2025-05-03 17:59:19,275:INFO:Tuning with n_jobs=-1
2025-05-03 17:59:19,278:INFO:Initializing skopt.BayesSearchCV
2025-05-03 18:00:44,906:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 18:00:44,907:INFO:Hyperparameter search completed
2025-05-03 18:00:44,907:INFO:SubProcess create_model() called ==================================
2025-05-03 18:00:44,907:INFO:Initializing create_model()
2025-05-03 18:00:44,907:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F882CB9590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 18:00:44,907:INFO:Checking exceptions
2025-05-03 18:00:44,907:INFO:Importing libraries
2025-05-03 18:00:44,907:INFO:Copying training dataset
2025-05-03 18:00:44,923:INFO:Defining folds
2025-05-03 18:00:44,923:INFO:Declaring metric variables
2025-05-03 18:00:44,924:INFO:Importing untrained model
2025-05-03 18:00:44,924:INFO:Declaring custom model
2025-05-03 18:00:44,924:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:00:44,924:INFO:Starting cross validation
2025-05-03 18:00:44,924:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:00:45,365:INFO:Calculating mean and std
2025-05-03 18:00:45,365:INFO:Creating metrics dataframe
2025-05-03 18:00:45,365:INFO:Finalizing model
2025-05-03 18:00:45,516:INFO:Uploading results into container
2025-05-03 18:00:45,518:INFO:Uploading model into container now
2025-05-03 18:00:45,518:INFO:_master_model_container: 12
2025-05-03 18:00:45,518:INFO:_display_container: 7
2025-05-03 18:00:45,518:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 18:00:45,518:INFO:create_model() successfully completed......................................
2025-05-03 18:00:45,642:INFO:SubProcess create_model() end ==================================
2025-05-03 18:00:45,642:INFO:choose_better activated
2025-05-03 18:00:45,642:INFO:SubProcess create_model() called ==================================
2025-05-03 18:00:45,642:INFO:Initializing create_model()
2025-05-03 18:00:45,642:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:00:45,642:INFO:Checking exceptions
2025-05-03 18:00:45,642:INFO:Importing libraries
2025-05-03 18:00:45,642:INFO:Copying training dataset
2025-05-03 18:00:45,656:INFO:Defining folds
2025-05-03 18:00:45,656:INFO:Declaring metric variables
2025-05-03 18:00:45,656:INFO:Importing untrained model
2025-05-03 18:00:45,656:INFO:Declaring custom model
2025-05-03 18:00:45,656:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:00:45,656:INFO:Starting cross validation
2025-05-03 18:00:45,656:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:00:45,957:INFO:Calculating mean and std
2025-05-03 18:00:45,957:INFO:Creating metrics dataframe
2025-05-03 18:00:45,957:INFO:Finalizing model
2025-05-03 18:00:46,070:INFO:Uploading results into container
2025-05-03 18:00:46,070:INFO:Uploading model into container now
2025-05-03 18:00:46,070:INFO:_master_model_container: 13
2025-05-03 18:00:46,070:INFO:_display_container: 8
2025-05-03 18:00:46,072:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 18:00:46,072:INFO:create_model() successfully completed......................................
2025-05-03 18:00:46,194:INFO:SubProcess create_model() end ==================================
2025-05-03 18:00:46,194:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 18:00:46,194:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 18:00:46,194:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 18:00:46,194:INFO:choose_better completed
2025-05-03 18:00:46,211:INFO:_master_model_container: 13
2025-05-03 18:00:46,211:INFO:_display_container: 7
2025-05-03 18:00:46,211:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 18:00:46,212:INFO:tune_model() successfully completed......................................
2025-05-03 18:00:46,329:INFO:Initializing tune_model()
2025-05-03 18:00:46,329:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 18:00:46,329:INFO:Checking exceptions
2025-05-03 18:00:46,329:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 18:00:46,347:INFO:Copying training dataset
2025-05-03 18:00:46,356:INFO:Checking base model
2025-05-03 18:00:46,356:INFO:Base model : Random Forest Classifier
2025-05-03 18:00:46,360:INFO:Declaring metric variables
2025-05-03 18:00:46,364:INFO:Defining Hyperparameters
2025-05-03 18:00:46,454:INFO:Tuning with n_jobs=-1
2025-05-03 18:00:46,472:INFO:Initializing skopt.BayesSearchCV
2025-05-03 18:04:20,862:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 18:04:20,862:INFO:Hyperparameter search completed
2025-05-03 18:04:20,862:INFO:SubProcess create_model() called ==================================
2025-05-03 18:04:20,862:INFO:Initializing create_model()
2025-05-03 18:04:20,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F883027990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 18:04:20,862:INFO:Checking exceptions
2025-05-03 18:04:20,862:INFO:Importing libraries
2025-05-03 18:04:20,862:INFO:Copying training dataset
2025-05-03 18:04:20,879:INFO:Defining folds
2025-05-03 18:04:20,879:INFO:Declaring metric variables
2025-05-03 18:04:20,879:INFO:Importing untrained model
2025-05-03 18:04:20,879:INFO:Declaring custom model
2025-05-03 18:04:20,879:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:04:20,896:INFO:Starting cross validation
2025-05-03 18:04:20,896:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:04:25,181:INFO:Calculating mean and std
2025-05-03 18:04:25,183:INFO:Creating metrics dataframe
2025-05-03 18:04:25,187:INFO:Finalizing model
2025-05-03 18:04:27,712:INFO:Uploading results into container
2025-05-03 18:04:27,712:INFO:Uploading model into container now
2025-05-03 18:04:27,712:INFO:_master_model_container: 14
2025-05-03 18:04:27,712:INFO:_display_container: 8
2025-05-03 18:04:27,712:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 18:04:27,712:INFO:create_model() successfully completed......................................
2025-05-03 18:04:27,828:INFO:SubProcess create_model() end ==================================
2025-05-03 18:04:27,828:INFO:choose_better activated
2025-05-03 18:04:27,831:INFO:SubProcess create_model() called ==================================
2025-05-03 18:04:27,831:INFO:Initializing create_model()
2025-05-03 18:04:27,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:04:27,831:INFO:Checking exceptions
2025-05-03 18:04:27,832:INFO:Importing libraries
2025-05-03 18:04:27,833:INFO:Copying training dataset
2025-05-03 18:04:27,840:INFO:Defining folds
2025-05-03 18:04:27,840:INFO:Declaring metric variables
2025-05-03 18:04:27,840:INFO:Importing untrained model
2025-05-03 18:04:27,840:INFO:Declaring custom model
2025-05-03 18:04:27,840:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:04:27,840:INFO:Starting cross validation
2025-05-03 18:04:27,840:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:04:29,250:INFO:Calculating mean and std
2025-05-03 18:04:29,250:INFO:Creating metrics dataframe
2025-05-03 18:04:29,253:INFO:Finalizing model
2025-05-03 18:04:29,942:INFO:Uploading results into container
2025-05-03 18:04:29,943:INFO:Uploading model into container now
2025-05-03 18:04:29,943:INFO:_master_model_container: 15
2025-05-03 18:04:29,943:INFO:_display_container: 9
2025-05-03 18:04:29,944:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 18:04:29,944:INFO:create_model() successfully completed......................................
2025-05-03 18:04:30,055:INFO:SubProcess create_model() end ==================================
2025-05-03 18:04:30,055:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 18:04:30,055:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 18:04:30,055:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 18:04:30,055:INFO:choose_better completed
2025-05-03 18:04:30,070:INFO:_master_model_container: 15
2025-05-03 18:04:30,070:INFO:_display_container: 8
2025-05-03 18:04:30,071:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 18:04:30,072:INFO:tune_model() successfully completed......................................
2025-05-03 18:04:30,209:INFO:Initializing create_model()
2025-05-03 18:04:30,209:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 18:04:30,209:INFO:Checking exceptions
2025-05-03 18:04:30,227:INFO:Importing libraries
2025-05-03 18:04:30,227:INFO:Copying training dataset
2025-05-03 18:04:30,250:INFO:Defining folds
2025-05-03 18:04:30,250:INFO:Declaring metric variables
2025-05-03 18:04:30,253:INFO:Importing untrained model
2025-05-03 18:04:30,258:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 18:04:30,266:INFO:Starting cross validation
2025-05-03 18:04:30,267:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:04:30,767:INFO:Calculating mean and std
2025-05-03 18:04:30,767:INFO:Creating metrics dataframe
2025-05-03 18:04:30,775:INFO:Finalizing model
2025-05-03 18:04:30,806:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:04:30,806:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:04:30,806:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:04:30,842:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:04:30,844:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:04:30,844:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:04:30,844:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 18:04:30,848:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001112 seconds.
2025-05-03 18:04:30,848:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 18:04:30,848:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 18:04:30,848:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 18:04:30,848:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 18:04:30,848:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 18:04:30,850:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 18:04:30,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,979:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,991:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,991:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:30,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,003:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,008:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,011:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,023:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,023:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,025:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,025:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,027:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,031:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,031:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:04:31,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:04:31,046:INFO:Uploading results into container
2025-05-03 18:04:31,047:INFO:Uploading model into container now
2025-05-03 18:04:31,058:INFO:_master_model_container: 16
2025-05-03 18:04:31,058:INFO:_display_container: 9
2025-05-03 18:04:31,060:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:04:31,060:INFO:create_model() successfully completed......................................
2025-05-03 18:04:31,187:INFO:Initializing create_model()
2025-05-03 18:04:31,187:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 18:04:31,187:INFO:Checking exceptions
2025-05-03 18:04:31,210:INFO:Importing libraries
2025-05-03 18:04:31,210:INFO:Copying training dataset
2025-05-03 18:04:31,232:INFO:Defining folds
2025-05-03 18:04:31,232:INFO:Declaring metric variables
2025-05-03 18:04:31,235:INFO:Importing untrained model
2025-05-03 18:04:31,240:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:04:31,246:INFO:Starting cross validation
2025-05-03 18:04:31,247:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:04:31,681:INFO:Calculating mean and std
2025-05-03 18:04:31,681:INFO:Creating metrics dataframe
2025-05-03 18:04:31,681:INFO:Finalizing model
2025-05-03 18:04:31,845:INFO:Uploading results into container
2025-05-03 18:04:31,845:INFO:Uploading model into container now
2025-05-03 18:04:31,855:INFO:_master_model_container: 17
2025-05-03 18:04:31,855:INFO:_display_container: 10
2025-05-03 18:04:31,857:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 18:04:31,857:INFO:create_model() successfully completed......................................
2025-05-03 18:04:31,984:INFO:Initializing create_model()
2025-05-03 18:04:31,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 18:04:31,984:INFO:Checking exceptions
2025-05-03 18:04:32,000:INFO:Importing libraries
2025-05-03 18:04:32,000:INFO:Copying training dataset
2025-05-03 18:04:32,022:INFO:Defining folds
2025-05-03 18:04:32,022:INFO:Declaring metric variables
2025-05-03 18:04:32,025:INFO:Importing untrained model
2025-05-03 18:04:32,029:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:04:32,032:INFO:Starting cross validation
2025-05-03 18:04:32,034:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:04:36,337:INFO:Calculating mean and std
2025-05-03 18:04:36,338:INFO:Creating metrics dataframe
2025-05-03 18:04:36,344:INFO:Finalizing model
2025-05-03 18:04:38,682:INFO:Uploading results into container
2025-05-03 18:04:38,686:INFO:Uploading model into container now
2025-05-03 18:04:38,686:INFO:_master_model_container: 18
2025-05-03 18:04:38,686:INFO:_display_container: 11
2025-05-03 18:04:38,686:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 18:04:38,686:INFO:create_model() successfully completed......................................
2025-05-03 18:04:38,814:INFO:Initializing interpret_model()
2025-05-03 18:04:38,818:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:04:38,818:INFO:Checking exceptions
2025-05-03 18:04:38,818:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:04:38,825:INFO:plot type: msa
2025-05-03 18:04:39,213:INFO:Visual Rendered Successfully
2025-05-03 18:04:39,213:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:39,472:INFO:Initializing interpret_model()
2025-05-03 18:04:39,472:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:04:39,472:INFO:Checking exceptions
2025-05-03 18:04:39,472:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:04:39,482:INFO:plot type: msa
2025-05-03 18:04:39,730:INFO:Visual Rendered Successfully
2025-05-03 18:04:39,731:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:39,868:INFO:Initializing interpret_model()
2025-05-03 18:04:39,868:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:04:39,868:INFO:Checking exceptions
2025-05-03 18:04:39,868:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:04:39,884:INFO:plot type: msa
2025-05-03 18:04:39,884:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:04:40,115:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:04:40,217:INFO:Visual Rendered Successfully
2025-05-03 18:04:40,217:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:40,342:INFO:Initializing save_model()
2025-05-03 18:04:40,342:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:04:40,342:INFO:Adding model into prep_pipe
2025-05-03 18:04:40,349:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 18:04:40,353:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 18:04:40,353:INFO:save_model() successfully completed......................................
2025-05-03 18:04:40,478:INFO:Initializing save_model()
2025-05-03 18:04:40,478:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:04:40,478:INFO:Adding model into prep_pipe
2025-05-03 18:04:40,495:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 18:04:40,499:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 18:04:40,499:INFO:save_model() successfully completed......................................
2025-05-03 18:04:40,627:INFO:Initializing save_model()
2025-05-03 18:04:40,627:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:04:40,627:INFO:Adding model into prep_pipe
2025-05-03 18:04:40,716:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 18:04:40,716:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 18:04:40,716:INFO:save_model() successfully completed......................................
2025-05-03 18:04:40,897:INFO:Initializing interpret_model()
2025-05-03 18:04:40,897:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:04:40,897:INFO:Checking exceptions
2025-05-03 18:04:40,897:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:04:40,917:INFO:plot type: summary
2025-05-03 18:04:40,917:INFO:Creating TreeExplainer
2025-05-03 18:04:40,949:INFO:Compiling shap values
2025-05-03 18:04:41,707:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 18:04:41,895:INFO:Visual Rendered Successfully
2025-05-03 18:04:41,895:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:41,991:INFO:Initializing interpret_model()
2025-05-03 18:04:41,991:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:04:41,991:INFO:Checking exceptions
2025-05-03 18:04:41,991:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:04:42,031:INFO:plot type: summary
2025-05-03 18:04:42,031:INFO:Creating TreeExplainer
2025-05-03 18:04:42,047:INFO:Compiling shap values
2025-05-03 18:04:42,798:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 18:04:43,890:INFO:Visual Rendered Successfully
2025-05-03 18:04:43,890:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:44,010:INFO:Initializing interpret_model()
2025-05-03 18:04:44,010:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:04:44,010:INFO:Checking exceptions
2025-05-03 18:04:44,010:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:04:44,035:INFO:plot type: summary
2025-05-03 18:04:44,035:INFO:Creating TreeExplainer
2025-05-03 18:04:44,064:INFO:Compiling shap values
2025-05-03 18:04:45,945:INFO:Visual Rendered Successfully
2025-05-03 18:04:45,945:INFO:interpret_model() successfully completed......................................
2025-05-03 18:04:46,062:INFO:Initializing interpret_model()
2025-05-03 18:04:46,062:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:04:46,062:INFO:Checking exceptions
2025-05-03 18:04:46,062:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:04:46,078:INFO:plot type: summary
2025-05-03 18:04:46,080:INFO:Creating TreeExplainer
2025-05-03 18:04:46,102:INFO:Compiling shap values
2025-05-03 18:04:48,958:INFO:Visual Rendered Successfully
2025-05-03 18:04:48,958:INFO:interpret_model() successfully completed......................................
2025-05-03 18:08:55,577:INFO:Initializing interpret_model()
2025-05-03 18:08:55,577:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=pfi, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:08:55,577:INFO:Checking exceptions
2025-05-03 18:08:55,577:INFO:Soft dependency imported: interpret_community: 0.32.0
2025-05-03 18:08:55,620:INFO:plot type: pfi
2025-05-03 18:08:55,742:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\mlflow\pyfunc\utils\data_validation.py:186: UserWarning:

[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.[0m


2025-05-03 18:08:57,118:INFO:Visual Rendered Successfully
2025-05-03 18:08:57,118:INFO:interpret_model() successfully completed......................................
2025-05-03 18:08:57,235:INFO:Initializing interpret_model()
2025-05-03 18:08:57,235:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F8825FB950>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:08:57,235:INFO:Checking exceptions
2025-05-03 18:08:57,235:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:08:57,263:INFO:plot type: summary
2025-05-03 18:08:57,267:INFO:Creating TreeExplainer
2025-05-03 18:08:57,294:INFO:Compiling shap values
2025-05-03 18:14:28,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 18:14:28,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 18:14:28,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 18:14:28,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 18:14:32,238:INFO:PyCaret ClassificationExperiment
2025-05-03 18:14:32,238:INFO:Logging name: clf-default-name
2025-05-03 18:14:32,238:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 18:14:32,238:INFO:version 3.3.2
2025-05-03 18:14:32,238:INFO:Initializing setup()
2025-05-03 18:14:32,238:INFO:self.USI: 1337
2025-05-03 18:14:32,238:INFO:self._variable_keys: {'y', 'y_test', 'logging_param', 'y_train', 'fold_groups_param', 'exp_id', 'html_param', 'target_param', 'X_test', 'is_multiclass', 'log_plots_param', 'n_jobs_param', 'seed', '_ml_usecase', 'gpu_n_jobs_param', 'gpu_param', 'fold_shuffle_param', 'fix_imbalance', 'fold_generator', '_available_plots', 'idx', 'X', 'pipeline', 'memory', 'X_train', 'USI', 'data', 'exp_name_log'}
2025-05-03 18:14:32,238:INFO:Checking environment
2025-05-03 18:14:32,238:INFO:python_version: 3.11.11
2025-05-03 18:14:32,238:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 18:14:32,238:INFO:machine: AMD64
2025-05-03 18:14:32,238:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 18:14:32,244:INFO:Memory: svmem(total=16965230592, available=4300349440, percent=74.7, used=12664881152, free=4300349440)
2025-05-03 18:14:32,245:INFO:Physical Core: 4
2025-05-03 18:14:32,245:INFO:Logical Core: 8
2025-05-03 18:14:32,245:INFO:Checking libraries
2025-05-03 18:14:32,245:INFO:System:
2025-05-03 18:14:32,245:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 18:14:32,245:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 18:14:32,245:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 18:14:32,245:INFO:PyCaret required dependencies:
2025-05-03 18:14:32,249:INFO:                 pip: 25.0
2025-05-03 18:14:32,249:INFO:          setuptools: 75.8.0
2025-05-03 18:14:32,249:INFO:             pycaret: 3.3.2
2025-05-03 18:14:32,249:INFO:             IPython: 8.32.0
2025-05-03 18:14:32,249:INFO:          ipywidgets: 8.1.6
2025-05-03 18:14:32,249:INFO:                tqdm: 4.67.1
2025-05-03 18:14:32,249:INFO:               numpy: 1.26.4
2025-05-03 18:14:32,249:INFO:              pandas: 2.1.4
2025-05-03 18:14:32,249:INFO:              jinja2: 3.1.6
2025-05-03 18:14:32,249:INFO:               scipy: 1.11.4
2025-05-03 18:14:32,249:INFO:              joblib: 1.3.2
2025-05-03 18:14:32,249:INFO:             sklearn: 1.4.2
2025-05-03 18:14:32,249:INFO:                pyod: 2.0.5
2025-05-03 18:14:32,249:INFO:            imblearn: 0.13.0
2025-05-03 18:14:32,249:INFO:   category_encoders: 2.7.0
2025-05-03 18:14:32,249:INFO:            lightgbm: 4.6.0
2025-05-03 18:14:32,249:INFO:               numba: 0.61.0
2025-05-03 18:14:32,249:INFO:            requests: 2.32.3
2025-05-03 18:14:32,249:INFO:          matplotlib: 3.7.5
2025-05-03 18:14:32,249:INFO:          scikitplot: 0.3.7
2025-05-03 18:14:32,249:INFO:         yellowbrick: 1.5
2025-05-03 18:14:32,249:INFO:              plotly: 5.24.1
2025-05-03 18:14:32,249:INFO:    plotly-resampler: Not installed
2025-05-03 18:14:32,249:INFO:             kaleido: 0.2.1
2025-05-03 18:14:32,249:INFO:           schemdraw: 0.15
2025-05-03 18:14:32,249:INFO:         statsmodels: 0.14.4
2025-05-03 18:14:32,249:INFO:              sktime: 0.26.0
2025-05-03 18:14:32,249:INFO:               tbats: 1.1.3
2025-05-03 18:14:32,249:INFO:            pmdarima: 2.0.4
2025-05-03 18:14:32,249:INFO:              psutil: 6.1.1
2025-05-03 18:14:32,249:INFO:          markupsafe: 3.0.2
2025-05-03 18:14:32,249:INFO:             pickle5: Not installed
2025-05-03 18:14:32,249:INFO:         cloudpickle: 3.1.1
2025-05-03 18:14:32,249:INFO:         deprecation: 2.1.0
2025-05-03 18:14:32,249:INFO:              xxhash: 3.5.0
2025-05-03 18:14:32,249:INFO:           wurlitzer: Not installed
2025-05-03 18:14:32,249:INFO:PyCaret optional dependencies:
2025-05-03 18:14:32,614:INFO:                shap: 0.46.0
2025-05-03 18:14:32,614:INFO:           interpret: 0.6.9
2025-05-03 18:14:32,615:INFO:                umap: Not installed
2025-05-03 18:14:32,615:INFO:     ydata_profiling: Not installed
2025-05-03 18:14:32,615:INFO:  explainerdashboard: Not installed
2025-05-03 18:14:32,615:INFO:             autoviz: Not installed
2025-05-03 18:14:32,615:INFO:           fairlearn: Not installed
2025-05-03 18:14:32,615:INFO:          deepchecks: Not installed
2025-05-03 18:14:32,615:INFO:             xgboost: 3.0.0
2025-05-03 18:14:32,615:INFO:            catboost: Not installed
2025-05-03 18:14:32,615:INFO:              kmodes: Not installed
2025-05-03 18:14:32,615:INFO:             mlxtend: Not installed
2025-05-03 18:14:32,615:INFO:       statsforecast: Not installed
2025-05-03 18:14:32,615:INFO:        tune_sklearn: Not installed
2025-05-03 18:14:32,615:INFO:                 ray: Not installed
2025-05-03 18:14:32,615:INFO:            hyperopt: 0.2.7
2025-05-03 18:14:32,615:INFO:              optuna: Not installed
2025-05-03 18:14:32,615:INFO:               skopt: 0.10.2
2025-05-03 18:14:32,615:INFO:              mlflow: 2.22.0
2025-05-03 18:14:32,615:INFO:              gradio: Not installed
2025-05-03 18:14:32,615:INFO:             fastapi: 0.115.12
2025-05-03 18:14:32,615:INFO:             uvicorn: 0.34.2
2025-05-03 18:14:32,615:INFO:              m2cgen: Not installed
2025-05-03 18:14:32,615:INFO:           evidently: Not installed
2025-05-03 18:14:32,615:INFO:               fugue: Not installed
2025-05-03 18:14:32,616:INFO:           streamlit: Not installed
2025-05-03 18:14:32,616:INFO:             prophet: Not installed
2025-05-03 18:14:32,616:INFO:None
2025-05-03 18:14:32,616:INFO:Set up data.
2025-05-03 18:14:32,629:INFO:Set up folding strategy.
2025-05-03 18:14:32,629:INFO:Set up train/test split.
2025-05-03 18:14:32,643:INFO:Set up index.
2025-05-03 18:14:32,643:INFO:Assigning column types.
2025-05-03 18:14:32,668:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 18:14:32,696:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 18:14:32,710:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 18:14:32,742:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:32,745:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:32,783:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 18:14:32,783:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 18:14:32,809:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:32,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:32,818:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 18:14:32,911:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 18:14:32,942:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:32,942:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:32,991:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 18:14:33,014:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:33,014:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:33,014:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 18:14:33,093:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:33,096:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:33,161:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:33,161:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:33,161:INFO:Set up column name cleaning.
2025-05-03 18:14:33,176:INFO:Finished creating preprocessing pipeline.
2025-05-03 18:14:33,176:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 18:14:33,176:INFO:Creating final display dataframe.
2025-05-03 18:14:33,281:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 18:14:33,349:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:33,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:33,417:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 18:14:33,419:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 18:14:33,420:INFO:setup() successfully completed in 1.19s...............
2025-05-03 18:14:33,440:INFO:Initializing compare_models()
2025-05-03 18:14:33,440:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 18:14:33,440:INFO:Checking exceptions
2025-05-03 18:14:33,451:INFO:Preparing display monitor
2025-05-03 18:14:33,476:INFO:Initializing Logistic Regression
2025-05-03 18:14:33,476:INFO:Total runtime is 0.0 minutes
2025-05-03 18:14:33,481:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:33,482:INFO:Initializing create_model()
2025-05-03 18:14:33,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:33,482:INFO:Checking exceptions
2025-05-03 18:14:33,482:INFO:Importing libraries
2025-05-03 18:14:33,482:INFO:Copying training dataset
2025-05-03 18:14:33,499:INFO:Defining folds
2025-05-03 18:14:33,499:INFO:Declaring metric variables
2025-05-03 18:14:33,502:INFO:Importing untrained model
2025-05-03 18:14:33,506:INFO:Logistic Regression Imported successfully
2025-05-03 18:14:33,511:INFO:Starting cross validation
2025-05-03 18:14:33,512:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:36,987:INFO:Calculating mean and std
2025-05-03 18:14:36,987:INFO:Creating metrics dataframe
2025-05-03 18:14:36,987:INFO:Uploading results into container
2025-05-03 18:14:36,992:INFO:Uploading model into container now
2025-05-03 18:14:36,992:INFO:_master_model_container: 1
2025-05-03 18:14:36,992:INFO:_display_container: 2
2025-05-03 18:14:36,992:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 18:14:36,992:INFO:create_model() successfully completed......................................
2025-05-03 18:14:37,104:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:37,104:INFO:Creating metrics dataframe
2025-05-03 18:14:37,104:INFO:Initializing Random Forest Classifier
2025-05-03 18:14:37,104:INFO:Total runtime is 0.06045513153076172 minutes
2025-05-03 18:14:37,113:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:37,113:INFO:Initializing create_model()
2025-05-03 18:14:37,113:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:37,113:INFO:Checking exceptions
2025-05-03 18:14:37,113:INFO:Importing libraries
2025-05-03 18:14:37,113:INFO:Copying training dataset
2025-05-03 18:14:37,130:INFO:Defining folds
2025-05-03 18:14:37,130:INFO:Declaring metric variables
2025-05-03 18:14:37,133:INFO:Importing untrained model
2025-05-03 18:14:37,137:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:14:37,141:INFO:Starting cross validation
2025-05-03 18:14:37,146:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:41,353:INFO:Calculating mean and std
2025-05-03 18:14:41,353:INFO:Creating metrics dataframe
2025-05-03 18:14:41,353:INFO:Uploading results into container
2025-05-03 18:14:41,353:INFO:Uploading model into container now
2025-05-03 18:14:41,353:INFO:_master_model_container: 2
2025-05-03 18:14:41,353:INFO:_display_container: 2
2025-05-03 18:14:41,353:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 18:14:41,353:INFO:create_model() successfully completed......................................
2025-05-03 18:14:41,450:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:41,450:INFO:Creating metrics dataframe
2025-05-03 18:14:41,466:INFO:Initializing Extreme Gradient Boosting
2025-05-03 18:14:41,466:INFO:Total runtime is 0.1331579089164734 minutes
2025-05-03 18:14:41,469:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:41,469:INFO:Initializing create_model()
2025-05-03 18:14:41,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:41,469:INFO:Checking exceptions
2025-05-03 18:14:41,469:INFO:Importing libraries
2025-05-03 18:14:41,469:INFO:Copying training dataset
2025-05-03 18:14:41,486:INFO:Defining folds
2025-05-03 18:14:41,486:INFO:Declaring metric variables
2025-05-03 18:14:41,486:INFO:Importing untrained model
2025-05-03 18:14:41,486:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:14:41,500:INFO:Starting cross validation
2025-05-03 18:14:41,502:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:44,262:INFO:Calculating mean and std
2025-05-03 18:14:44,262:INFO:Creating metrics dataframe
2025-05-03 18:14:44,266:INFO:Uploading results into container
2025-05-03 18:14:44,267:INFO:Uploading model into container now
2025-05-03 18:14:44,267:INFO:_master_model_container: 3
2025-05-03 18:14:44,267:INFO:_display_container: 2
2025-05-03 18:14:44,267:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 18:14:44,267:INFO:create_model() successfully completed......................................
2025-05-03 18:14:44,366:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:44,366:INFO:Creating metrics dataframe
2025-05-03 18:14:44,382:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 18:14:44,382:INFO:Total runtime is 0.18175294399261474 minutes
2025-05-03 18:14:44,382:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:44,382:INFO:Initializing create_model()
2025-05-03 18:14:44,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:44,382:INFO:Checking exceptions
2025-05-03 18:14:44,382:INFO:Importing libraries
2025-05-03 18:14:44,382:INFO:Copying training dataset
2025-05-03 18:14:44,399:INFO:Defining folds
2025-05-03 18:14:44,399:INFO:Declaring metric variables
2025-05-03 18:14:44,408:INFO:Importing untrained model
2025-05-03 18:14:44,408:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 18:14:44,417:INFO:Starting cross validation
2025-05-03 18:14:44,417:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:45,024:INFO:Calculating mean and std
2025-05-03 18:14:45,026:INFO:Creating metrics dataframe
2025-05-03 18:14:45,028:INFO:Uploading results into container
2025-05-03 18:14:45,028:INFO:Uploading model into container now
2025-05-03 18:14:45,030:INFO:_master_model_container: 4
2025-05-03 18:14:45,030:INFO:_display_container: 2
2025-05-03 18:14:45,031:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:14:45,032:INFO:create_model() successfully completed......................................
2025-05-03 18:14:45,163:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:45,163:INFO:Creating metrics dataframe
2025-05-03 18:14:45,169:INFO:Initializing Extra Trees Classifier
2025-05-03 18:14:45,170:INFO:Total runtime is 0.19489558537801105 minutes
2025-05-03 18:14:45,174:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:45,174:INFO:Initializing create_model()
2025-05-03 18:14:45,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:45,174:INFO:Checking exceptions
2025-05-03 18:14:45,174:INFO:Importing libraries
2025-05-03 18:14:45,174:INFO:Copying training dataset
2025-05-03 18:14:45,188:INFO:Defining folds
2025-05-03 18:14:45,188:INFO:Declaring metric variables
2025-05-03 18:14:45,225:INFO:Importing untrained model
2025-05-03 18:14:45,230:INFO:Extra Trees Classifier Imported successfully
2025-05-03 18:14:45,240:INFO:Starting cross validation
2025-05-03 18:14:45,240:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:46,508:INFO:Calculating mean and std
2025-05-03 18:14:46,508:INFO:Creating metrics dataframe
2025-05-03 18:14:46,508:INFO:Uploading results into container
2025-05-03 18:14:46,508:INFO:Uploading model into container now
2025-05-03 18:14:46,508:INFO:_master_model_container: 5
2025-05-03 18:14:46,508:INFO:_display_container: 2
2025-05-03 18:14:46,508:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 18:14:46,508:INFO:create_model() successfully completed......................................
2025-05-03 18:14:46,610:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:46,610:INFO:Creating metrics dataframe
2025-05-03 18:14:46,627:INFO:Initializing Ridge Classifier
2025-05-03 18:14:46,627:INFO:Total runtime is 0.21917297442754108 minutes
2025-05-03 18:14:46,627:INFO:SubProcess create_model() called ==================================
2025-05-03 18:14:46,627:INFO:Initializing create_model()
2025-05-03 18:14:46,627:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F75506FC50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:46,627:INFO:Checking exceptions
2025-05-03 18:14:46,627:INFO:Importing libraries
2025-05-03 18:14:46,627:INFO:Copying training dataset
2025-05-03 18:14:46,642:INFO:Defining folds
2025-05-03 18:14:46,642:INFO:Declaring metric variables
2025-05-03 18:14:46,650:INFO:Importing untrained model
2025-05-03 18:14:46,653:INFO:Ridge Classifier Imported successfully
2025-05-03 18:14:46,660:INFO:Starting cross validation
2025-05-03 18:14:46,660:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:46,729:INFO:Calculating mean and std
2025-05-03 18:14:46,729:INFO:Creating metrics dataframe
2025-05-03 18:14:46,731:INFO:Uploading results into container
2025-05-03 18:14:46,731:INFO:Uploading model into container now
2025-05-03 18:14:46,731:INFO:_master_model_container: 6
2025-05-03 18:14:46,731:INFO:_display_container: 2
2025-05-03 18:14:46,731:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 18:14:46,731:INFO:create_model() successfully completed......................................
2025-05-03 18:14:46,835:INFO:SubProcess create_model() end ==================================
2025-05-03 18:14:46,835:INFO:Creating metrics dataframe
2025-05-03 18:14:46,841:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 18:14:46,849:INFO:Initializing create_model()
2025-05-03 18:14:46,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:46,849:INFO:Checking exceptions
2025-05-03 18:14:46,854:INFO:Importing libraries
2025-05-03 18:14:46,854:INFO:Copying training dataset
2025-05-03 18:14:46,865:INFO:Defining folds
2025-05-03 18:14:46,865:INFO:Declaring metric variables
2025-05-03 18:14:46,865:INFO:Importing untrained model
2025-05-03 18:14:46,865:INFO:Declaring custom model
2025-05-03 18:14:46,865:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 18:14:46,865:INFO:Cross validation set to False
2025-05-03 18:14:46,865:INFO:Fitting Model
2025-05-03 18:14:46,891:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 18:14:46,894:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2025-05-03 18:14:46,894:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 18:14:46,894:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 18:14:46,894:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 18:14:46,894:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 18:14:46,895:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 18:14:46,895:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 18:14:47,024:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:14:47,024:INFO:create_model() successfully completed......................................
2025-05-03 18:14:47,166:INFO:_master_model_container: 6
2025-05-03 18:14:47,166:INFO:_display_container: 2
2025-05-03 18:14:47,166:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:14:47,166:INFO:compare_models() successfully completed......................................
2025-05-03 18:14:47,195:INFO:Initializing create_model()
2025-05-03 18:14:47,195:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:47,204:INFO:Checking exceptions
2025-05-03 18:14:47,227:INFO:Importing libraries
2025-05-03 18:14:47,227:INFO:Copying training dataset
2025-05-03 18:14:47,256:INFO:Defining folds
2025-05-03 18:14:47,256:INFO:Declaring metric variables
2025-05-03 18:14:47,259:INFO:Importing untrained model
2025-05-03 18:14:47,262:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 18:14:47,269:INFO:Starting cross validation
2025-05-03 18:14:47,269:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:47,943:INFO:Calculating mean and std
2025-05-03 18:14:47,943:INFO:Creating metrics dataframe
2025-05-03 18:14:47,949:INFO:Finalizing model
2025-05-03 18:14:47,980:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 18:14:47,983:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.
2025-05-03 18:14:47,983:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 18:14:47,983:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 18:14:47,983:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 18:14:47,983:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 18:14:47,983:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 18:14:47,983:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 18:14:48,137:INFO:Uploading results into container
2025-05-03 18:14:48,137:INFO:Uploading model into container now
2025-05-03 18:14:48,151:INFO:_master_model_container: 7
2025-05-03 18:14:48,151:INFO:_display_container: 3
2025-05-03 18:14:48,152:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:14:48,152:INFO:create_model() successfully completed......................................
2025-05-03 18:14:48,291:INFO:Initializing create_model()
2025-05-03 18:14:48,291:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:48,291:INFO:Checking exceptions
2025-05-03 18:14:48,306:INFO:Importing libraries
2025-05-03 18:14:48,307:INFO:Copying training dataset
2025-05-03 18:14:48,324:INFO:Defining folds
2025-05-03 18:14:48,324:INFO:Declaring metric variables
2025-05-03 18:14:48,327:INFO:Importing untrained model
2025-05-03 18:14:48,331:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:14:48,338:INFO:Starting cross validation
2025-05-03 18:14:48,339:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:48,729:INFO:Calculating mean and std
2025-05-03 18:14:48,729:INFO:Creating metrics dataframe
2025-05-03 18:14:48,733:INFO:Finalizing model
2025-05-03 18:14:48,876:INFO:Uploading results into container
2025-05-03 18:14:48,878:INFO:Uploading model into container now
2025-05-03 18:14:48,888:INFO:_master_model_container: 8
2025-05-03 18:14:48,888:INFO:_display_container: 4
2025-05-03 18:14:48,890:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 18:14:48,890:INFO:create_model() successfully completed......................................
2025-05-03 18:14:49,029:INFO:Initializing create_model()
2025-05-03 18:14:49,029:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 18:14:49,029:INFO:Checking exceptions
2025-05-03 18:14:49,048:INFO:Importing libraries
2025-05-03 18:14:49,048:INFO:Copying training dataset
2025-05-03 18:14:49,073:INFO:Defining folds
2025-05-03 18:14:49,073:INFO:Declaring metric variables
2025-05-03 18:14:49,076:INFO:Importing untrained model
2025-05-03 18:14:49,080:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:14:49,086:INFO:Starting cross validation
2025-05-03 18:14:49,086:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:50,709:INFO:Calculating mean and std
2025-05-03 18:14:50,709:INFO:Creating metrics dataframe
2025-05-03 18:14:50,712:INFO:Finalizing model
2025-05-03 18:14:51,328:INFO:Uploading results into container
2025-05-03 18:14:51,333:INFO:Uploading model into container now
2025-05-03 18:14:51,337:INFO:_master_model_container: 9
2025-05-03 18:14:51,337:INFO:_display_container: 5
2025-05-03 18:14:51,337:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 18:14:51,337:INFO:create_model() successfully completed......................................
2025-05-03 18:14:51,451:INFO:Initializing create_model()
2025-05-03 18:14:51,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 18:14:51,451:INFO:Checking exceptions
2025-05-03 18:14:51,472:INFO:Importing libraries
2025-05-03 18:14:51,472:INFO:Copying training dataset
2025-05-03 18:14:51,491:INFO:Defining folds
2025-05-03 18:14:51,491:INFO:Declaring metric variables
2025-05-03 18:14:51,494:INFO:Importing untrained model
2025-05-03 18:14:51,499:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 18:14:51,505:INFO:Starting cross validation
2025-05-03 18:14:51,506:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:51,958:INFO:Calculating mean and std
2025-05-03 18:14:51,958:INFO:Creating metrics dataframe
2025-05-03 18:14:51,965:INFO:Finalizing model
2025-05-03 18:14:51,979:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:14:51,979:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:14:51,979:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:14:51,999:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:14:51,999:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:14:51,999:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:14:51,999:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 18:14:52,003:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.
2025-05-03 18:14:52,003:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 18:14:52,003:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 18:14:52,003:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 18:14:52,003:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 18:14:52,003:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 18:14:52,003:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 18:14:52,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,105:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,107:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,110:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,116:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,118:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,120:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,124:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,127:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,128:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,130:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,131:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,132:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,134:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,136:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,138:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,140:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,143:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,143:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,144:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,145:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,146:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,146:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,147:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,148:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,149:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,151:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,151:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,154:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,156:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,157:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,157:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,159:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,161:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,161:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,163:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,164:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,167:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,169:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,171:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,171:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,172:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,173:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,173:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,174:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,174:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,176:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,181:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,185:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,187:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,189:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,191:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,192:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,193:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,194:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,194:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,195:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,195:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,196:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,197:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,199:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,199:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 18:14:52,200:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 18:14:52,211:INFO:Uploading results into container
2025-05-03 18:14:52,213:INFO:Uploading model into container now
2025-05-03 18:14:52,225:INFO:_master_model_container: 10
2025-05-03 18:14:52,226:INFO:_display_container: 6
2025-05-03 18:14:52,227:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 18:14:52,227:INFO:create_model() successfully completed......................................
2025-05-03 18:14:52,378:INFO:Initializing create_model()
2025-05-03 18:14:52,378:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 18:14:52,378:INFO:Checking exceptions
2025-05-03 18:14:52,403:INFO:Importing libraries
2025-05-03 18:14:52,404:INFO:Copying training dataset
2025-05-03 18:14:52,423:INFO:Defining folds
2025-05-03 18:14:52,423:INFO:Declaring metric variables
2025-05-03 18:14:52,427:INFO:Importing untrained model
2025-05-03 18:14:52,431:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 18:14:52,437:INFO:Starting cross validation
2025-05-03 18:14:52,438:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:52,911:INFO:Calculating mean and std
2025-05-03 18:14:52,911:INFO:Creating metrics dataframe
2025-05-03 18:14:52,915:INFO:Finalizing model
2025-05-03 18:14:53,104:INFO:Uploading results into container
2025-05-03 18:14:53,105:INFO:Uploading model into container now
2025-05-03 18:14:53,118:INFO:_master_model_container: 11
2025-05-03 18:14:53,118:INFO:_display_container: 7
2025-05-03 18:14:53,118:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 18:14:53,118:INFO:create_model() successfully completed......................................
2025-05-03 18:14:53,255:INFO:Initializing create_model()
2025-05-03 18:14:53,255:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 18:14:53,255:INFO:Checking exceptions
2025-05-03 18:14:53,268:INFO:Importing libraries
2025-05-03 18:14:53,268:INFO:Copying training dataset
2025-05-03 18:14:53,289:INFO:Defining folds
2025-05-03 18:14:53,289:INFO:Declaring metric variables
2025-05-03 18:14:53,294:INFO:Importing untrained model
2025-05-03 18:14:53,299:INFO:Random Forest Classifier Imported successfully
2025-05-03 18:14:53,304:INFO:Starting cross validation
2025-05-03 18:14:53,304:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 18:14:57,981:INFO:Calculating mean and std
2025-05-03 18:14:57,983:INFO:Creating metrics dataframe
2025-05-03 18:14:57,988:INFO:Finalizing model
2025-05-03 18:15:00,533:INFO:Uploading results into container
2025-05-03 18:15:00,534:INFO:Uploading model into container now
2025-05-03 18:15:00,540:INFO:_master_model_container: 12
2025-05-03 18:15:00,540:INFO:_display_container: 8
2025-05-03 18:15:00,540:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 18:15:00,540:INFO:create_model() successfully completed......................................
2025-05-03 18:15:00,668:INFO:Initializing interpret_model()
2025-05-03 18:15:00,668:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:15:00,668:INFO:Checking exceptions
2025-05-03 18:15:00,668:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:15:00,687:INFO:plot type: msa
2025-05-03 18:15:01,050:INFO:Visual Rendered Successfully
2025-05-03 18:15:01,050:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:01,284:INFO:Initializing interpret_model()
2025-05-03 18:15:01,284:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:15:01,284:INFO:Checking exceptions
2025-05-03 18:15:01,284:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:15:01,303:INFO:plot type: msa
2025-05-03 18:15:01,562:INFO:Visual Rendered Successfully
2025-05-03 18:15:01,562:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:01,720:INFO:Initializing interpret_model()
2025-05-03 18:15:01,721:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:15:01,721:INFO:Checking exceptions
2025-05-03 18:15:01,721:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:15:01,760:INFO:plot type: msa
2025-05-03 18:15:01,761:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:15:02,002:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:15:02,100:INFO:Visual Rendered Successfully
2025-05-03 18:15:02,100:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:02,236:INFO:Initializing save_model()
2025-05-03 18:15:02,236:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:15:02,236:INFO:Adding model into prep_pipe
2025-05-03 18:15:02,245:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 18:15:02,249:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 18:15:02,251:INFO:save_model() successfully completed......................................
2025-05-03 18:15:02,384:INFO:Initializing save_model()
2025-05-03 18:15:02,384:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:15:02,384:INFO:Adding model into prep_pipe
2025-05-03 18:15:02,401:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 18:15:02,405:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 18:15:02,405:INFO:save_model() successfully completed......................................
2025-05-03 18:15:02,541:INFO:Initializing save_model()
2025-05-03 18:15:02,541:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:15:02,541:INFO:Adding model into prep_pipe
2025-05-03 18:15:02,636:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 18:15:02,639:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 18:15:02,639:INFO:save_model() successfully completed......................................
2025-05-03 18:15:02,786:INFO:Initializing interpret_model()
2025-05-03 18:15:02,786:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:15:02,786:INFO:Checking exceptions
2025-05-03 18:15:02,787:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:15:02,804:INFO:plot type: summary
2025-05-03 18:15:02,804:INFO:Creating TreeExplainer
2025-05-03 18:15:02,842:INFO:Compiling shap values
2025-05-03 18:15:03,764:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 18:15:03,953:INFO:Visual Rendered Successfully
2025-05-03 18:15:03,953:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:04,071:INFO:Initializing interpret_model()
2025-05-03 18:15:04,071:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:15:04,071:INFO:Checking exceptions
2025-05-03 18:15:04,071:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:15:04,104:INFO:plot type: summary
2025-05-03 18:15:04,104:INFO:Creating TreeExplainer
2025-05-03 18:15:04,146:INFO:Compiling shap values
2025-05-03 18:15:05,088:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning:

LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray


2025-05-03 18:15:06,273:INFO:Visual Rendered Successfully
2025-05-03 18:15:06,273:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:06,404:INFO:Initializing interpret_model()
2025-05-03 18:15:06,404:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:15:06,404:INFO:Checking exceptions
2025-05-03 18:15:06,404:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:15:06,433:INFO:plot type: summary
2025-05-03 18:15:06,433:INFO:Creating TreeExplainer
2025-05-03 18:15:06,462:INFO:Compiling shap values
2025-05-03 18:15:08,568:INFO:Visual Rendered Successfully
2025-05-03 18:15:08,569:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:08,687:INFO:Initializing interpret_model()
2025-05-03 18:15:08,687:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:15:08,688:INFO:Checking exceptions
2025-05-03 18:15:08,688:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:15:08,703:INFO:plot type: summary
2025-05-03 18:15:08,703:INFO:Creating TreeExplainer
2025-05-03 18:15:08,727:INFO:Compiling shap values
2025-05-03 18:15:11,806:INFO:Visual Rendered Successfully
2025-05-03 18:15:11,806:INFO:interpret_model() successfully completed......................................
2025-05-03 18:15:11,959:INFO:Initializing interpret_model()
2025-05-03 18:15:11,959:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=summary, feature=None, observation=None, use_train_data=True, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:15:11,959:INFO:Checking exceptions
2025-05-03 18:15:11,960:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:15:11,983:INFO:plot type: summary
2025-05-03 18:15:11,984:INFO:Creating TreeExplainer
2025-05-03 18:15:12,011:INFO:Compiling shap values
2025-05-03 18:32:25,409:INFO:Visual Rendered Successfully
2025-05-03 18:32:25,409:INFO:interpret_model() successfully completed......................................
2025-05-03 18:32:25,577:INFO:Initializing save_model()
2025-05-03 18:32:25,577:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 18:32:25,577:INFO:Adding model into prep_pipe
2025-05-03 18:32:25,709:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 18:32:25,726:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 18:32:25,726:INFO:save_model() successfully completed......................................
2025-05-03 18:32:26,106:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:97: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 18:32:26,106:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:132: DataConversionWarning:

A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().


2025-05-03 18:43:25,104:INFO:Initializing interpret_model()
2025-05-03 18:43:25,104:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=msa, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 18:43:25,104:INFO:Checking exceptions
2025-05-03 18:43:25,104:INFO:Soft dependency imported: interpret: 0.6.9
2025-05-03 18:43:25,127:INFO:plot type: msa
2025-05-03 18:43:25,132:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:43:25,400:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:43:25,502:INFO:Visual Rendered Successfully
2025-05-03 18:43:25,502:INFO:interpret_model() successfully completed......................................
2025-05-03 18:49:42,988:INFO:Initializing interpret_model()
2025-05-03 18:49:42,988:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 18:49:42,989:INFO:Checking exceptions
2025-05-03 18:49:42,989:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 18:53:07,043:INFO:Initializing plot_model()
2025-05-03 18:53:07,043:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:53:07,043:INFO:Checking exceptions
2025-05-03 18:53:28,789:INFO:Initializing plot_model()
2025-05-03 18:53:28,790:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:53:28,790:INFO:Checking exceptions
2025-05-03 18:53:28,807:INFO:Preloading libraries
2025-05-03 18:53:29,017:INFO:Copying training dataset
2025-05-03 18:53:29,017:INFO:Plot type: auc
2025-05-03 18:53:29,103:INFO:Fitting Model
2025-05-03 18:53:29,103:INFO:Scoring test/hold-out set
2025-05-03 18:53:29,119:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:53:29,119:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:53:29,119:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:53:29,135:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 18:53:29,211:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:53:29,211:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:53:29,211:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:53:29,224:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 18:53:29,454:INFO:Visual Rendered Successfully
2025-05-03 18:53:29,604:INFO:plot_model() successfully completed......................................
2025-05-03 18:53:40,500:INFO:Initializing plot_model()
2025-05-03 18:53:40,500:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:53:40,500:INFO:Checking exceptions
2025-05-03 18:54:27,407:INFO:Initializing plot_model()
2025-05-03 18:54:27,407:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:54:27,407:INFO:Checking exceptions
2025-05-03 18:54:39,524:INFO:Initializing plot_model()
2025-05-03 18:54:39,524:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:54:39,524:INFO:Checking exceptions
2025-05-03 18:54:39,587:INFO:Preloading libraries
2025-05-03 18:54:39,634:INFO:Copying training dataset
2025-05-03 18:54:39,634:INFO:Plot type: feature
2025-05-03 18:54:39,635:WARNING:No coef_ found. Trying feature_importances_
2025-05-03 18:54:39,827:INFO:Visual Rendered Successfully
2025-05-03 18:54:39,977:INFO:plot_model() successfully completed......................................
2025-05-03 18:56:56,364:INFO:Initializing plot_model()
2025-05-03 18:56:56,366:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:56:56,366:INFO:Checking exceptions
2025-05-03 18:56:56,376:INFO:Preloading libraries
2025-05-03 18:56:56,385:INFO:Copying training dataset
2025-05-03 18:56:56,385:INFO:Plot type: auc
2025-05-03 18:56:56,532:INFO:Fitting Model
2025-05-03 18:56:56,532:INFO:Scoring test/hold-out set
2025-05-03 18:56:56,733:INFO:Visual Rendered Successfully
2025-05-03 18:56:56,881:INFO:plot_model() successfully completed......................................
2025-05-03 18:57:11,798:INFO:Initializing plot_model()
2025-05-03 18:57:11,798:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:57:11,798:INFO:Checking exceptions
2025-05-03 18:57:11,807:INFO:Preloading libraries
2025-05-03 18:57:11,862:INFO:Copying training dataset
2025-05-03 18:57:11,862:INFO:Plot type: auc
2025-05-03 18:57:11,969:INFO:Fitting Model
2025-05-03 18:57:11,969:INFO:Scoring test/hold-out set
2025-05-03 18:57:12,165:INFO:Visual Rendered Successfully
2025-05-03 18:57:12,300:INFO:plot_model() successfully completed......................................
2025-05-03 18:57:15,852:INFO:Initializing plot_model()
2025-05-03 18:57:15,852:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:57:15,852:INFO:Checking exceptions
2025-05-03 18:57:15,863:INFO:Preloading libraries
2025-05-03 18:57:15,868:INFO:Copying training dataset
2025-05-03 18:57:15,868:INFO:Plot type: auc
2025-05-03 18:57:15,981:INFO:Fitting Model
2025-05-03 18:57:15,982:INFO:Scoring test/hold-out set
2025-05-03 18:57:15,982:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:57:15,982:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:57:15,982:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:57:15,992:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:57:15,992:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:57:15,992:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:57:16,177:INFO:Visual Rendered Successfully
2025-05-03 18:57:16,317:INFO:plot_model() successfully completed......................................
2025-05-03 18:57:19,216:INFO:Initializing plot_model()
2025-05-03 18:57:19,216:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:57:19,216:INFO:Checking exceptions
2025-05-03 18:57:19,225:INFO:Preloading libraries
2025-05-03 18:57:19,237:INFO:Copying training dataset
2025-05-03 18:57:19,237:INFO:Plot type: auc
2025-05-03 18:57:19,346:INFO:Fitting Model
2025-05-03 18:57:19,347:INFO:Scoring test/hold-out set
2025-05-03 18:57:19,348:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:57:19,348:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:57:19,349:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:57:19,360:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:57:19,361:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:57:19,361:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:57:19,542:INFO:Visual Rendered Successfully
2025-05-03 18:57:19,664:INFO:plot_model() successfully completed......................................
2025-05-03 18:57:34,239:INFO:Initializing plot_model()
2025-05-03 18:57:34,239:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:57:34,239:INFO:Checking exceptions
2025-05-03 18:57:34,293:INFO:Preloading libraries
2025-05-03 18:57:34,330:INFO:Copying training dataset
2025-05-03 18:57:34,331:INFO:Plot type: auc
2025-05-03 18:57:34,404:INFO:Fitting Model
2025-05-03 18:57:34,404:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning:

X does not have valid feature names, but RandomForestClassifier was fitted with feature names


2025-05-03 18:57:34,404:INFO:Scoring test/hold-out set
2025-05-03 18:57:34,708:INFO:Visual Rendered Successfully
2025-05-03 18:57:34,867:INFO:plot_model() successfully completed......................................
2025-05-03 18:58:42,178:INFO:Initializing plot_model()
2025-05-03 18:58:42,178:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:58:42,178:INFO:Checking exceptions
2025-05-03 18:58:42,186:INFO:Preloading libraries
2025-05-03 18:58:42,329:INFO:Copying training dataset
2025-05-03 18:58:42,329:INFO:Plot type: auc
2025-05-03 18:58:42,411:INFO:Fitting Model
2025-05-03 18:58:42,411:INFO:Scoring test/hold-out set
2025-05-03 18:58:42,428:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:58:42,428:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:58:42,428:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:58:42,441:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 18:58:42,539:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 18:58:42,539:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 18:58:42,540:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 18:58:42,585:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 18:58:42,861:INFO:Visual Rendered Successfully
2025-05-03 18:58:42,996:INFO:plot_model() successfully completed......................................
2025-05-03 18:59:16,513:INFO:Initializing plot_model()
2025-05-03 18:59:16,513:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=lift, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 18:59:16,513:INFO:Checking exceptions
2025-05-03 18:59:16,524:INFO:Preloading libraries
2025-05-03 18:59:16,529:INFO:Copying training dataset
2025-05-03 18:59:16,529:INFO:Plot type: lift
2025-05-03 18:59:16,530:INFO:Generating predictions / predict_proba on X_test
2025-05-03 18:59:16,751:INFO:Visual Rendered Successfully
2025-05-03 18:59:16,883:INFO:plot_model() successfully completed......................................
2025-05-03 19:00:34,209:INFO:Initializing plot_model()
2025-05-03 19:00:34,209:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=threshold, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 19:00:34,209:INFO:Checking exceptions
2025-05-03 19:00:34,223:INFO:Preloading libraries
2025-05-03 19:00:34,367:INFO:Copying training dataset
2025-05-03 19:00:34,367:INFO:Plot type: threshold
2025-05-03 19:00:34,466:INFO:Fitting Model
2025-05-03 19:01:14,131:INFO:Initializing plot_model()
2025-05-03 19:01:14,131:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 19:01:14,131:INFO:Checking exceptions
2025-05-03 19:01:14,149:INFO:Preloading libraries
2025-05-03 19:01:14,286:INFO:Copying training dataset
2025-05-03 19:01:14,286:INFO:Plot type: auc
2025-05-03 19:01:14,372:INFO:Fitting Model
2025-05-03 19:01:14,372:INFO:Scoring test/hold-out set
2025-05-03 19:01:14,372:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 19:01:14,372:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 19:01:14,372:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 19:01:14,397:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 19:01:14,494:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 19:01:14,494:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 19:01:14,494:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 19:01:14,515:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 19:01:14,834:INFO:Visual Rendered Successfully
2025-05-03 19:01:15,003:INFO:plot_model() successfully completed......................................
2025-05-03 19:01:29,006:INFO:Initializing plot_model()
2025-05-03 19:01:29,006:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F751EBCA50>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=pr, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 19:01:29,006:INFO:Checking exceptions
2025-05-03 19:01:29,009:INFO:Preloading libraries
2025-05-03 19:01:29,175:INFO:Copying training dataset
2025-05-03 19:01:29,175:INFO:Plot type: pr
2025-05-03 19:01:29,307:INFO:Fitting Model
2025-05-03 19:01:29,307:INFO:Scoring test/hold-out set
2025-05-03 19:01:29,307:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 19:01:29,307:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 19:01:29,307:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 19:01:29,359:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 19:01:29,460:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 19:01:29,460:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 19:01:29,460:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 19:01:29,487:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:486: UserWarning:

X has feature names, but RandomForestClassifier was fitted without feature names


2025-05-03 19:01:29,952:INFO:Visual Rendered Successfully
2025-05-03 19:01:30,182:INFO:plot_model() successfully completed......................................
2025-05-03 22:34:44,615:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:34:44,616:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:34:44,616:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:34:44,616:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:34:49,410:INFO:PyCaret ClassificationExperiment
2025-05-03 22:34:49,410:INFO:Logging name: clf-default-name
2025-05-03 22:34:49,410:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 22:34:49,410:INFO:version 3.3.2
2025-05-03 22:34:49,412:INFO:Initializing setup()
2025-05-03 22:34:49,412:INFO:self.USI: 32ca
2025-05-03 22:34:49,412:INFO:self._variable_keys: {'X_train', 'exp_id', 'gpu_n_jobs_param', 'fix_imbalance', 'X', 'seed', 'fold_shuffle_param', 'target_param', 'is_multiclass', 'idx', '_available_plots', 'fold_generator', 'fold_groups_param', 'exp_name_log', 'y', '_ml_usecase', 'USI', 'logging_param', 'data', 'y_test', 'y_train', 'X_test', 'n_jobs_param', 'memory', 'log_plots_param', 'pipeline', 'gpu_param', 'html_param'}
2025-05-03 22:34:49,412:INFO:Checking environment
2025-05-03 22:34:49,412:INFO:python_version: 3.11.11
2025-05-03 22:34:49,412:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 22:34:49,412:INFO:machine: AMD64
2025-05-03 22:34:49,412:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 22:34:49,416:INFO:Memory: svmem(total=16965230592, available=4402671616, percent=74.0, used=12562558976, free=4402671616)
2025-05-03 22:34:49,416:INFO:Physical Core: 4
2025-05-03 22:34:49,416:INFO:Logical Core: 8
2025-05-03 22:34:49,416:INFO:Checking libraries
2025-05-03 22:34:49,416:INFO:System:
2025-05-03 22:34:49,416:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 22:34:49,416:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 22:34:49,416:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 22:34:49,416:INFO:PyCaret required dependencies:
2025-05-03 22:34:49,421:INFO:                 pip: 25.0
2025-05-03 22:34:49,421:INFO:          setuptools: 75.8.0
2025-05-03 22:34:49,421:INFO:             pycaret: 3.3.2
2025-05-03 22:34:49,421:INFO:             IPython: 8.32.0
2025-05-03 22:34:49,421:INFO:          ipywidgets: 8.1.6
2025-05-03 22:34:49,421:INFO:                tqdm: 4.67.1
2025-05-03 22:34:49,421:INFO:               numpy: 1.26.4
2025-05-03 22:34:49,421:INFO:              pandas: 2.1.4
2025-05-03 22:34:49,421:INFO:              jinja2: 3.1.6
2025-05-03 22:34:49,421:INFO:               scipy: 1.11.4
2025-05-03 22:34:49,421:INFO:              joblib: 1.3.2
2025-05-03 22:34:49,421:INFO:             sklearn: 1.4.2
2025-05-03 22:34:49,421:INFO:                pyod: 2.0.5
2025-05-03 22:34:49,421:INFO:            imblearn: 0.13.0
2025-05-03 22:34:49,421:INFO:   category_encoders: 2.7.0
2025-05-03 22:34:49,421:INFO:            lightgbm: 4.6.0
2025-05-03 22:34:49,421:INFO:               numba: 0.61.0
2025-05-03 22:34:49,421:INFO:            requests: 2.32.3
2025-05-03 22:34:49,421:INFO:          matplotlib: 3.7.5
2025-05-03 22:34:49,421:INFO:          scikitplot: 0.3.7
2025-05-03 22:34:49,421:INFO:         yellowbrick: 1.5
2025-05-03 22:34:49,421:INFO:              plotly: 5.24.1
2025-05-03 22:34:49,421:INFO:    plotly-resampler: Not installed
2025-05-03 22:34:49,421:INFO:             kaleido: 0.2.1
2025-05-03 22:34:49,421:INFO:           schemdraw: 0.15
2025-05-03 22:34:49,421:INFO:         statsmodels: 0.14.4
2025-05-03 22:34:49,421:INFO:              sktime: 0.26.0
2025-05-03 22:34:49,421:INFO:               tbats: 1.1.3
2025-05-03 22:34:49,421:INFO:            pmdarima: 2.0.4
2025-05-03 22:34:49,421:INFO:              psutil: 6.1.1
2025-05-03 22:34:49,421:INFO:          markupsafe: 3.0.2
2025-05-03 22:34:49,421:INFO:             pickle5: Not installed
2025-05-03 22:34:49,421:INFO:         cloudpickle: 3.1.1
2025-05-03 22:34:49,421:INFO:         deprecation: 2.1.0
2025-05-03 22:34:49,421:INFO:              xxhash: 3.5.0
2025-05-03 22:34:49,421:INFO:           wurlitzer: Not installed
2025-05-03 22:34:49,423:INFO:PyCaret optional dependencies:
2025-05-03 22:34:49,900:INFO:                shap: 0.46.0
2025-05-03 22:34:49,900:INFO:           interpret: 0.6.9
2025-05-03 22:34:49,900:INFO:                umap: Not installed
2025-05-03 22:34:49,900:INFO:     ydata_profiling: Not installed
2025-05-03 22:34:49,908:INFO:  explainerdashboard: Not installed
2025-05-03 22:34:49,908:INFO:             autoviz: Not installed
2025-05-03 22:34:49,908:INFO:           fairlearn: Not installed
2025-05-03 22:34:49,908:INFO:          deepchecks: Not installed
2025-05-03 22:34:49,908:INFO:             xgboost: 3.0.0
2025-05-03 22:34:49,908:INFO:            catboost: Not installed
2025-05-03 22:34:49,908:INFO:              kmodes: Not installed
2025-05-03 22:34:49,908:INFO:             mlxtend: Not installed
2025-05-03 22:34:49,908:INFO:       statsforecast: Not installed
2025-05-03 22:34:49,908:INFO:        tune_sklearn: Not installed
2025-05-03 22:34:49,908:INFO:                 ray: Not installed
2025-05-03 22:34:49,908:INFO:            hyperopt: 0.2.7
2025-05-03 22:34:49,908:INFO:              optuna: Not installed
2025-05-03 22:34:49,908:INFO:               skopt: 0.10.2
2025-05-03 22:34:49,908:INFO:              mlflow: 2.22.0
2025-05-03 22:34:49,908:INFO:              gradio: Not installed
2025-05-03 22:34:49,908:INFO:             fastapi: 0.115.12
2025-05-03 22:34:49,908:INFO:             uvicorn: 0.34.2
2025-05-03 22:34:49,908:INFO:              m2cgen: Not installed
2025-05-03 22:34:49,908:INFO:           evidently: Not installed
2025-05-03 22:34:49,908:INFO:               fugue: Not installed
2025-05-03 22:34:49,908:INFO:           streamlit: Not installed
2025-05-03 22:34:49,908:INFO:             prophet: Not installed
2025-05-03 22:34:49,908:INFO:None
2025-05-03 22:34:49,908:INFO:Set up data.
2025-05-03 22:34:49,924:INFO:Set up folding strategy.
2025-05-03 22:34:49,924:INFO:Set up train/test split.
2025-05-03 22:34:49,957:INFO:Set up index.
2025-05-03 22:34:49,957:INFO:Assigning column types.
2025-05-03 22:34:49,974:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 22:34:50,039:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,049:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,085:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,088:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,142:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,142:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,170:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,170:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 22:34:50,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,250:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,252:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,292:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:34:50,325:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,334:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,334:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 22:34:50,412:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,412:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,494:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,498:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,504:INFO:Set up column name cleaning.
2025-05-03 22:34:50,548:INFO:Finished creating preprocessing pipeline.
2025-05-03 22:34:50,549:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 22:34:50,549:INFO:Creating final display dataframe.
2025-05-03 22:34:50,646:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 22:34:50,712:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,720:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,795:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:34:50,803:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:34:50,803:INFO:setup() successfully completed in 1.4s...............
2025-05-03 22:34:50,817:INFO:Initializing compare_models()
2025-05-03 22:34:50,817:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 22:34:50,819:INFO:Checking exceptions
2025-05-03 22:34:50,838:INFO:Preparing display monitor
2025-05-03 22:34:50,892:INFO:Initializing Logistic Regression
2025-05-03 22:34:50,892:INFO:Total runtime is 0.0 minutes
2025-05-03 22:34:50,900:INFO:SubProcess create_model() called ==================================
2025-05-03 22:34:50,900:INFO:Initializing create_model()
2025-05-03 22:34:50,900:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:34:50,900:INFO:Checking exceptions
2025-05-03 22:34:50,900:INFO:Importing libraries
2025-05-03 22:34:50,900:INFO:Copying training dataset
2025-05-03 22:34:50,916:INFO:Defining folds
2025-05-03 22:34:50,916:INFO:Declaring metric variables
2025-05-03 22:34:50,916:INFO:Importing untrained model
2025-05-03 22:34:50,925:INFO:Logistic Regression Imported successfully
2025-05-03 22:34:50,925:INFO:Starting cross validation
2025-05-03 22:34:50,933:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:34:55,023:INFO:Calculating mean and std
2025-05-03 22:34:55,025:INFO:Creating metrics dataframe
2025-05-03 22:34:55,029:INFO:Uploading results into container
2025-05-03 22:34:55,029:INFO:Uploading model into container now
2025-05-03 22:34:55,029:INFO:_master_model_container: 1
2025-05-03 22:34:55,029:INFO:_display_container: 2
2025-05-03 22:34:55,029:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 22:34:55,029:INFO:create_model() successfully completed......................................
2025-05-03 22:34:55,161:INFO:SubProcess create_model() end ==================================
2025-05-03 22:34:55,161:INFO:Creating metrics dataframe
2025-05-03 22:34:55,171:INFO:Initializing Random Forest Classifier
2025-05-03 22:34:55,171:INFO:Total runtime is 0.07132664918899537 minutes
2025-05-03 22:34:55,171:INFO:SubProcess create_model() called ==================================
2025-05-03 22:34:55,178:INFO:Initializing create_model()
2025-05-03 22:34:55,178:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:34:55,178:INFO:Checking exceptions
2025-05-03 22:34:55,178:INFO:Importing libraries
2025-05-03 22:34:55,178:INFO:Copying training dataset
2025-05-03 22:34:55,194:INFO:Defining folds
2025-05-03 22:34:55,194:INFO:Declaring metric variables
2025-05-03 22:34:55,194:INFO:Importing untrained model
2025-05-03 22:34:55,203:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:34:55,211:INFO:Starting cross validation
2025-05-03 22:34:55,211:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:00,058:INFO:Calculating mean and std
2025-05-03 22:35:00,060:INFO:Creating metrics dataframe
2025-05-03 22:35:00,062:INFO:Uploading results into container
2025-05-03 22:35:00,062:INFO:Uploading model into container now
2025-05-03 22:35:00,062:INFO:_master_model_container: 2
2025-05-03 22:35:00,062:INFO:_display_container: 2
2025-05-03 22:35:00,065:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:35:00,065:INFO:create_model() successfully completed......................................
2025-05-03 22:35:00,223:INFO:SubProcess create_model() end ==================================
2025-05-03 22:35:00,226:INFO:Creating metrics dataframe
2025-05-03 22:35:00,234:INFO:Initializing Extreme Gradient Boosting
2025-05-03 22:35:00,234:INFO:Total runtime is 0.1556949774424235 minutes
2025-05-03 22:35:00,234:INFO:SubProcess create_model() called ==================================
2025-05-03 22:35:00,234:INFO:Initializing create_model()
2025-05-03 22:35:00,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:00,234:INFO:Checking exceptions
2025-05-03 22:35:00,234:INFO:Importing libraries
2025-05-03 22:35:00,234:INFO:Copying training dataset
2025-05-03 22:35:00,256:INFO:Defining folds
2025-05-03 22:35:00,256:INFO:Declaring metric variables
2025-05-03 22:35:00,256:INFO:Importing untrained model
2025-05-03 22:35:00,267:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:35:00,274:INFO:Starting cross validation
2025-05-03 22:35:00,274:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:03,696:INFO:Calculating mean and std
2025-05-03 22:35:03,698:INFO:Creating metrics dataframe
2025-05-03 22:35:03,700:INFO:Uploading results into container
2025-05-03 22:35:03,700:INFO:Uploading model into container now
2025-05-03 22:35:03,700:INFO:_master_model_container: 3
2025-05-03 22:35:03,700:INFO:_display_container: 2
2025-05-03 22:35:03,702:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:35:03,702:INFO:create_model() successfully completed......................................
2025-05-03 22:35:03,859:INFO:SubProcess create_model() end ==================================
2025-05-03 22:35:03,859:INFO:Creating metrics dataframe
2025-05-03 22:35:03,867:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 22:35:03,867:INFO:Total runtime is 0.21625995238622028 minutes
2025-05-03 22:35:03,875:INFO:SubProcess create_model() called ==================================
2025-05-03 22:35:03,875:INFO:Initializing create_model()
2025-05-03 22:35:03,875:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:03,875:INFO:Checking exceptions
2025-05-03 22:35:03,875:INFO:Importing libraries
2025-05-03 22:35:03,875:INFO:Copying training dataset
2025-05-03 22:35:03,894:INFO:Defining folds
2025-05-03 22:35:03,894:INFO:Declaring metric variables
2025-05-03 22:35:03,898:INFO:Importing untrained model
2025-05-03 22:35:03,900:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:35:03,906:INFO:Starting cross validation
2025-05-03 22:35:03,906:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:04,384:INFO:Calculating mean and std
2025-05-03 22:35:04,386:INFO:Creating metrics dataframe
2025-05-03 22:35:04,389:INFO:Uploading results into container
2025-05-03 22:35:04,389:INFO:Uploading model into container now
2025-05-03 22:35:04,389:INFO:_master_model_container: 4
2025-05-03 22:35:04,389:INFO:_display_container: 2
2025-05-03 22:35:04,391:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:35:04,391:INFO:create_model() successfully completed......................................
2025-05-03 22:35:04,537:INFO:SubProcess create_model() end ==================================
2025-05-03 22:35:04,537:INFO:Creating metrics dataframe
2025-05-03 22:35:04,545:INFO:Initializing Extra Trees Classifier
2025-05-03 22:35:04,545:INFO:Total runtime is 0.227553387482961 minutes
2025-05-03 22:35:04,545:INFO:SubProcess create_model() called ==================================
2025-05-03 22:35:04,553:INFO:Initializing create_model()
2025-05-03 22:35:04,553:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:04,553:INFO:Checking exceptions
2025-05-03 22:35:04,553:INFO:Importing libraries
2025-05-03 22:35:04,553:INFO:Copying training dataset
2025-05-03 22:35:04,569:INFO:Defining folds
2025-05-03 22:35:04,569:INFO:Declaring metric variables
2025-05-03 22:35:04,569:INFO:Importing untrained model
2025-05-03 22:35:04,578:INFO:Extra Trees Classifier Imported successfully
2025-05-03 22:35:04,587:INFO:Starting cross validation
2025-05-03 22:35:04,587:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:06,012:INFO:Calculating mean and std
2025-05-03 22:35:06,012:INFO:Creating metrics dataframe
2025-05-03 22:35:06,012:INFO:Uploading results into container
2025-05-03 22:35:06,012:INFO:Uploading model into container now
2025-05-03 22:35:06,012:INFO:_master_model_container: 5
2025-05-03 22:35:06,012:INFO:_display_container: 2
2025-05-03 22:35:06,012:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 22:35:06,012:INFO:create_model() successfully completed......................................
2025-05-03 22:35:06,268:INFO:SubProcess create_model() end ==================================
2025-05-03 22:35:06,268:INFO:Creating metrics dataframe
2025-05-03 22:35:06,290:INFO:Initializing Ridge Classifier
2025-05-03 22:35:06,290:INFO:Total runtime is 0.2566310087839762 minutes
2025-05-03 22:35:06,294:INFO:SubProcess create_model() called ==================================
2025-05-03 22:35:06,297:INFO:Initializing create_model()
2025-05-03 22:35:06,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7F7D9C990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:06,297:INFO:Checking exceptions
2025-05-03 22:35:06,297:INFO:Importing libraries
2025-05-03 22:35:06,297:INFO:Copying training dataset
2025-05-03 22:35:06,330:INFO:Defining folds
2025-05-03 22:35:06,330:INFO:Declaring metric variables
2025-05-03 22:35:06,339:INFO:Importing untrained model
2025-05-03 22:35:06,347:INFO:Ridge Classifier Imported successfully
2025-05-03 22:35:06,361:INFO:Starting cross validation
2025-05-03 22:35:06,364:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:06,455:INFO:Calculating mean and std
2025-05-03 22:35:06,455:INFO:Creating metrics dataframe
2025-05-03 22:35:06,455:INFO:Uploading results into container
2025-05-03 22:35:06,455:INFO:Uploading model into container now
2025-05-03 22:35:06,455:INFO:_master_model_container: 6
2025-05-03 22:35:06,455:INFO:_display_container: 2
2025-05-03 22:35:06,455:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 22:35:06,455:INFO:create_model() successfully completed......................................
2025-05-03 22:35:06,594:INFO:SubProcess create_model() end ==================================
2025-05-03 22:35:06,594:INFO:Creating metrics dataframe
2025-05-03 22:35:06,603:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 22:35:06,619:INFO:Initializing create_model()
2025-05-03 22:35:06,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:06,619:INFO:Checking exceptions
2025-05-03 22:35:06,620:INFO:Importing libraries
2025-05-03 22:35:06,620:INFO:Copying training dataset
2025-05-03 22:35:06,644:INFO:Defining folds
2025-05-03 22:35:06,644:INFO:Declaring metric variables
2025-05-03 22:35:06,644:INFO:Importing untrained model
2025-05-03 22:35:06,644:INFO:Declaring custom model
2025-05-03 22:35:06,644:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:35:06,644:INFO:Cross validation set to False
2025-05-03 22:35:06,644:INFO:Fitting Model
2025-05-03 22:35:06,683:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:35:06,687:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.
2025-05-03 22:35:06,687:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:35:06,687:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:35:06,687:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:35:06,689:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:35:06,689:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:35:06,689:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:35:06,859:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:35:06,859:INFO:create_model() successfully completed......................................
2025-05-03 22:35:07,119:INFO:_master_model_container: 6
2025-05-03 22:35:07,119:INFO:_display_container: 2
2025-05-03 22:35:07,119:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:35:07,119:INFO:compare_models() successfully completed......................................
2025-05-03 22:35:07,133:INFO:Initializing create_model()
2025-05-03 22:35:07,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:07,136:INFO:Checking exceptions
2025-05-03 22:35:07,153:INFO:Importing libraries
2025-05-03 22:35:07,153:INFO:Copying training dataset
2025-05-03 22:35:07,173:INFO:Defining folds
2025-05-03 22:35:07,173:INFO:Declaring metric variables
2025-05-03 22:35:07,173:INFO:Importing untrained model
2025-05-03 22:35:07,181:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:35:07,192:INFO:Starting cross validation
2025-05-03 22:35:07,192:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:07,849:INFO:Calculating mean and std
2025-05-03 22:35:07,850:INFO:Creating metrics dataframe
2025-05-03 22:35:07,856:INFO:Finalizing model
2025-05-03 22:35:07,883:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:35:07,888:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.
2025-05-03 22:35:07,888:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:35:07,888:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:35:07,888:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:35:07,888:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:35:07,888:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:35:07,888:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:35:08,071:INFO:Uploading results into container
2025-05-03 22:35:08,071:INFO:Uploading model into container now
2025-05-03 22:35:08,081:INFO:_master_model_container: 7
2025-05-03 22:35:08,082:INFO:_display_container: 3
2025-05-03 22:35:08,083:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:35:08,083:INFO:create_model() successfully completed......................................
2025-05-03 22:35:08,338:INFO:Initializing create_model()
2025-05-03 22:35:08,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:08,338:INFO:Checking exceptions
2025-05-03 22:35:08,356:INFO:Importing libraries
2025-05-03 22:35:08,356:INFO:Copying training dataset
2025-05-03 22:35:08,387:INFO:Defining folds
2025-05-03 22:35:08,387:INFO:Declaring metric variables
2025-05-03 22:35:08,405:INFO:Importing untrained model
2025-05-03 22:35:08,413:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:35:08,432:INFO:Starting cross validation
2025-05-03 22:35:08,444:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:08,899:INFO:Calculating mean and std
2025-05-03 22:35:08,899:INFO:Creating metrics dataframe
2025-05-03 22:35:08,906:INFO:Finalizing model
2025-05-03 22:35:09,066:INFO:Uploading results into container
2025-05-03 22:35:09,066:INFO:Uploading model into container now
2025-05-03 22:35:09,076:INFO:_master_model_container: 8
2025-05-03 22:35:09,076:INFO:_display_container: 4
2025-05-03 22:35:09,078:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:35:09,078:INFO:create_model() successfully completed......................................
2025-05-03 22:35:09,230:INFO:Initializing create_model()
2025-05-03 22:35:09,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:35:09,230:INFO:Checking exceptions
2025-05-03 22:35:09,249:INFO:Importing libraries
2025-05-03 22:35:09,252:INFO:Copying training dataset
2025-05-03 22:35:09,271:INFO:Defining folds
2025-05-03 22:35:09,271:INFO:Declaring metric variables
2025-05-03 22:35:09,287:INFO:Importing untrained model
2025-05-03 22:35:09,308:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:35:09,329:INFO:Starting cross validation
2025-05-03 22:35:09,335:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:35:10,691:INFO:Calculating mean and std
2025-05-03 22:35:10,692:INFO:Creating metrics dataframe
2025-05-03 22:35:10,696:INFO:Finalizing model
2025-05-03 22:35:11,299:INFO:Uploading results into container
2025-05-03 22:35:11,299:INFO:Uploading model into container now
2025-05-03 22:35:11,305:INFO:_master_model_container: 9
2025-05-03 22:35:11,305:INFO:_display_container: 5
2025-05-03 22:35:11,305:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:35:11,305:INFO:create_model() successfully completed......................................
2025-05-03 22:35:11,453:INFO:Initializing tune_model()
2025-05-03 22:35:11,453:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:35:11,457:INFO:Checking exceptions
2025-05-03 22:35:11,457:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:35:11,486:INFO:Copying training dataset
2025-05-03 22:35:11,505:INFO:Checking base model
2025-05-03 22:35:11,505:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 22:35:11,507:INFO:Declaring metric variables
2025-05-03 22:35:11,511:INFO:Defining Hyperparameters
2025-05-03 22:35:11,643:INFO:Tuning with n_jobs=-1
2025-05-03 22:35:11,652:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:37:08,633:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 22:37:08,635:INFO:Hyperparameter search completed
2025-05-03 22:37:08,635:INFO:SubProcess create_model() called ==================================
2025-05-03 22:37:08,635:INFO:Initializing create_model()
2025-05-03 22:37:08,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7EFE95110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 22:37:08,635:INFO:Checking exceptions
2025-05-03 22:37:08,635:INFO:Importing libraries
2025-05-03 22:37:08,635:INFO:Copying training dataset
2025-05-03 22:37:08,655:INFO:Defining folds
2025-05-03 22:37:08,655:INFO:Declaring metric variables
2025-05-03 22:37:08,655:INFO:Importing untrained model
2025-05-03 22:37:08,655:INFO:Declaring custom model
2025-05-03 22:37:08,655:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:37:08,663:INFO:Starting cross validation
2025-05-03 22:37:08,672:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:37:09,224:INFO:Calculating mean and std
2025-05-03 22:37:09,226:INFO:Creating metrics dataframe
2025-05-03 22:37:09,233:INFO:Finalizing model
2025-05-03 22:37:09,245:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:37:09,245:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:37:09,245:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:37:09,262:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:37:09,263:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:37:09,263:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:37:09,263:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:37:09,267:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.
2025-05-03 22:37:09,267:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:37:09,267:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:37:09,267:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:37:09,268:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:37:09,268:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:37:09,268:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:37:09,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,314:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,390:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,392:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,396:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,398:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,402:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,404:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,408:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,408:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,408:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,409:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,411:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,413:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,415:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,415:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,416:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,417:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,418:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,420:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,420:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,421:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,421:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,422:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,422:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,423:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,424:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,424:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,426:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,426:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,427:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,427:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,428:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,430:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,430:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,431:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,431:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,432:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,432:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,434:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,435:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,435:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,436:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,436:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,437:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,437:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,438:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,440:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:37:09,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:37:09,469:INFO:Uploading results into container
2025-05-03 22:37:09,470:INFO:Uploading model into container now
2025-05-03 22:37:09,470:INFO:_master_model_container: 10
2025-05-03 22:37:09,470:INFO:_display_container: 6
2025-05-03 22:37:09,472:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:37:09,472:INFO:create_model() successfully completed......................................
2025-05-03 22:37:09,621:INFO:SubProcess create_model() end ==================================
2025-05-03 22:37:09,621:INFO:choose_better activated
2025-05-03 22:37:09,629:INFO:SubProcess create_model() called ==================================
2025-05-03 22:37:09,629:INFO:Initializing create_model()
2025-05-03 22:37:09,629:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:37:09,629:INFO:Checking exceptions
2025-05-03 22:37:09,629:INFO:Importing libraries
2025-05-03 22:37:09,629:INFO:Copying training dataset
2025-05-03 22:37:09,645:INFO:Defining folds
2025-05-03 22:37:09,645:INFO:Declaring metric variables
2025-05-03 22:37:09,645:INFO:Importing untrained model
2025-05-03 22:37:09,645:INFO:Declaring custom model
2025-05-03 22:37:09,645:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:37:09,645:INFO:Starting cross validation
2025-05-03 22:37:09,645:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:37:10,189:INFO:Calculating mean and std
2025-05-03 22:37:10,189:INFO:Creating metrics dataframe
2025-05-03 22:37:10,191:INFO:Finalizing model
2025-05-03 22:37:10,216:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:37:10,220:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.
2025-05-03 22:37:10,220:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:37:10,220:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:37:10,220:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:37:10,220:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:37:10,220:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:37:10,220:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:37:10,338:INFO:Uploading results into container
2025-05-03 22:37:10,338:INFO:Uploading model into container now
2025-05-03 22:37:10,340:INFO:_master_model_container: 11
2025-05-03 22:37:10,340:INFO:_display_container: 7
2025-05-03 22:37:10,340:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:37:10,340:INFO:create_model() successfully completed......................................
2025-05-03 22:37:10,492:INFO:SubProcess create_model() end ==================================
2025-05-03 22:37:10,492:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 22:37:10,492:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 22:37:10,492:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 22:37:10,492:INFO:choose_better completed
2025-05-03 22:37:10,500:INFO:_master_model_container: 11
2025-05-03 22:37:10,500:INFO:_display_container: 6
2025-05-03 22:37:10,500:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:37:10,505:INFO:tune_model() successfully completed......................................
2025-05-03 22:37:10,656:INFO:Initializing tune_model()
2025-05-03 22:37:10,656:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:37:10,656:INFO:Checking exceptions
2025-05-03 22:37:10,656:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:37:10,681:INFO:Copying training dataset
2025-05-03 22:37:10,698:INFO:Checking base model
2025-05-03 22:37:10,698:INFO:Base model : Extreme Gradient Boosting
2025-05-03 22:37:10,706:INFO:Declaring metric variables
2025-05-03 22:37:10,710:INFO:Defining Hyperparameters
2025-05-03 22:37:10,846:INFO:Tuning with n_jobs=-1
2025-05-03 22:37:10,848:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:39:05,212:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 22:39:05,212:INFO:Hyperparameter search completed
2025-05-03 22:39:05,212:INFO:SubProcess create_model() called ==================================
2025-05-03 22:39:05,212:INFO:Initializing create_model()
2025-05-03 22:39:05,212:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7EF889910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 22:39:05,212:INFO:Checking exceptions
2025-05-03 22:39:05,212:INFO:Importing libraries
2025-05-03 22:39:05,220:INFO:Copying training dataset
2025-05-03 22:39:05,241:INFO:Defining folds
2025-05-03 22:39:05,241:INFO:Declaring metric variables
2025-05-03 22:39:05,245:INFO:Importing untrained model
2025-05-03 22:39:05,245:INFO:Declaring custom model
2025-05-03 22:39:05,248:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:39:05,252:INFO:Starting cross validation
2025-05-03 22:39:05,252:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:39:05,707:INFO:Calculating mean and std
2025-05-03 22:39:05,707:INFO:Creating metrics dataframe
2025-05-03 22:39:05,712:INFO:Finalizing model
2025-05-03 22:39:05,883:INFO:Uploading results into container
2025-05-03 22:39:05,885:INFO:Uploading model into container now
2025-05-03 22:39:05,885:INFO:_master_model_container: 12
2025-05-03 22:39:05,885:INFO:_display_container: 7
2025-05-03 22:39:05,887:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:39:05,887:INFO:create_model() successfully completed......................................
2025-05-03 22:39:06,031:INFO:SubProcess create_model() end ==================================
2025-05-03 22:39:06,031:INFO:choose_better activated
2025-05-03 22:39:06,033:INFO:SubProcess create_model() called ==================================
2025-05-03 22:39:06,037:INFO:Initializing create_model()
2025-05-03 22:39:06,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:39:06,037:INFO:Checking exceptions
2025-05-03 22:39:06,037:INFO:Importing libraries
2025-05-03 22:39:06,037:INFO:Copying training dataset
2025-05-03 22:39:06,079:INFO:Defining folds
2025-05-03 22:39:06,079:INFO:Declaring metric variables
2025-05-03 22:39:06,079:INFO:Importing untrained model
2025-05-03 22:39:06,079:INFO:Declaring custom model
2025-05-03 22:39:06,079:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:39:06,079:INFO:Starting cross validation
2025-05-03 22:39:06,079:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:39:06,404:INFO:Calculating mean and std
2025-05-03 22:39:06,404:INFO:Creating metrics dataframe
2025-05-03 22:39:06,404:INFO:Finalizing model
2025-05-03 22:39:06,527:INFO:Uploading results into container
2025-05-03 22:39:06,530:INFO:Uploading model into container now
2025-05-03 22:39:06,530:INFO:_master_model_container: 13
2025-05-03 22:39:06,530:INFO:_display_container: 8
2025-05-03 22:39:06,530:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:39:06,530:INFO:create_model() successfully completed......................................
2025-05-03 22:39:06,671:INFO:SubProcess create_model() end ==================================
2025-05-03 22:39:06,671:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 22:39:06,671:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 22:39:06,679:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 22:39:06,679:INFO:choose_better completed
2025-05-03 22:39:06,687:INFO:_master_model_container: 13
2025-05-03 22:39:06,687:INFO:_display_container: 7
2025-05-03 22:39:06,687:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:39:06,687:INFO:tune_model() successfully completed......................................
2025-05-03 22:39:06,843:INFO:Initializing tune_model()
2025-05-03 22:39:06,843:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:39:06,843:INFO:Checking exceptions
2025-05-03 22:39:06,843:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:39:06,869:INFO:Copying training dataset
2025-05-03 22:39:06,891:INFO:Checking base model
2025-05-03 22:39:06,891:INFO:Base model : Random Forest Classifier
2025-05-03 22:39:06,891:INFO:Declaring metric variables
2025-05-03 22:39:06,891:INFO:Defining Hyperparameters
2025-05-03 22:39:07,024:INFO:Tuning with n_jobs=-1
2025-05-03 22:39:07,037:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:42:51,723:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 22:42:51,723:INFO:Hyperparameter search completed
2025-05-03 22:42:51,723:INFO:SubProcess create_model() called ==================================
2025-05-03 22:42:51,723:INFO:Initializing create_model()
2025-05-03 22:42:51,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001B7EFE95110>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 22:42:51,723:INFO:Checking exceptions
2025-05-03 22:42:51,723:INFO:Importing libraries
2025-05-03 22:42:51,731:INFO:Copying training dataset
2025-05-03 22:42:51,749:INFO:Defining folds
2025-05-03 22:42:51,749:INFO:Declaring metric variables
2025-05-03 22:42:51,756:INFO:Importing untrained model
2025-05-03 22:42:51,756:INFO:Declaring custom model
2025-05-03 22:42:51,759:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:42:51,765:INFO:Starting cross validation
2025-05-03 22:42:51,768:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:42:55,851:INFO:Calculating mean and std
2025-05-03 22:42:55,851:INFO:Creating metrics dataframe
2025-05-03 22:42:55,856:INFO:Finalizing model
2025-05-03 22:42:58,054:INFO:Uploading results into container
2025-05-03 22:42:58,056:INFO:Uploading model into container now
2025-05-03 22:42:58,056:INFO:_master_model_container: 14
2025-05-03 22:42:58,056:INFO:_display_container: 8
2025-05-03 22:42:58,058:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:42:58,058:INFO:create_model() successfully completed......................................
2025-05-03 22:42:58,224:INFO:SubProcess create_model() end ==================================
2025-05-03 22:42:58,224:INFO:choose_better activated
2025-05-03 22:42:58,226:INFO:SubProcess create_model() called ==================================
2025-05-03 22:42:58,226:INFO:Initializing create_model()
2025-05-03 22:42:58,226:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:42:58,226:INFO:Checking exceptions
2025-05-03 22:42:58,232:INFO:Importing libraries
2025-05-03 22:42:58,232:INFO:Copying training dataset
2025-05-03 22:42:58,249:INFO:Defining folds
2025-05-03 22:42:58,249:INFO:Declaring metric variables
2025-05-03 22:42:58,249:INFO:Importing untrained model
2025-05-03 22:42:58,249:INFO:Declaring custom model
2025-05-03 22:42:58,249:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:42:58,249:INFO:Starting cross validation
2025-05-03 22:42:58,249:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:42:59,508:INFO:Calculating mean and std
2025-05-03 22:42:59,508:INFO:Creating metrics dataframe
2025-05-03 22:42:59,510:INFO:Finalizing model
2025-05-03 22:43:00,109:INFO:Uploading results into container
2025-05-03 22:43:00,109:INFO:Uploading model into container now
2025-05-03 22:43:00,109:INFO:_master_model_container: 15
2025-05-03 22:43:00,109:INFO:_display_container: 9
2025-05-03 22:43:00,109:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:43:00,109:INFO:create_model() successfully completed......................................
2025-05-03 22:43:00,244:INFO:SubProcess create_model() end ==================================
2025-05-03 22:43:00,244:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 22:43:00,244:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 22:43:00,244:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 22:43:00,244:INFO:choose_better completed
2025-05-03 22:43:00,258:INFO:_master_model_container: 15
2025-05-03 22:43:00,258:INFO:_display_container: 8
2025-05-03 22:43:00,258:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:43:00,261:INFO:tune_model() successfully completed......................................
2025-05-03 22:43:00,442:INFO:Initializing create_model()
2025-05-03 22:43:00,447:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 22:43:00,447:INFO:Checking exceptions
2025-05-03 22:43:00,463:INFO:Importing libraries
2025-05-03 22:43:00,463:INFO:Copying training dataset
2025-05-03 22:43:00,488:INFO:Defining folds
2025-05-03 22:43:00,488:INFO:Declaring metric variables
2025-05-03 22:43:00,492:INFO:Importing untrained model
2025-05-03 22:43:00,492:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:43:00,503:INFO:Starting cross validation
2025-05-03 22:43:00,503:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:43:00,878:INFO:Calculating mean and std
2025-05-03 22:43:00,878:INFO:Creating metrics dataframe
2025-05-03 22:43:00,884:INFO:Finalizing model
2025-05-03 22:43:00,896:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:43:00,896:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:43:00,896:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:43:00,914:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:43:00,914:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:43:00,914:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:43:00,914:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:43:00,918:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002488 seconds.
2025-05-03 22:43:00,918:INFO:You can set `force_col_wise=true` to remove the overhead.
2025-05-03 22:43:00,918:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:43:00,918:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:43:00,918:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:43:00,918:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:43:00,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:00,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,011:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,017:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,036:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,042:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,042:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,044:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,048:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:43:01,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:43:01,082:INFO:Uploading results into container
2025-05-03 22:43:01,084:INFO:Uploading model into container now
2025-05-03 22:43:01,094:INFO:_master_model_container: 16
2025-05-03 22:43:01,094:INFO:_display_container: 9
2025-05-03 22:43:01,094:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:43:01,094:INFO:create_model() successfully completed......................................
2025-05-03 22:43:01,233:INFO:Initializing create_model()
2025-05-03 22:43:01,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 22:43:01,233:INFO:Checking exceptions
2025-05-03 22:43:01,266:INFO:Importing libraries
2025-05-03 22:43:01,266:INFO:Copying training dataset
2025-05-03 22:43:01,292:INFO:Defining folds
2025-05-03 22:43:01,292:INFO:Declaring metric variables
2025-05-03 22:43:01,300:INFO:Importing untrained model
2025-05-03 22:43:01,303:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:43:01,309:INFO:Starting cross validation
2025-05-03 22:43:01,309:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:43:01,801:INFO:Calculating mean and std
2025-05-03 22:43:01,801:INFO:Creating metrics dataframe
2025-05-03 22:43:01,805:INFO:Finalizing model
2025-05-03 22:43:01,980:INFO:Uploading results into container
2025-05-03 22:43:01,980:INFO:Uploading model into container now
2025-05-03 22:43:01,990:INFO:_master_model_container: 17
2025-05-03 22:43:01,990:INFO:_display_container: 10
2025-05-03 22:43:01,992:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:43:01,992:INFO:create_model() successfully completed......................................
2025-05-03 22:43:02,133:INFO:Initializing create_model()
2025-05-03 22:43:02,133:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 22:43:02,133:INFO:Checking exceptions
2025-05-03 22:43:02,150:INFO:Importing libraries
2025-05-03 22:43:02,150:INFO:Copying training dataset
2025-05-03 22:43:02,176:INFO:Defining folds
2025-05-03 22:43:02,176:INFO:Declaring metric variables
2025-05-03 22:43:02,176:INFO:Importing untrained model
2025-05-03 22:43:02,183:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:43:02,183:INFO:Starting cross validation
2025-05-03 22:43:02,183:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:43:06,228:INFO:Calculating mean and std
2025-05-03 22:43:06,228:INFO:Creating metrics dataframe
2025-05-03 22:43:06,232:INFO:Finalizing model
2025-05-03 22:43:08,439:INFO:Uploading results into container
2025-05-03 22:43:08,439:INFO:Uploading model into container now
2025-05-03 22:43:08,447:INFO:_master_model_container: 18
2025-05-03 22:43:08,447:INFO:_display_container: 11
2025-05-03 22:43:08,456:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:43:08,456:INFO:create_model() successfully completed......................................
2025-05-03 22:43:08,646:INFO:Initializing plot_model()
2025-05-03 22:43:08,646:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:08,646:INFO:Checking exceptions
2025-05-03 22:43:08,660:INFO:Preloading libraries
2025-05-03 22:43:08,676:INFO:Copying training dataset
2025-05-03 22:43:08,676:INFO:Plot type: auc
2025-05-03 22:43:08,797:INFO:Fitting Model
2025-05-03 22:43:08,797:INFO:Scoring test/hold-out set
2025-05-03 22:43:08,799:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:43:08,799:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:43:08,799:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:43:08,813:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:43:08,813:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:43:08,813:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:43:09,081:INFO:Visual Rendered Successfully
2025-05-03 22:43:09,221:INFO:plot_model() successfully completed......................................
2025-05-03 22:43:09,251:INFO:Initializing plot_model()
2025-05-03 22:43:09,251:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:09,251:INFO:Checking exceptions
2025-05-03 22:43:09,264:INFO:Preloading libraries
2025-05-03 22:43:09,274:INFO:Copying training dataset
2025-05-03 22:43:09,274:INFO:Plot type: auc
2025-05-03 22:43:09,392:INFO:Fitting Model
2025-05-03 22:43:09,392:INFO:Scoring test/hold-out set
2025-05-03 22:43:09,645:INFO:Visual Rendered Successfully
2025-05-03 22:43:09,775:INFO:plot_model() successfully completed......................................
2025-05-03 22:43:09,791:INFO:Initializing plot_model()
2025-05-03 22:43:09,791:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:09,791:INFO:Checking exceptions
2025-05-03 22:43:09,858:INFO:Preloading libraries
2025-05-03 22:43:09,911:INFO:Copying training dataset
2025-05-03 22:43:09,911:INFO:Plot type: auc
2025-05-03 22:43:10,023:INFO:Fitting Model
2025-05-03 22:43:10,035:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2025-05-03 22:43:10,038:INFO:Scoring test/hold-out set
2025-05-03 22:43:10,426:INFO:Visual Rendered Successfully
2025-05-03 22:43:10,567:INFO:plot_model() successfully completed......................................
2025-05-03 22:43:10,591:INFO:Initializing save_model()
2025-05-03 22:43:10,591:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:43:10,591:INFO:Adding model into prep_pipe
2025-05-03 22:43:10,606:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 22:43:10,610:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 22:43:10,610:INFO:save_model() successfully completed......................................
2025-05-03 22:43:10,754:INFO:Initializing save_model()
2025-05-03 22:43:10,754:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:43:10,754:INFO:Adding model into prep_pipe
2025-05-03 22:43:10,764:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 22:43:10,770:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 22:43:10,770:INFO:save_model() successfully completed......................................
2025-05-03 22:43:10,917:INFO:Initializing save_model()
2025-05-03 22:43:10,917:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:43:10,917:INFO:Adding model into prep_pipe
2025-05-03 22:43:11,041:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 22:43:11,041:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 22:43:11,041:INFO:save_model() successfully completed......................................
2025-05-03 22:43:11,222:INFO:Initializing interpret_model()
2025-05-03 22:43:11,222:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 22:43:11,222:INFO:Checking exceptions
2025-05-03 22:43:11,222:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:43:11,248:INFO:plot type: summary
2025-05-03 22:43:11,248:INFO:Creating TreeExplainer
2025-05-03 22:43:11,288:INFO:Compiling shap values
2025-05-03 22:43:12,004:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')

2025-05-03 22:43:12,231:INFO:Visual Rendered Successfully
2025-05-03 22:43:12,234:INFO:interpret_model() successfully completed......................................
2025-05-03 22:43:12,403:INFO:Initializing interpret_model()
2025-05-03 22:43:12,403:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 22:43:12,403:INFO:Checking exceptions
2025-05-03 22:43:12,403:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:43:12,435:INFO:plot type: summary
2025-05-03 22:43:12,435:INFO:Creating TreeExplainer
2025-05-03 22:43:12,490:INFO:Compiling shap values
2025-05-03 22:43:13,281:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')

2025-05-03 22:43:14,806:INFO:Visual Rendered Successfully
2025-05-03 22:43:14,806:INFO:interpret_model() successfully completed......................................
2025-05-03 22:43:14,957:INFO:Initializing interpret_model()
2025-05-03 22:43:14,957:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 22:43:14,957:INFO:Checking exceptions
2025-05-03 22:43:14,957:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:43:14,976:INFO:plot type: summary
2025-05-03 22:43:14,976:INFO:Creating TreeExplainer
2025-05-03 22:43:15,012:INFO:Compiling shap values
2025-05-03 22:43:16,870:INFO:Visual Rendered Successfully
2025-05-03 22:43:16,878:INFO:interpret_model() successfully completed......................................
2025-05-03 22:43:17,020:INFO:Initializing interpret_model()
2025-05-03 22:43:17,020:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 22:43:17,020:INFO:Checking exceptions
2025-05-03 22:43:17,020:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:43:17,044:INFO:plot type: summary
2025-05-03 22:43:17,047:INFO:Creating TreeExplainer
2025-05-03 22:43:17,087:INFO:Compiling shap values
2025-05-03 22:43:20,200:INFO:Visual Rendered Successfully
2025-05-03 22:43:20,200:INFO:interpret_model() successfully completed......................................
2025-05-03 22:43:20,364:INFO:Initializing plot_model()
2025-05-03 22:43:20,371:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:20,371:INFO:Checking exceptions
2025-05-03 22:43:20,425:INFO:Preloading libraries
2025-05-03 22:43:20,475:INFO:Copying training dataset
2025-05-03 22:43:20,475:INFO:Plot type: feature
2025-05-03 22:43:20,475:WARNING:No coef_ found. Trying feature_importances_
2025-05-03 22:43:20,705:INFO:Visual Rendered Successfully
2025-05-03 22:43:20,835:INFO:plot_model() successfully completed......................................
2025-05-03 22:43:20,883:INFO:Initializing plot_model()
2025-05-03 22:43:20,883:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:20,883:INFO:Checking exceptions
2025-05-03 22:43:20,902:INFO:Preloading libraries
2025-05-03 22:43:21,028:INFO:Copying training dataset
2025-05-03 22:43:21,028:INFO:Plot type: auc
2025-05-03 22:43:21,127:INFO:Fitting Model
2025-05-03 22:43:53,871:INFO:Initializing plot_model()
2025-05-03 22:43:53,871:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=pr, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:43:53,871:INFO:Checking exceptions
2025-05-03 22:43:53,887:INFO:Preloading libraries
2025-05-03 22:43:54,007:INFO:Copying training dataset
2025-05-03 22:43:54,007:INFO:Plot type: pr
2025-05-03 22:43:54,131:INFO:Fitting Model
2025-05-03 22:48:08,421:INFO:Initializing plot_model()
2025-05-03 22:48:08,421:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:48:08,421:INFO:Checking exceptions
2025-05-03 22:48:08,437:INFO:Preloading libraries
2025-05-03 22:48:08,525:INFO:Copying training dataset
2025-05-03 22:48:08,525:INFO:Plot type: auc
2025-05-03 22:48:08,647:INFO:Fitting Model
2025-05-03 22:48:42,008:INFO:Initializing plot_model()
2025-05-03 22:48:42,010:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B7F743B850>, estimator=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:48:42,010:INFO:Checking exceptions
2025-05-03 22:48:42,026:INFO:Preloading libraries
2025-05-03 22:48:42,103:INFO:Copying training dataset
2025-05-03 22:48:42,103:INFO:Plot type: auc
2025-05-03 22:48:42,222:INFO:Fitting Model
2025-05-03 22:50:29,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:50:29,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:50:29,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:50:29,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-03 22:50:34,024:INFO:PyCaret ClassificationExperiment
2025-05-03 22:50:34,024:INFO:Logging name: clf-default-name
2025-05-03 22:50:34,024:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-03 22:50:34,024:INFO:version 3.3.2
2025-05-03 22:50:34,029:INFO:Initializing setup()
2025-05-03 22:50:34,029:INFO:self.USI: 8c1c
2025-05-03 22:50:34,029:INFO:self._variable_keys: {'fold_shuffle_param', 'y_train', 'fix_imbalance', 'target_param', 'exp_name_log', 'logging_param', 'gpu_n_jobs_param', 'memory', 'exp_id', 'X_train', 'gpu_param', 'fold_groups_param', 'fold_generator', 'y', 'is_multiclass', 'seed', 'idx', 'n_jobs_param', 'USI', 'X', 'html_param', '_ml_usecase', 'data', 'pipeline', '_available_plots', 'X_test', 'y_test', 'log_plots_param'}
2025-05-03 22:50:34,029:INFO:Checking environment
2025-05-03 22:50:34,029:INFO:python_version: 3.11.11
2025-05-03 22:50:34,029:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2025-05-03 22:50:34,029:INFO:machine: AMD64
2025-05-03 22:50:34,029:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-03 22:50:34,034:INFO:Memory: svmem(total=16965230592, available=4292808704, percent=74.7, used=12672421888, free=4292808704)
2025-05-03 22:50:34,034:INFO:Physical Core: 4
2025-05-03 22:50:34,034:INFO:Logical Core: 8
2025-05-03 22:50:34,034:INFO:Checking libraries
2025-05-03 22:50:34,034:INFO:System:
2025-05-03 22:50:34,034:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2025-05-03 22:50:34,034:INFO:executable: c:\Users\isma_\miniconda3\envs\proyectoprediccion\python.exe
2025-05-03 22:50:34,034:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-03 22:50:34,034:INFO:PyCaret required dependencies:
2025-05-03 22:50:34,036:INFO:                 pip: 25.0
2025-05-03 22:50:34,036:INFO:          setuptools: 75.8.0
2025-05-03 22:50:34,036:INFO:             pycaret: 3.3.2
2025-05-03 22:50:34,036:INFO:             IPython: 8.32.0
2025-05-03 22:50:34,036:INFO:          ipywidgets: 8.1.6
2025-05-03 22:50:34,036:INFO:                tqdm: 4.67.1
2025-05-03 22:50:34,036:INFO:               numpy: 1.26.4
2025-05-03 22:50:34,036:INFO:              pandas: 2.1.4
2025-05-03 22:50:34,036:INFO:              jinja2: 3.1.6
2025-05-03 22:50:34,036:INFO:               scipy: 1.11.4
2025-05-03 22:50:34,036:INFO:              joblib: 1.3.2
2025-05-03 22:50:34,036:INFO:             sklearn: 1.4.2
2025-05-03 22:50:34,036:INFO:                pyod: 2.0.5
2025-05-03 22:50:34,036:INFO:            imblearn: 0.13.0
2025-05-03 22:50:34,036:INFO:   category_encoders: 2.7.0
2025-05-03 22:50:34,036:INFO:            lightgbm: 4.6.0
2025-05-03 22:50:34,036:INFO:               numba: 0.61.0
2025-05-03 22:50:34,036:INFO:            requests: 2.32.3
2025-05-03 22:50:34,036:INFO:          matplotlib: 3.7.5
2025-05-03 22:50:34,036:INFO:          scikitplot: 0.3.7
2025-05-03 22:50:34,036:INFO:         yellowbrick: 1.5
2025-05-03 22:50:34,036:INFO:              plotly: 5.24.1
2025-05-03 22:50:34,036:INFO:    plotly-resampler: Not installed
2025-05-03 22:50:34,036:INFO:             kaleido: 0.2.1
2025-05-03 22:50:34,036:INFO:           schemdraw: 0.15
2025-05-03 22:50:34,038:INFO:         statsmodels: 0.14.4
2025-05-03 22:50:34,038:INFO:              sktime: 0.26.0
2025-05-03 22:50:34,038:INFO:               tbats: 1.1.3
2025-05-03 22:50:34,038:INFO:            pmdarima: 2.0.4
2025-05-03 22:50:34,038:INFO:              psutil: 6.1.1
2025-05-03 22:50:34,038:INFO:          markupsafe: 3.0.2
2025-05-03 22:50:34,038:INFO:             pickle5: Not installed
2025-05-03 22:50:34,038:INFO:         cloudpickle: 3.1.1
2025-05-03 22:50:34,038:INFO:         deprecation: 2.1.0
2025-05-03 22:50:34,038:INFO:              xxhash: 3.5.0
2025-05-03 22:50:34,038:INFO:           wurlitzer: Not installed
2025-05-03 22:50:34,038:INFO:PyCaret optional dependencies:
2025-05-03 22:50:34,478:INFO:                shap: 0.46.0
2025-05-03 22:50:34,478:INFO:           interpret: 0.6.9
2025-05-03 22:50:34,478:INFO:                umap: Not installed
2025-05-03 22:50:34,478:INFO:     ydata_profiling: Not installed
2025-05-03 22:50:34,478:INFO:  explainerdashboard: Not installed
2025-05-03 22:50:34,478:INFO:             autoviz: Not installed
2025-05-03 22:50:34,478:INFO:           fairlearn: Not installed
2025-05-03 22:50:34,478:INFO:          deepchecks: Not installed
2025-05-03 22:50:34,478:INFO:             xgboost: 3.0.0
2025-05-03 22:50:34,478:INFO:            catboost: Not installed
2025-05-03 22:50:34,478:INFO:              kmodes: Not installed
2025-05-03 22:50:34,478:INFO:             mlxtend: Not installed
2025-05-03 22:50:34,478:INFO:       statsforecast: Not installed
2025-05-03 22:50:34,478:INFO:        tune_sklearn: Not installed
2025-05-03 22:50:34,478:INFO:                 ray: Not installed
2025-05-03 22:50:34,478:INFO:            hyperopt: 0.2.7
2025-05-03 22:50:34,480:INFO:              optuna: Not installed
2025-05-03 22:50:34,480:INFO:               skopt: 0.10.2
2025-05-03 22:50:34,480:INFO:              mlflow: 2.22.0
2025-05-03 22:50:34,480:INFO:              gradio: Not installed
2025-05-03 22:50:34,480:INFO:             fastapi: 0.115.12
2025-05-03 22:50:34,480:INFO:             uvicorn: 0.34.2
2025-05-03 22:50:34,480:INFO:              m2cgen: Not installed
2025-05-03 22:50:34,480:INFO:           evidently: Not installed
2025-05-03 22:50:34,480:INFO:               fugue: Not installed
2025-05-03 22:50:34,480:INFO:           streamlit: Not installed
2025-05-03 22:50:34,480:INFO:             prophet: Not installed
2025-05-03 22:50:34,480:INFO:None
2025-05-03 22:50:34,480:INFO:Set up data.
2025-05-03 22:50:34,495:INFO:Set up folding strategy.
2025-05-03 22:50:34,495:INFO:Set up train/test split.
2025-05-03 22:50:34,517:INFO:Set up index.
2025-05-03 22:50:34,517:INFO:Assigning column types.
2025-05-03 22:50:34,525:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-03 22:50:34,574:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,582:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,625:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:34,625:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:34,670:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,670:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,699:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:34,699:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:34,699:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-03 22:50:34,748:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,773:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:34,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:34,832:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-03 22:50:34,870:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:34,870:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:34,870:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-03 22:50:34,945:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:34,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:35,028:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:35,028:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:35,028:INFO:Set up column name cleaning.
2025-05-03 22:50:35,052:INFO:Finished creating preprocessing pipeline.
2025-05-03 22:50:35,059:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-03 22:50:35,059:INFO:Creating final display dataframe.
2025-05-03 22:50:35,155:INFO:Setup _display_container:                    Description        Value
0                   Session id           42
1                       Target       income
2                  Target type       Binary
3          Original data shape  (39073, 15)
4       Transformed data shape  (39073, 15)
5  Transformed train set shape  (27351, 15)
6   Transformed test set shape  (11722, 15)
7             Numeric features           14
2025-05-03 22:50:35,230:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:35,232:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:35,307:INFO:Soft dependency imported: xgboost: 3.0.0
2025-05-03 22:50:35,307:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-03 22:50:35,307:INFO:setup() successfully completed in 1.28s...............
2025-05-03 22:50:35,329:INFO:Initializing compare_models()
2025-05-03 22:50:35,329:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, include=['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], exclude=None, fold=None, round=4, cross_validation=True, sort=F1, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, 'include': ['lr', 'rf', 'xgboost', 'lightgbm', 'et', 'ridge'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'F1', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-05-03 22:50:35,329:INFO:Checking exceptions
2025-05-03 22:50:35,348:INFO:Preparing display monitor
2025-05-03 22:50:35,411:INFO:Initializing Logistic Regression
2025-05-03 22:50:35,411:INFO:Total runtime is 0.0 minutes
2025-05-03 22:50:35,415:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:35,415:INFO:Initializing create_model()
2025-05-03 22:50:35,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:35,415:INFO:Checking exceptions
2025-05-03 22:50:35,415:INFO:Importing libraries
2025-05-03 22:50:35,415:INFO:Copying training dataset
2025-05-03 22:50:35,430:INFO:Defining folds
2025-05-03 22:50:35,430:INFO:Declaring metric variables
2025-05-03 22:50:35,438:INFO:Importing untrained model
2025-05-03 22:50:35,441:INFO:Logistic Regression Imported successfully
2025-05-03 22:50:35,446:INFO:Starting cross validation
2025-05-03 22:50:35,446:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:39,377:INFO:Calculating mean and std
2025-05-03 22:50:39,379:INFO:Creating metrics dataframe
2025-05-03 22:50:39,381:INFO:Uploading results into container
2025-05-03 22:50:39,381:INFO:Uploading model into container now
2025-05-03 22:50:39,381:INFO:_master_model_container: 1
2025-05-03 22:50:39,383:INFO:_display_container: 2
2025-05-03 22:50:39,383:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-03 22:50:39,383:INFO:create_model() successfully completed......................................
2025-05-03 22:50:39,523:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:39,523:INFO:Creating metrics dataframe
2025-05-03 22:50:39,530:INFO:Initializing Random Forest Classifier
2025-05-03 22:50:39,530:INFO:Total runtime is 0.06865891218185424 minutes
2025-05-03 22:50:39,535:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:39,535:INFO:Initializing create_model()
2025-05-03 22:50:39,535:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:39,535:INFO:Checking exceptions
2025-05-03 22:50:39,535:INFO:Importing libraries
2025-05-03 22:50:39,535:INFO:Copying training dataset
2025-05-03 22:50:39,555:INFO:Defining folds
2025-05-03 22:50:39,555:INFO:Declaring metric variables
2025-05-03 22:50:39,560:INFO:Importing untrained model
2025-05-03 22:50:39,567:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:50:39,596:INFO:Starting cross validation
2025-05-03 22:50:39,598:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:43,975:INFO:Calculating mean and std
2025-05-03 22:50:43,975:INFO:Creating metrics dataframe
2025-05-03 22:50:43,977:INFO:Uploading results into container
2025-05-03 22:50:43,977:INFO:Uploading model into container now
2025-05-03 22:50:43,977:INFO:_master_model_container: 2
2025-05-03 22:50:43,977:INFO:_display_container: 2
2025-05-03 22:50:43,977:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:50:43,980:INFO:create_model() successfully completed......................................
2025-05-03 22:50:44,112:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:44,112:INFO:Creating metrics dataframe
2025-05-03 22:50:44,116:INFO:Initializing Extreme Gradient Boosting
2025-05-03 22:50:44,116:INFO:Total runtime is 0.14508912563323972 minutes
2025-05-03 22:50:44,127:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:44,127:INFO:Initializing create_model()
2025-05-03 22:50:44,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=xgboost, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:44,127:INFO:Checking exceptions
2025-05-03 22:50:44,127:INFO:Importing libraries
2025-05-03 22:50:44,127:INFO:Copying training dataset
2025-05-03 22:50:44,140:INFO:Defining folds
2025-05-03 22:50:44,140:INFO:Declaring metric variables
2025-05-03 22:50:44,152:INFO:Importing untrained model
2025-05-03 22:50:44,157:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:50:44,166:INFO:Starting cross validation
2025-05-03 22:50:44,166:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:47,509:INFO:Calculating mean and std
2025-05-03 22:50:47,511:INFO:Creating metrics dataframe
2025-05-03 22:50:47,513:INFO:Uploading results into container
2025-05-03 22:50:47,513:INFO:Uploading model into container now
2025-05-03 22:50:47,513:INFO:_master_model_container: 3
2025-05-03 22:50:47,513:INFO:_display_container: 2
2025-05-03 22:50:47,513:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:50:47,515:INFO:create_model() successfully completed......................................
2025-05-03 22:50:47,654:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:47,654:INFO:Creating metrics dataframe
2025-05-03 22:50:47,662:INFO:Initializing Light Gradient Boosting Machine
2025-05-03 22:50:47,662:INFO:Total runtime is 0.20418564081192014 minutes
2025-05-03 22:50:47,669:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:47,669:INFO:Initializing create_model()
2025-05-03 22:50:47,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:47,671:INFO:Checking exceptions
2025-05-03 22:50:47,671:INFO:Importing libraries
2025-05-03 22:50:47,671:INFO:Copying training dataset
2025-05-03 22:50:47,686:INFO:Defining folds
2025-05-03 22:50:47,686:INFO:Declaring metric variables
2025-05-03 22:50:47,694:INFO:Importing untrained model
2025-05-03 22:50:47,694:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:50:47,705:INFO:Starting cross validation
2025-05-03 22:50:47,705:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:48,170:INFO:Calculating mean and std
2025-05-03 22:50:48,172:INFO:Creating metrics dataframe
2025-05-03 22:50:48,174:INFO:Uploading results into container
2025-05-03 22:50:48,174:INFO:Uploading model into container now
2025-05-03 22:50:48,175:INFO:_master_model_container: 4
2025-05-03 22:50:48,175:INFO:_display_container: 2
2025-05-03 22:50:48,175:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:50:48,177:INFO:create_model() successfully completed......................................
2025-05-03 22:50:48,322:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:48,322:INFO:Creating metrics dataframe
2025-05-03 22:50:48,338:INFO:Initializing Extra Trees Classifier
2025-05-03 22:50:48,338:INFO:Total runtime is 0.21546261707941688 minutes
2025-05-03 22:50:48,350:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:48,350:INFO:Initializing create_model()
2025-05-03 22:50:48,350:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=et, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:48,350:INFO:Checking exceptions
2025-05-03 22:50:48,350:INFO:Importing libraries
2025-05-03 22:50:48,350:INFO:Copying training dataset
2025-05-03 22:50:48,371:INFO:Defining folds
2025-05-03 22:50:48,371:INFO:Declaring metric variables
2025-05-03 22:50:48,381:INFO:Importing untrained model
2025-05-03 22:50:48,386:INFO:Extra Trees Classifier Imported successfully
2025-05-03 22:50:48,389:INFO:Starting cross validation
2025-05-03 22:50:48,389:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:49,500:INFO:Calculating mean and std
2025-05-03 22:50:49,500:INFO:Creating metrics dataframe
2025-05-03 22:50:49,500:INFO:Uploading results into container
2025-05-03 22:50:49,500:INFO:Uploading model into container now
2025-05-03 22:50:49,506:INFO:_master_model_container: 5
2025-05-03 22:50:49,506:INFO:_display_container: 2
2025-05-03 22:50:49,506:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-05-03 22:50:49,506:INFO:create_model() successfully completed......................................
2025-05-03 22:50:49,661:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:49,661:INFO:Creating metrics dataframe
2025-05-03 22:50:49,672:INFO:Initializing Ridge Classifier
2025-05-03 22:50:49,672:INFO:Total runtime is 0.23769191503524778 minutes
2025-05-03 22:50:49,678:INFO:SubProcess create_model() called ==================================
2025-05-03 22:50:49,678:INFO:Initializing create_model()
2025-05-03 22:50:49,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=ridge, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4C790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:49,678:INFO:Checking exceptions
2025-05-03 22:50:49,678:INFO:Importing libraries
2025-05-03 22:50:49,678:INFO:Copying training dataset
2025-05-03 22:50:49,707:INFO:Defining folds
2025-05-03 22:50:49,707:INFO:Declaring metric variables
2025-05-03 22:50:49,713:INFO:Importing untrained model
2025-05-03 22:50:49,715:INFO:Ridge Classifier Imported successfully
2025-05-03 22:50:49,724:INFO:Starting cross validation
2025-05-03 22:50:49,727:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:49,811:INFO:Calculating mean and std
2025-05-03 22:50:49,811:INFO:Creating metrics dataframe
2025-05-03 22:50:49,813:INFO:Uploading results into container
2025-05-03 22:50:49,813:INFO:Uploading model into container now
2025-05-03 22:50:49,813:INFO:_master_model_container: 6
2025-05-03 22:50:49,815:INFO:_display_container: 2
2025-05-03 22:50:49,815:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-05-03 22:50:49,815:INFO:create_model() successfully completed......................................
2025-05-03 22:50:49,949:INFO:SubProcess create_model() end ==================================
2025-05-03 22:50:49,949:INFO:Creating metrics dataframe
2025-05-03 22:50:49,960:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-05-03 22:50:49,977:INFO:Initializing create_model()
2025-05-03 22:50:49,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:49,977:INFO:Checking exceptions
2025-05-03 22:50:49,981:INFO:Importing libraries
2025-05-03 22:50:49,981:INFO:Copying training dataset
2025-05-03 22:50:49,999:INFO:Defining folds
2025-05-03 22:50:49,999:INFO:Declaring metric variables
2025-05-03 22:50:49,999:INFO:Importing untrained model
2025-05-03 22:50:49,999:INFO:Declaring custom model
2025-05-03 22:50:49,999:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:50:49,999:INFO:Cross validation set to False
2025-05-03 22:50:49,999:INFO:Fitting Model
2025-05-03 22:50:50,052:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:50:50,056:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001181 seconds.
2025-05-03 22:50:50,056:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:50:50,056:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:50:50,056:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:50:50,056:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:50:50,056:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:50:50,058:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:50:50,227:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:50:50,233:INFO:create_model() successfully completed......................................
2025-05-03 22:50:50,408:INFO:_master_model_container: 6
2025-05-03 22:50:50,408:INFO:_display_container: 2
2025-05-03 22:50:50,408:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:50:50,408:INFO:compare_models() successfully completed......................................
2025-05-03 22:50:50,440:INFO:Initializing create_model()
2025-05-03 22:50:50,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:50,443:INFO:Checking exceptions
2025-05-03 22:50:50,474:INFO:Importing libraries
2025-05-03 22:50:50,474:INFO:Copying training dataset
2025-05-03 22:50:50,497:INFO:Defining folds
2025-05-03 22:50:50,497:INFO:Declaring metric variables
2025-05-03 22:50:50,505:INFO:Importing untrained model
2025-05-03 22:50:50,514:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:50:50,521:INFO:Starting cross validation
2025-05-03 22:50:50,521:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:51,063:INFO:Calculating mean and std
2025-05-03 22:50:51,063:INFO:Creating metrics dataframe
2025-05-03 22:50:51,069:INFO:Finalizing model
2025-05-03 22:50:51,097:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:50:51,101:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.
2025-05-03 22:50:51,101:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:50:51,101:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:50:51,101:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:50:51,101:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:50:51,101:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:50:51,101:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:50:51,224:INFO:Uploading results into container
2025-05-03 22:50:51,224:INFO:Uploading model into container now
2025-05-03 22:50:51,234:INFO:_master_model_container: 7
2025-05-03 22:50:51,234:INFO:_display_container: 3
2025-05-03 22:50:51,234:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:50:51,234:INFO:create_model() successfully completed......................................
2025-05-03 22:50:51,406:INFO:Initializing create_model()
2025-05-03 22:50:51,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:51,406:INFO:Checking exceptions
2025-05-03 22:50:51,430:INFO:Importing libraries
2025-05-03 22:50:51,430:INFO:Copying training dataset
2025-05-03 22:50:51,447:INFO:Defining folds
2025-05-03 22:50:51,447:INFO:Declaring metric variables
2025-05-03 22:50:51,455:INFO:Importing untrained model
2025-05-03 22:50:51,455:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:50:51,463:INFO:Starting cross validation
2025-05-03 22:50:51,463:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:51,858:INFO:Calculating mean and std
2025-05-03 22:50:51,858:INFO:Creating metrics dataframe
2025-05-03 22:50:51,864:INFO:Finalizing model
2025-05-03 22:50:52,009:INFO:Uploading results into container
2025-05-03 22:50:52,009:INFO:Uploading model into container now
2025-05-03 22:50:52,018:INFO:_master_model_container: 8
2025-05-03 22:50:52,019:INFO:_display_container: 4
2025-05-03 22:50:52,019:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:50:52,019:INFO:create_model() successfully completed......................................
2025-05-03 22:50:52,183:INFO:Initializing create_model()
2025-05-03 22:50:52,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:50:52,183:INFO:Checking exceptions
2025-05-03 22:50:52,212:INFO:Importing libraries
2025-05-03 22:50:52,212:INFO:Copying training dataset
2025-05-03 22:50:52,249:INFO:Defining folds
2025-05-03 22:50:52,249:INFO:Declaring metric variables
2025-05-03 22:50:52,257:INFO:Importing untrained model
2025-05-03 22:50:52,265:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:50:52,277:INFO:Starting cross validation
2025-05-03 22:50:52,281:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:50:53,594:INFO:Calculating mean and std
2025-05-03 22:50:53,595:INFO:Creating metrics dataframe
2025-05-03 22:50:53,602:INFO:Finalizing model
2025-05-03 22:50:54,170:INFO:Uploading results into container
2025-05-03 22:50:54,170:INFO:Uploading model into container now
2025-05-03 22:50:54,176:INFO:_master_model_container: 9
2025-05-03 22:50:54,176:INFO:_display_container: 5
2025-05-03 22:50:54,176:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:50:54,176:INFO:create_model() successfully completed......................................
2025-05-03 22:50:54,333:INFO:Initializing tune_model()
2025-05-03 22:50:54,333:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:50:54,333:INFO:Checking exceptions
2025-05-03 22:50:54,335:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:50:54,356:INFO:Copying training dataset
2025-05-03 22:50:54,381:INFO:Checking base model
2025-05-03 22:50:54,381:INFO:Base model : Light Gradient Boosting Machine
2025-05-03 22:50:54,381:INFO:Declaring metric variables
2025-05-03 22:50:54,391:INFO:Defining Hyperparameters
2025-05-03 22:50:54,527:INFO:Tuning with n_jobs=-1
2025-05-03 22:50:54,538:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:52:54,325:INFO:best_params: OrderedDict([('actual_estimator__bagging_fraction', 0.4), ('actual_estimator__bagging_freq', 0), ('actual_estimator__feature_fraction', 0.6637577331805016), ('actual_estimator__learning_rate', 0.49999999999999994), ('actual_estimator__min_child_samples', 100), ('actual_estimator__min_split_gain', 0.0), ('actual_estimator__n_estimators', 197), ('actual_estimator__num_leaves', 256), ('actual_estimator__reg_alpha', 10.0), ('actual_estimator__reg_lambda', 10.0)])
2025-05-03 22:52:54,325:INFO:Hyperparameter search completed
2025-05-03 22:52:54,325:INFO:SubProcess create_model() called ==================================
2025-05-03 22:52:54,325:INFO:Initializing create_model()
2025-05-03 22:52:54,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940708650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016, 'learning_rate': 0.49999999999999994, 'min_child_samples': 100, 'min_split_gain': 0.0, 'n_estimators': 197, 'num_leaves': 256, 'reg_alpha': 10.0, 'reg_lambda': 10.0})
2025-05-03 22:52:54,334:INFO:Checking exceptions
2025-05-03 22:52:54,334:INFO:Importing libraries
2025-05-03 22:52:54,334:INFO:Copying training dataset
2025-05-03 22:52:54,350:INFO:Defining folds
2025-05-03 22:52:54,350:INFO:Declaring metric variables
2025-05-03 22:52:54,359:INFO:Importing untrained model
2025-05-03 22:52:54,359:INFO:Declaring custom model
2025-05-03 22:52:54,361:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:52:54,366:INFO:Starting cross validation
2025-05-03 22:52:54,366:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:52:54,786:INFO:Calculating mean and std
2025-05-03 22:52:54,788:INFO:Creating metrics dataframe
2025-05-03 22:52:54,795:INFO:Finalizing model
2025-05-03 22:52:54,806:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:52:54,808:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:52:54,808:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:52:54,823:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:52:54,823:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:52:54,825:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:52:54,825:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:52:54,827:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000979 seconds.
2025-05-03 22:52:54,827:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:52:54,827:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:52:54,827:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:52:54,827:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:52:54,827:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:52:54,827:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:52:54,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,935:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,949:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,970:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:52:54,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:52:54,988:INFO:Uploading results into container
2025-05-03 22:52:54,990:INFO:Uploading model into container now
2025-05-03 22:52:54,990:INFO:_master_model_container: 10
2025-05-03 22:52:54,990:INFO:_display_container: 6
2025-05-03 22:52:54,990:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:52:54,992:INFO:create_model() successfully completed......................................
2025-05-03 22:52:55,137:INFO:SubProcess create_model() end ==================================
2025-05-03 22:52:55,137:INFO:choose_better activated
2025-05-03 22:52:55,141:INFO:SubProcess create_model() called ==================================
2025-05-03 22:52:55,141:INFO:Initializing create_model()
2025-05-03 22:52:55,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:52:55,141:INFO:Checking exceptions
2025-05-03 22:52:55,141:INFO:Importing libraries
2025-05-03 22:52:55,141:INFO:Copying training dataset
2025-05-03 22:52:55,157:INFO:Defining folds
2025-05-03 22:52:55,157:INFO:Declaring metric variables
2025-05-03 22:52:55,157:INFO:Importing untrained model
2025-05-03 22:52:55,157:INFO:Declaring custom model
2025-05-03 22:52:55,165:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:52:55,165:INFO:Starting cross validation
2025-05-03 22:52:55,165:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:52:55,641:INFO:Calculating mean and std
2025-05-03 22:52:55,641:INFO:Creating metrics dataframe
2025-05-03 22:52:55,643:INFO:Finalizing model
2025-05-03 22:52:55,669:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:52:55,681:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.
2025-05-03 22:52:55,681:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:52:55,681:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:52:55,681:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:52:55,681:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:52:55,683:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:52:55,683:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:52:55,790:INFO:Uploading results into container
2025-05-03 22:52:55,793:INFO:Uploading model into container now
2025-05-03 22:52:55,793:INFO:_master_model_container: 11
2025-05-03 22:52:55,793:INFO:_display_container: 7
2025-05-03 22:52:55,795:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:52:55,795:INFO:create_model() successfully completed......................................
2025-05-03 22:52:55,932:INFO:SubProcess create_model() end ==================================
2025-05-03 22:52:55,940:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.6584
2025-05-03 22:52:55,940:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for F1 is 0.664
2025-05-03 22:52:55,940:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2025-05-03 22:52:55,940:INFO:choose_better completed
2025-05-03 22:52:55,959:INFO:_master_model_container: 11
2025-05-03 22:52:55,959:INFO:_display_container: 6
2025-05-03 22:52:55,959:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.49999999999999994, max_depth=-1,
               min_child_samples=100, min_child_weight=0.001,
               min_split_gain=0.0, n_estimators=197, n_jobs=-1, num_leaves=256,
               objective=None, random_state=42, reg_alpha=10.0, reg_lambda=10.0,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:52:55,959:INFO:tune_model() successfully completed......................................
2025-05-03 22:52:56,129:INFO:Initializing tune_model()
2025-05-03 22:52:56,131:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:52:56,131:INFO:Checking exceptions
2025-05-03 22:52:56,131:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:52:56,153:INFO:Copying training dataset
2025-05-03 22:52:56,169:INFO:Checking base model
2025-05-03 22:52:56,169:INFO:Base model : Extreme Gradient Boosting
2025-05-03 22:52:56,169:INFO:Declaring metric variables
2025-05-03 22:52:56,177:INFO:Defining Hyperparameters
2025-05-03 22:52:56,310:INFO:Tuning with n_jobs=-1
2025-05-03 22:52:56,315:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:54:50,053:INFO:best_params: OrderedDict([('actual_estimator__colsample_bytree', 0.7717015338451563), ('actual_estimator__learning_rate', 0.17502697429833836), ('actual_estimator__max_depth', 6), ('actual_estimator__min_child_weight', 4), ('actual_estimator__n_estimators', 104), ('actual_estimator__reg_alpha', 3.651573868008674e-10), ('actual_estimator__reg_lambda', 0.00018991349641985853), ('actual_estimator__scale_pos_weight', 2.2642531676225333), ('actual_estimator__subsample', 0.5085836727877318)])
2025-05-03 22:54:50,053:INFO:Hyperparameter search completed
2025-05-03 22:54:50,053:INFO:SubProcess create_model() called ==================================
2025-05-03 22:54:50,062:INFO:Initializing create_model()
2025-05-03 22:54:50,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000119406BCE50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'colsample_bytree': 0.7717015338451563, 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318})
2025-05-03 22:54:50,062:INFO:Checking exceptions
2025-05-03 22:54:50,062:INFO:Importing libraries
2025-05-03 22:54:50,062:INFO:Copying training dataset
2025-05-03 22:54:50,079:INFO:Defining folds
2025-05-03 22:54:50,079:INFO:Declaring metric variables
2025-05-03 22:54:50,089:INFO:Importing untrained model
2025-05-03 22:54:50,089:INFO:Declaring custom model
2025-05-03 22:54:50,095:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:54:50,095:INFO:Starting cross validation
2025-05-03 22:54:50,095:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:54:50,570:INFO:Calculating mean and std
2025-05-03 22:54:50,570:INFO:Creating metrics dataframe
2025-05-03 22:54:50,576:INFO:Finalizing model
2025-05-03 22:54:50,730:INFO:Uploading results into container
2025-05-03 22:54:50,730:INFO:Uploading model into container now
2025-05-03 22:54:50,732:INFO:_master_model_container: 12
2025-05-03 22:54:50,732:INFO:_display_container: 7
2025-05-03 22:54:50,732:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:54:50,732:INFO:create_model() successfully completed......................................
2025-05-03 22:54:50,870:INFO:SubProcess create_model() end ==================================
2025-05-03 22:54:50,878:INFO:choose_better activated
2025-05-03 22:54:50,884:INFO:SubProcess create_model() called ==================================
2025-05-03 22:54:50,884:INFO:Initializing create_model()
2025-05-03 22:54:50,884:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:54:50,884:INFO:Checking exceptions
2025-05-03 22:54:50,886:INFO:Importing libraries
2025-05-03 22:54:50,886:INFO:Copying training dataset
2025-05-03 22:54:50,910:INFO:Defining folds
2025-05-03 22:54:50,910:INFO:Declaring metric variables
2025-05-03 22:54:50,910:INFO:Importing untrained model
2025-05-03 22:54:50,910:INFO:Declaring custom model
2025-05-03 22:54:50,910:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:54:50,910:INFO:Starting cross validation
2025-05-03 22:54:50,910:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:54:51,354:INFO:Calculating mean and std
2025-05-03 22:54:51,354:INFO:Creating metrics dataframe
2025-05-03 22:54:51,354:INFO:Finalizing model
2025-05-03 22:54:51,488:INFO:Uploading results into container
2025-05-03 22:54:51,488:INFO:Uploading model into container now
2025-05-03 22:54:51,488:INFO:_master_model_container: 13
2025-05-03 22:54:51,488:INFO:_display_container: 8
2025-05-03 22:54:51,491:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-05-03 22:54:51,491:INFO:create_model() successfully completed......................................
2025-05-03 22:54:51,631:INFO:SubProcess create_model() end ==================================
2025-05-03 22:54:51,639:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...) result for F1 is 0.6442
2025-05-03 22:54:51,639:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) result for F1 is 0.6765
2025-05-03 22:54:51,639:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...) is best model
2025-05-03 22:54:51,639:INFO:choose_better completed
2025-05-03 22:54:51,656:INFO:_master_model_container: 13
2025-05-03 22:54:51,656:INFO:_display_container: 7
2025-05-03 22:54:51,666:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:54:51,666:INFO:tune_model() successfully completed......................................
2025-05-03 22:54:51,921:INFO:Initializing tune_model()
2025-05-03 22:54:51,921:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=50, custom_grid=None, optimize=F1, custom_scorer=None, search_library=scikit-optimize, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2025-05-03 22:54:51,921:INFO:Checking exceptions
2025-05-03 22:54:51,921:INFO:Soft dependency imported: skopt: 0.10.2
2025-05-03 22:54:51,952:INFO:Copying training dataset
2025-05-03 22:54:51,972:INFO:Checking base model
2025-05-03 22:54:51,972:INFO:Base model : Random Forest Classifier
2025-05-03 22:54:51,976:INFO:Declaring metric variables
2025-05-03 22:54:51,976:INFO:Defining Hyperparameters
2025-05-03 22:54:52,116:INFO:Tuning with n_jobs=-1
2025-05-03 22:54:52,116:INFO:Initializing skopt.BayesSearchCV
2025-05-03 22:58:44,386:INFO:best_params: OrderedDict([('actual_estimator__bootstrap', True), ('actual_estimator__class_weight', 'balanced_subsample'), ('actual_estimator__criterion', 'gini'), ('actual_estimator__max_depth', 11), ('actual_estimator__max_features', 0.4), ('actual_estimator__min_impurity_decrease', 4.490672633438402e-06), ('actual_estimator__min_samples_leaf', 2), ('actual_estimator__min_samples_split', 10), ('actual_estimator__n_estimators', 300)])
2025-05-03 22:58:44,386:INFO:Hyperparameter search completed
2025-05-03 22:58:44,386:INFO:SubProcess create_model() called ==================================
2025-05-03 22:58:44,386:INFO:Initializing create_model()
2025-05-03 22:58:44,386:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011940C4CDD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300})
2025-05-03 22:58:44,386:INFO:Checking exceptions
2025-05-03 22:58:44,386:INFO:Importing libraries
2025-05-03 22:58:44,386:INFO:Copying training dataset
2025-05-03 22:58:44,403:INFO:Defining folds
2025-05-03 22:58:44,403:INFO:Declaring metric variables
2025-05-03 22:58:44,412:INFO:Importing untrained model
2025-05-03 22:58:44,412:INFO:Declaring custom model
2025-05-03 22:58:44,417:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:58:44,425:INFO:Starting cross validation
2025-05-03 22:58:44,425:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:58:48,427:INFO:Calculating mean and std
2025-05-03 22:58:48,427:INFO:Creating metrics dataframe
2025-05-03 22:58:48,433:INFO:Finalizing model
2025-05-03 22:58:50,674:INFO:Uploading results into container
2025-05-03 22:58:50,674:INFO:Uploading model into container now
2025-05-03 22:58:50,674:INFO:_master_model_container: 14
2025-05-03 22:58:50,674:INFO:_display_container: 8
2025-05-03 22:58:50,676:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:58:50,676:INFO:create_model() successfully completed......................................
2025-05-03 22:58:50,813:INFO:SubProcess create_model() end ==================================
2025-05-03 22:58:50,813:INFO:choose_better activated
2025-05-03 22:58:50,821:INFO:SubProcess create_model() called ==================================
2025-05-03 22:58:50,821:INFO:Initializing create_model()
2025-05-03 22:58:50,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-03 22:58:50,821:INFO:Checking exceptions
2025-05-03 22:58:50,821:INFO:Importing libraries
2025-05-03 22:58:50,821:INFO:Copying training dataset
2025-05-03 22:58:50,844:INFO:Defining folds
2025-05-03 22:58:50,844:INFO:Declaring metric variables
2025-05-03 22:58:50,844:INFO:Importing untrained model
2025-05-03 22:58:50,844:INFO:Declaring custom model
2025-05-03 22:58:50,846:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:58:50,846:INFO:Starting cross validation
2025-05-03 22:58:50,846:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:58:52,162:INFO:Calculating mean and std
2025-05-03 22:58:52,162:INFO:Creating metrics dataframe
2025-05-03 22:58:52,164:INFO:Finalizing model
2025-05-03 22:58:52,779:INFO:Uploading results into container
2025-05-03 22:58:52,779:INFO:Uploading model into container now
2025-05-03 22:58:52,779:INFO:_master_model_container: 15
2025-05-03 22:58:52,779:INFO:_display_container: 9
2025-05-03 22:58:52,779:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-05-03 22:58:52,779:INFO:create_model() successfully completed......................................
2025-05-03 22:58:52,919:INFO:SubProcess create_model() end ==================================
2025-05-03 22:58:52,919:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for F1 is 0.6345
2025-05-03 22:58:52,919:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) result for F1 is 0.6791
2025-05-03 22:58:52,919:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False) is best model
2025-05-03 22:58:52,919:INFO:choose_better completed
2025-05-03 22:58:52,935:INFO:_master_model_container: 15
2025-05-03 22:58:52,935:INFO:_display_container: 8
2025-05-03 22:58:52,935:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:58:52,935:INFO:tune_model() successfully completed......................................
2025-05-03 22:58:53,101:INFO:Initializing create_model()
2025-05-03 22:58:53,101:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.5, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': -1, 'num_leaves': 256, 'objective': None, 'random_state': 42, 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'bagging_fraction': 0.4, 'bagging_freq': 0, 'feature_fraction': 0.6637577331805016})
2025-05-03 22:58:53,101:INFO:Checking exceptions
2025-05-03 22:58:53,116:INFO:Importing libraries
2025-05-03 22:58:53,116:INFO:Copying training dataset
2025-05-03 22:58:53,144:INFO:Defining folds
2025-05-03 22:58:53,144:INFO:Declaring metric variables
2025-05-03 22:58:53,146:INFO:Importing untrained model
2025-05-03 22:58:53,151:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-03 22:58:53,157:INFO:Starting cross validation
2025-05-03 22:58:53,157:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:58:53,559:INFO:Calculating mean and std
2025-05-03 22:58:53,559:INFO:Creating metrics dataframe
2025-05-03 22:58:53,564:INFO:Finalizing model
2025-05-03 22:58:53,578:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:58:53,578:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:58:53,578:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:58:53,595:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:58:53,595:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:58:53,597:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:58:53,597:INFO:[LightGBM] [Info] Number of positive: 6578, number of negative: 20773
2025-05-03 22:58:53,599:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.
2025-05-03 22:58:53,599:INFO:You can set `force_row_wise=true` to remove the overhead.
2025-05-03 22:58:53,601:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2025-05-03 22:58:53,601:INFO:[LightGBM] [Info] Total Bins 482
2025-05-03 22:58:53,601:INFO:[LightGBM] [Info] Number of data points in the train set: 27351, number of used features: 14
2025-05-03 22:58:53,601:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.240503 -> initscore=-1.149923
2025-05-03 22:58:53,602:INFO:[LightGBM] [Info] Start training from score -1.149923
2025-05-03 22:58:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,698:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,700:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,702:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,706:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,712:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,724:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,726:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,730:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,732:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,736:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,740:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,746:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2025-05-03 22:58:53,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2025-05-03 22:58:53,764:INFO:Uploading results into container
2025-05-03 22:58:53,766:INFO:Uploading model into container now
2025-05-03 22:58:53,777:INFO:_master_model_container: 16
2025-05-03 22:58:53,777:INFO:_display_container: 9
2025-05-03 22:58:53,779:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-03 22:58:53,779:INFO:create_model() successfully completed......................................
2025-05-03 22:58:53,920:INFO:Initializing create_model()
2025-05-03 22:58:53,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'objective': 'binary:logistic', 'booster': 'gbtree', 'colsample_bytree': 0.7717015338451563, 'device': 'cpu', 'learning_rate': 0.17502697429833836, 'max_depth': 6, 'min_child_weight': 4, 'n_estimators': 104, 'n_jobs': -1, 'random_state': 42, 'reg_alpha': 3.651573868008674e-10, 'reg_lambda': 0.00018991349641985853, 'scale_pos_weight': 2.2642531676225333, 'subsample': 0.5085836727877318, 'tree_method': 'auto', 'verbosity': 0, 'enable_categorical': False})
2025-05-03 22:58:53,928:INFO:Checking exceptions
2025-05-03 22:58:53,945:INFO:Importing libraries
2025-05-03 22:58:53,945:INFO:Copying training dataset
2025-05-03 22:58:53,978:INFO:Defining folds
2025-05-03 22:58:53,980:INFO:Declaring metric variables
2025-05-03 22:58:53,984:INFO:Importing untrained model
2025-05-03 22:58:53,988:INFO:Extreme Gradient Boosting Imported successfully
2025-05-03 22:58:53,994:INFO:Starting cross validation
2025-05-03 22:58:54,000:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:58:54,494:INFO:Calculating mean and std
2025-05-03 22:58:54,494:INFO:Creating metrics dataframe
2025-05-03 22:58:54,499:INFO:Finalizing model
2025-05-03 22:58:54,664:INFO:Uploading results into container
2025-05-03 22:58:54,666:INFO:Uploading model into container now
2025-05-03 22:58:54,676:INFO:_master_model_container: 17
2025-05-03 22:58:54,676:INFO:_display_container: 10
2025-05-03 22:58:54,676:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...)
2025-05-03 22:58:54,678:INFO:create_model() successfully completed......................................
2025-05-03 22:58:54,821:INFO:Initializing create_model()
2025-05-03 22:58:54,821:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 11, 'max_features': 0.4, 'min_impurity_decrease': 4.490672633438402e-06, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 42})
2025-05-03 22:58:54,821:INFO:Checking exceptions
2025-05-03 22:58:54,846:INFO:Importing libraries
2025-05-03 22:58:54,846:INFO:Copying training dataset
2025-05-03 22:58:54,870:INFO:Defining folds
2025-05-03 22:58:54,870:INFO:Declaring metric variables
2025-05-03 22:58:54,879:INFO:Importing untrained model
2025-05-03 22:58:54,887:INFO:Random Forest Classifier Imported successfully
2025-05-03 22:58:54,895:INFO:Starting cross validation
2025-05-03 22:58:54,895:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2025-05-03 22:58:59,043:INFO:Calculating mean and std
2025-05-03 22:58:59,045:INFO:Creating metrics dataframe
2025-05-03 22:58:59,045:INFO:Finalizing model
2025-05-03 22:59:01,322:INFO:Uploading results into container
2025-05-03 22:59:01,322:INFO:Uploading model into container now
2025-05-03 22:59:01,334:INFO:_master_model_container: 18
2025-05-03 22:59:01,334:INFO:_display_container: 11
2025-05-03 22:59:01,334:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False)
2025-05-03 22:59:01,334:INFO:create_model() successfully completed......................................
2025-05-03 22:59:01,485:INFO:Initializing plot_model()
2025-05-03 22:59:01,485:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:59:01,487:INFO:Checking exceptions
2025-05-03 22:59:01,493:INFO:Preloading libraries
2025-05-03 22:59:01,504:INFO:Copying training dataset
2025-05-03 22:59:01,504:INFO:Plot type: auc
2025-05-03 22:59:01,623:INFO:Fitting Model
2025-05-03 22:59:01,623:INFO:Scoring test/hold-out set
2025-05-03 22:59:01,623:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:59:01,623:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:59:01,623:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:59:01,639:INFO:[LightGBM] [Warning] feature_fraction is set=0.6637577331805016, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637577331805016
2025-05-03 22:59:01,639:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2025-05-03 22:59:01,639:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2025-05-03 22:59:01,885:INFO:Visual Rendered Successfully
2025-05-03 22:59:02,016:INFO:plot_model() successfully completed......................................
2025-05-03 22:59:02,045:INFO:Initializing plot_model()
2025-05-03 22:59:02,045:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:59:02,045:INFO:Checking exceptions
2025-05-03 22:59:02,060:INFO:Preloading libraries
2025-05-03 22:59:02,070:INFO:Copying training dataset
2025-05-03 22:59:02,070:INFO:Plot type: auc
2025-05-03 22:59:02,206:INFO:Fitting Model
2025-05-03 22:59:02,210:INFO:Scoring test/hold-out set
2025-05-03 22:59:02,446:INFO:Visual Rendered Successfully
2025-05-03 22:59:02,592:INFO:plot_model() successfully completed......................................
2025-05-03 22:59:02,609:INFO:Initializing plot_model()
2025-05-03 22:59:02,609:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=auc, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:59:02,609:INFO:Checking exceptions
2025-05-03 22:59:02,672:INFO:Preloading libraries
2025-05-03 22:59:02,714:INFO:Copying training dataset
2025-05-03 22:59:02,714:INFO:Plot type: auc
2025-05-03 22:59:02,830:INFO:Fitting Model
2025-05-03 22:59:02,830:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2025-05-03 22:59:02,830:INFO:Scoring test/hold-out set
2025-05-03 22:59:03,257:INFO:Visual Rendered Successfully
2025-05-03 22:59:03,421:INFO:plot_model() successfully completed......................................
2025-05-03 22:59:03,454:INFO:Initializing save_model()
2025-05-03 22:59:03,454:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=../models/LightGBM_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:59:03,454:INFO:Adding model into prep_pipe
2025-05-03 22:59:03,465:INFO:../models/LightGBM_bayes_opt.pkl saved in current working directory
2025-05-03 22:59:03,472:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.4, bagging_freq=0,
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0,
                                feature_fraction=0.6637577331805016,
                                importance_type='split', learning_rate=0.5,
                                max_depth=-1, min_child_samples=100,
                                min_child_weight=0.001, min_split_gain=0.0,
                                n_estimators=197, n_jobs=-1, num_leaves=256,
                                objective=None, random_state=42, reg_alpha=10.0,
                                reg_lambda=10.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2025-05-03 22:59:03,472:INFO:save_model() successfully completed......................................
2025-05-03 22:59:03,619:INFO:Initializing save_model()
2025-05-03 22:59:03,619:INFO:save_model(model=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), model_name=../models/XGBoost_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:59:03,619:INFO:Adding model into prep_pipe
2025-05-03 22:59:03,629:INFO:../models/XGBoost_bayes_opt.pkl saved in current working directory
2025-05-03 22:59:03,636:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 XGBClassifier(base_score=None, booster='gbtree',
                               callbacks=None, colsample_bylevel=None,
                               colsample_bynode=None,
                               colsample_bytree=0.7717015338451563,
                               device='cpu',...
                               gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None,
                               learning_rate=0.17502697429833836, max_bin=None,
                               max_cat_threshold=None, max_cat_to_onehot=None,
                               max_delta_step=None, max_depth=6,
                               max_leaves=None, min_child_weight=4, missing=nan,
                               monotone_constraints=None, multi_strategy=None,
                               n_estimators=104, n_jobs=-1,
                               num_parallel_tree=None, ...))],
         verbose=False)
2025-05-03 22:59:03,636:INFO:save_model() successfully completed......................................
2025-05-03 22:59:03,782:INFO:Initializing save_model()
2025-05-03 22:59:03,782:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), model_name=../models/RandomForest_bayes_opt, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:59:03,782:INFO:Adding model into prep_pipe
2025-05-03 22:59:03,937:INFO:../models/RandomForest_bayes_opt.pkl saved in current working directory
2025-05-03 22:59:03,937:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight='balanced_subsample',
                                        criterion='gini', max_depth=11,
                                        max_features=0.4, max_leaf_nodes=None,
                                        max_samples=None,
                                        min_impurity_decrease=4.490672633438402e-06,
                                        min_samples_leaf=2,
                                        min_samples_split=10,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=300,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2025-05-03 22:59:03,937:INFO:save_model() successfully completed......................................
2025-05-03 22:59:04,132:INFO:Initializing interpret_model()
2025-05-03 22:59:04,132:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 22:59:04,132:INFO:Checking exceptions
2025-05-03 22:59:04,133:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:59:04,158:INFO:plot type: summary
2025-05-03 22:59:04,158:INFO:Creating TreeExplainer
2025-05-03 22:59:04,225:INFO:Compiling shap values
2025-05-03 22:59:04,998:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')

2025-05-03 22:59:05,215:INFO:Visual Rendered Successfully
2025-05-03 22:59:05,215:INFO:interpret_model() successfully completed......................................
2025-05-03 22:59:05,363:INFO:Initializing interpret_model()
2025-05-03 22:59:05,363:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0,
               feature_fraction=0.6637577331805016, importance_type='split',
               learning_rate=0.5, max_depth=-1, min_child_samples=100,
               min_child_weight=0.001, min_split_gain=0.0, n_estimators=197,
               n_jobs=-1, num_leaves=256, objective=None, random_state=42,
               reg_alpha=10.0, reg_lambda=10.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 22:59:05,363:INFO:Checking exceptions
2025-05-03 22:59:05,363:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:59:05,396:INFO:plot type: summary
2025-05-03 22:59:05,396:INFO:Creating TreeExplainer
2025-05-03 22:59:05,438:INFO:Compiling shap values
2025-05-03 22:59:06,196:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\shap\explainers\_tree.py:448: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray
  warnings.warn('LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray')

2025-05-03 22:59:07,913:INFO:Visual Rendered Successfully
2025-05-03 22:59:07,913:INFO:interpret_model() successfully completed......................................
2025-05-03 22:59:08,087:INFO:Initializing interpret_model()
2025-05-03 22:59:08,087:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={'plot_type': 'bar'})
2025-05-03 22:59:08,087:INFO:Checking exceptions
2025-05-03 22:59:08,087:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:59:08,137:INFO:plot type: summary
2025-05-03 22:59:08,137:INFO:Creating TreeExplainer
2025-05-03 22:59:08,179:INFO:Compiling shap values
2025-05-03 22:59:10,029:INFO:Visual Rendered Successfully
2025-05-03 22:59:10,029:INFO:interpret_model() successfully completed......................................
2025-05-03 22:59:10,160:INFO:Initializing interpret_model()
2025-05-03 22:59:10,160:INFO:interpret_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7717015338451563, device='cpu',
              early_stopping_rounds=None, enable_categorical=False,
              eval_metric=None, feature_types=None, feature_weights=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.17502697429833836,
              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=6, max_leaves=None,
              min_child_weight=4, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=104, n_jobs=-1,
              num_parallel_tree=None, ...), plot=summary, feature=None, observation=None, use_train_data=False, X_new_sample=None, y_new_sample=None, save=False, kwargs={})
2025-05-03 22:59:10,160:INFO:Checking exceptions
2025-05-03 22:59:10,160:INFO:Soft dependency imported: shap: 0.46.0
2025-05-03 22:59:10,185:INFO:plot type: summary
2025-05-03 22:59:10,185:INFO:Creating TreeExplainer
2025-05-03 22:59:10,221:INFO:Compiling shap values
2025-05-03 22:59:13,417:INFO:Visual Rendered Successfully
2025-05-03 22:59:13,417:INFO:interpret_model() successfully completed......................................
2025-05-03 22:59:13,582:INFO:Initializing plot_model()
2025-05-03 22:59:13,584:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001194000D750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                       class_weight='balanced_subsample', criterion='gini',
                       max_depth=11, max_features=0.4, max_leaf_nodes=None,
                       max_samples=None,
                       min_impurity_decrease=4.490672633438402e-06,
                       min_samples_leaf=2, min_samples_split=10,
                       min_weight_fraction_leaf=0.0, monotonic_cst=None,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=42, verbose=0, warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2025-05-03 22:59:13,584:INFO:Checking exceptions
2025-05-03 22:59:13,649:INFO:Preloading libraries
2025-05-03 22:59:13,688:INFO:Copying training dataset
2025-05-03 22:59:13,688:INFO:Plot type: feature
2025-05-03 22:59:13,688:WARNING:No coef_ found. Trying feature_importances_
2025-05-03 22:59:13,927:INFO:Visual Rendered Successfully
2025-05-03 22:59:14,064:INFO:plot_model() successfully completed......................................
2025-05-03 22:59:14,115:INFO:Initializing save_model()
2025-05-03 22:59:14,115:INFO:save_model(model=VotingClassifier(estimators=[('lgbm',
                              LGBMClassifier(bagging_fraction=0.4,
                                             bagging_freq=0,
                                             boosting_type='gbdt',
                                             class_weight=None,
                                             colsample_bytree=1.0,
                                             feature_fraction=0.6637577331805016,
                                             importance_type='split',
                                             learning_rate=0.5, max_depth=-1,
                                             min_child_samples=100,
                                             min_child_weight=0.001,
                                             min_split_gain=0.0,
                                             n_estimators=197, n_jobs=-1,
                                             num_leaves=256, objec...
                                                     max_depth=11,
                                                     max_features=0.4,
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=4.490672633438402e-06,
                                                     min_samples_leaf=2,
                                                     min_samples_split=10,
                                                     min_weight_fraction_leaf=0.0,
                                                     monotonic_cst=None,
                                                     n_estimators=300,
                                                     n_jobs=-1, oob_score=False,
                                                     random_state=42, verbose=0,
                                                     warm_start=False))],
                 flatten_transform=True, n_jobs=-1, verbose=False,
                 voting='soft', weights=None), model_name=../models/VotingClassifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\isma_\AppData\Local\Temp\joblib),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-03 22:59:14,115:INFO:Adding model into prep_pipe
2025-05-03 22:59:14,261:INFO:../models/VotingClassifier.pkl saved in current working directory
2025-05-03 22:59:14,286:INFO:Pipeline(memory=Memory(location=None),
         steps=[('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 VotingClassifier(estimators=[('lgbm',
                                               LGBMClassifier(bagging_fraction=0.4,
                                                              bagging_freq=0,
                                                              boosting_type='gbdt',
                                                              class_weight=None,
                                                              colsample_bytree=1.0,
                                                              feature_...
                                                                      max_features=0.4,
                                                                      max_leaf_nodes=None,
                                                                      max_samples=None,
                                                                      min_impurity_decrease=4.490672633438402e-06,
                                                                      min_samples_leaf=2,
                                                                      min_samples_split=10,
                                                                      min_weight_fraction_leaf=0.0,
                                                                      monotonic_cst=None,
                                                                      n_estimators=300,
                                                                      n_jobs=-1,
                                                                      oob_score=False,
                                                                      random_state=42,
                                                                      verbose=0,
                                                                      warm_start=False))],
                                  flatten_transform=True, n_jobs=-1,
                                  verbose=False, voting='soft',
                                  weights=None))],
         verbose=False)
2025-05-03 22:59:14,286:INFO:save_model() successfully completed......................................
2025-05-03 22:59:14,818:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)

2025-05-03 22:59:14,818:WARNING:c:\Users\isma_\miniconda3\envs\proyectoprediccion\Lib\site-packages\sklearn\preprocessing\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)

